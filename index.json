[
    {
        "uri": "https://engineering.cerner.com/authors/matt-henkes/",
        "title": "",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/accessibility/",
        "title": "accessibility",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/",
        "title": "Authors",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/engineering/",
        "title": "engineering",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/",
        "title": "Engineering Health",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/lessons-learned-building-an-accessible-web-application-framework/",
        "title": "Lessons learned building an accessible web application framework",
        "tags": ["accessibility", "terra", "engineering", "oss"],
        "description": "",
        "content": "Since 2017, we at Cerner have been building Terra, a single page web application framework, and accessibility has always been a primary goal. Over the years we\u0026rsquo;ve learned a lot about how to build and test accessible websites, as well as the diverse group of people we\u0026rsquo;re serving. With this blog post, we\u0026rsquo;d like to take a moment to share what we\u0026rsquo;ve learned so far.\nWhy accessibility matters At Cerner, we build software for many different users in many different roles. Doctors and nurses may first come to mind as the primary users for our EHR, but hospitals aren\u0026rsquo;t made up of providers alone and neither are our users. We also provide applications for administrative users and with our patient portal application (HealtheLife), our users are the general patient population. Naturally, with such a broad user base a percentage will require assistive technologies to be able to use our applications. To be as inclusive as possible, Cerner applications must be as accessible as possible, and the Terra Application Framework is one way we are working towards that goal.\nWhat does it mean for a site to be accessible I\u0026rsquo;ll be honest, when we started in 2017, I thought creating an accessible website simply meant appropriately applying ARIA labels to DOM elements. In the last few years, we\u0026rsquo;ve learned, with the help of our accessibility researchers, that creating an accessible site is so much more than that. So, let\u0026rsquo;s hop in our time machine and discuss how we should have approached accessibility with appropriate hindsight (foresight? Time travel is confusing).\nIdentify how users interact with our site with accessible technologies As we discussed above, our broad user base includes a diverse array of people and an equally diverse array of accessibility needs. When discussing web accessibility, it\u0026rsquo;s common to assume that means ensuring a screen reader appropriately can navigate and read your site. Screen reader compatibility is an important aspect of accessibility, it assists users with impaired vision. However, a screen reader won\u0026rsquo;t help users with impaired mobility. To help us cover a wider array of expected interactions and needs, we\u0026rsquo;ve compiled the following use cases.\n Impaired vision Impaired mobility Impaired cognition  These categories may also be experienced temporarily, where individuals may need improved accessibility due to an injury, limited device capability, or location.\nCreate areas of focus for each use case For each of the above use cases, we\u0026rsquo;ve identified the appropriate assistive technologies to apply.\nImpaired vision Focus on helping the user access the site content by ensuring appropriate contrast ratios, font sizing, spacing, color usage, and screen reader support. A good way to get a feel for this user\u0026rsquo;s experience is to enable a screen reader (all Macs have VoiceOver), close your eyes, and navigate your site. Does it make sense?\nImpaired mobility Focus on helping the user navigate the site using devices other than a cursor by enabling keyboard navigation and supplying redundant information access. For example, providing alternative methods to take actions that require gestures on mobile devices.\nImpaired cognition Focus on applying layout simplicity to our workflows and components as well as improving keyword comprehension by emphasizing common design patterns and expected behaviors. For example, buttons should not navigate a user to a new page, that is the role of links.\nAll impairments Provide a consistent semantic layout and access to information regardless of form factor (desktop, mobile). Tooltips are a good example of this. On the desktop, tooltips are accessed when hovering over the element; where as on a mobile devices touch screen you cannot hover. The information contained in the tooltip has to be offered in a way that can be accessed on both devices.\nDesign for accessibility Accessibility cannot simply be layered on any component as an afterthought. This was a mistake we made early on. We’d work through designing and building a complex component, only to test the screen reader compatibility at the end. Unsurprisingly, the component would fail, and we’d have to repeat the process again to correct the issues, sometimes with a complete redesign. Just as engineering constraints inform a design, so to do accessibility constraints. Designing for them up front will reduce expensive re-work late in a project.\nAs you can see, there is far more about accessibility to consider than ARIA labels. We understand that implementing accessibility on the web is a complex and difficult task and we\u0026rsquo;ve built the Terra application framework to help.\nWhat does Terra offer for accessibility Terra provides a stable base on which to build accessible applications. The Terra application framework and each Terra component are designed and built to meet our accessibility standards. To help ensure that our components meet these standards, we perform manual testing, as well as, automated testing using axe-core.\nWhile using Terra gives you a good head start on building an accessible application, you cannot assume your application is fully accessible. The way Terra components are assembled and labeled in an application have a large impact on its accessibility. Any custom components need to be vetted with the same care as Terra components to ensure accessibility. Application owners should make sure to test their apps using a screen reader.\nDeveloping applications for the web is inherently complex. Adding accessibility does not simplify development. By ensuring accessibility is applied to the Terra application framework and components, we are making the right thing, the easy thing to do for applications utilizing Terra. Furthermore, this has made our products more predictable in how users access their functionality since it is being consistently applied. It has been our experience that implementing a consistent semantic layout and a sensible keyboard navigation order lead directly to a logically organized information architecture.\nIf you would like to learn more about Terra, check out its documentation. It’s all open source and we’d love to hear what you think.\nIn closing, I\u0026rsquo;d like to provide a quote from my friend, Neil Pfeiffer, UX Designer, that I think sums up our mission nicely.\n Our job is to help all users better understand their surroundings, providing them not only with clear and direct informational context, but also what choices and actions they can make based on that information, and always put them in control.\n Thanks for reading, and remember, we\u0026rsquo;re all in this together :)\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/oss/",
        "title": "oss",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/post/",
        "title": "Posts",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/",
        "title": "Tags",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/terra/",
        "title": "terra",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/paul-sites/",
        "title": "",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/creating-the-giant-litebrite/",
        "title": "Creating the Giant LiteBrite",
        "tags": ["makerspace", "diy"],
        "description": "",
        "content": "Have you ever wondered what happens when you let two engineers loose on a project with a broad vision and little direction? In our case, LiteBrite happens.\nEvery year Cerner hosts DevCon, an internal 2-day conference for all of Cerner engineering to come together, give talks about technologies, processes, and ideas - all with a little fun sprinkled in. DevCon always has a theme, for 2019 that was set to be “The 80s.” Along with many other decorations, the Engineering Culture team had a dream of having a giant LiteBrite for attendees to interact with at the conference.\nThe Request In the Fall of 2018 the Engineering Culture team approached us, Aaron Kaplan and Paul Sites, with their idea. Both of us are Cerner associates who are involved in Cerner’s MakerSpace administration, heavily involved in mentoring FIRST Robotics, and are known to be quite handy. We were asked if we could create a giant LiteBrite for DevCon to match the 80s theme. Culture team showed us a few examples, the primary one being the following YouTube video. With a few vague hints and videos, a budget of a few thousand dollars, and the goal of making something cool that lit up with pixel art, we got to work.\nResearch What is \u0026ldquo;Giant\u0026rdquo;? Talking with the Culture Team we established “giant” to be somewhere around 8 feet wide and 6ft tall. We knew we wanted it to be big enough that multiple people could play with it and that any adult could comfortably reach all the pegs. We ran with that and started to think about peg density. We wanted to have enough pegs in the board so that fairly complex images could be created.\nConsulting with Cerner UX designer Jordan Lawrence, we experimented with different grid layouts and peg spacings to determine what is possible for resolution and content.\n   While the staggered layout gave a better result on curved designs, we ultimately decided it was less intuitive for the users, and increased our design complexity, so we settled on a square grid pattern. At this point we also decided for ease of construction that we would keep the board within the confines of a 8 ft x 4 ft sheet - a size many materials are readily available in. To balance cost with resolution, we settled on using 912 pegs total - 38 wide by 24 tall.\nDesign Route #1 - Classic LiteBrite We began by exploring the most simple route to creation: a white lit background, a bunch of holes, and a variety of colored pegs. After looking at the costs of colored acrylic rods we determined that to recreate the same spectrum as the original LiteBrite, while having enough pegs of each color, would cost over $8,000 dollars, just for the pegs. This design was quickly shelved based on the cost.\nDesign Route #2 - Modern LiteBrite Knowing that the \u0026ldquo;classic\u0026rdquo; option wasn\u0026rsquo;t going to work we thought back to that initial video. Pegs that are fixed, but that are interactive and respond to users pressing them.\nPaul contacted the owner of the YouTube video to find out additional details of his build. We learned that his version had 225 pegs, each peg was independent (they couldn\u0026rsquo;t be remote controlled or modified), and the project cost more than $10,000.\nChoosing a path - Modern LiteBrite Knowing we could build something more advanced than a “standard” LiteBrite within our budget, we started building upon the \u0026ldquo;modern\u0026rdquo; LiteBrite concept.\nWe needed to ensure the following components were accounted for:\n Pegs to simulate the look of a real LiteBrite A way to cycle through colors Support multiple users tapping pegs at once. Overall packaging of the LiteBrite (frame, wheels, etc)  We explored countless concepts and configurations until we finally landed on the following concept:\n 1.5 inch diameter acrylic pegs An addressable RGB LED behind each peg for illumination A 3D-printed cap that retained the LED and prevented the peg from falling out An arcade button for receiving input  Prototyping At this point we were excited. We demonstrated we can have an interactive peg. Excellent. Then it dawned on us we want to have nine-hundred and twelve of these. We began to realize that what was easy for one was going to become very challenging to scale.\n   Luckily, Adafruit sells an amazing product, the FadeCandy. This board is capable of controlling 512 LED\u0026rsquo;s, supports daisy-chaining, and it has great libraries to make displaying content moderately easy.\nThe next challenge to solve was reading the 912 user inputs. Microcontrollers and other devices that are designed to handle large numbers of inputs still do not come close to meeting our needs, plus, they are expensive.\nInstead of creating one monolithic controller, we decided to use Arduino Nanos as addressable units of inputs. Each row of 38 inputs would be wired to a networked Nano with 5 shift registers allowing us to \u0026ldquo;expand\u0026rdquo; their inputs to 40 each.\nAfter the design was established Hans Perera, a fellow Makerspace Admin, suggested that to get all 24 of these done, a custom printed circuit board (PCB) would be our best option. Hans then stepped up and did the development on the PCBs while we forged ahead with the rest of the project.\nCustom printed circuit board (PCB)\n   Hans placing components on the custom PCBs.\n   Construction Taking Over the Robotics Shop The Culture team was unable to provide the space or equipment needed to create the LiteBrite. Fortunately, the FIRST Robotics team we mentor (Team #1987, The Broncobots), has a full machine shop that they allowed us to use for the project.\nA Strong Frame With an estimated weight of close to 500 pounds, the LiteBrite was going to need a sturdy and stable frame. Joshua Wentworth, a Broncobots team mentor, volunteered the solution of a welded steel H-frame to hold the grid. This would allow for a compact, strong, and relatively low cost solution for the frame.\nThe frame pieces prior to being welded together.\n   Paul used the team\u0026rsquo;s CNC router to cut the 912 holes in the main board. Pre-cut acrylic pegs are expensive, instead we purchased 4ft sticks of raw acrylic and cut them to length ourselves.\n   LED Strands Each of the 912 pegs required an RGB LED, and due to the spacing of the pegs, off-the-shelf LED strips wouldn\u0026rsquo;t work. Instead, we used individual RGB LED chips, which required 6 soldered connections each, meaning 2,736 wires cut to length and stripped. Early estimates showed that we would need over 36 hours of just stripping the wires. Given our impending deadline we purchased an automatic wire stripper. Over 36 hours of wire stripping, reduced to about 2 with the automatic wire stripper.\nWith Aaron\u0026rsquo;s soldering jig, a soldering pot for wire tinning, our LED testing controller, plus help from Cerner associates and several robotics members, we got to work mass producing our LED strands.\n   Buttons Once we had a handle on the LEDs we turned to the buttons. Each button needed power and a discrete signal wire running back to the control board. With some forethought towards service and troubleshooting significant thought was put into wire management. Buttons were distributed among 6 separate panels that were able to be disconnected and removed, simplifying assembly and troubleshooting.\nWe calculated the lengths of wire required to each button to prevent excess wire. Using the automatic wire stripper we precisely cut staggered lengths of wire and created a wiring harness for each row of buttons.\nPrecision cut staggered wires to use for making wiring harnesses for buttons.\n   Starting to install the button panels behind the LED pegs.\n   Software Time was of the essence and we could not wait for the physical hardware to be complete to develop the software. Our technology choices allowed for software development to happen in parallel to the physical construction.\nWe needed the color of a peg to change the instant a user pressed it to give the best possible user experience and encourage interaction. This necessitated that certain software components be optimized to minimize input lag.\nArduino Each Arduino is responsible for the 38 buttons attached to it, and because of the multi user need, extra care was taken to ensure there were no double presses or miss counting occurring. Every Arduino is assigned a static IP address that correlates to the row it is listening to. Whenever a button in its charge is pressed, the Arduino broadcasts a UDP packet containing the state of the entire row in the form of a binary string.\n01000000000000000000000000000000000000 Using the IP and the index in the string one could know the (X,Y) position of the button that was pressed. Bitwise operators were leveraged to ensure buttons were only registered once per press. With 38 buttons to connect to one tiny Arduino, all with white wires there was a high risk of connecting a button to an incorrect input.\nStarting to install the button panels behind the LED pegs.\n   We considered labelling each wire, but that was expensive, time consuming and still error prone. Then it hit us, lets plug in buttons at random and make the controller do the hard work. From this the Arduino \u0026ldquo;training'' mode was created. In training mode each peg is pressed in order. The Arduino records the presses and creates a map to correlate expected input index to the physical input pin. Doing this added minimal processing overhead but streamlined assembly and will aid in troubleshooting.\nLiteBrite Controller This is the main control software, it receives the UDP packets sent from the Arduinos and sets the LEDs through the FadeCandy board. When a peg is pressed the packet is sent that contains the (X,Y) coordinate for that button. The computer then updates the corresponding LED pixel at the matching coordinate. Each tap of a peg cycles a pixel through a list of available colors.\nAlthough we set out to create a LiteBrite, what we really created is better described as an interactive dot matrix display. This has several advantages, the largest of which being able to display premade static, and dynamic content. A good example of this in practice is our clear functionality. When the clear button is pressed Pac-Man will chomp across the display eating peoples designs and resetting all pegs to black.\nA big question we knew would come up is, \u0026ldquo;Is my work lost?\u0026rdquo; As the LiteBrite records ever press, we can state that no work is lost. We can even recall unique designs and play them back as a timelapse to showcase people\u0026rsquo;s work.\nBy collecting this data we also know exactly how many presses the LiteBrite has had. As of the time of this writing, we\u0026rsquo;ve had 443,449 presses and counting!\nLighting it Up    Ultimately, LiteBrite was completed at about 1:10 a.m. the first day of the DevCon event. It was loaded into a trailer and delivered hours before the event started that morning.\nLiteBrite was a whirlwind of a project that involved at least 1,100 people-hours over the course of around a month and a half. Although we led the primary design and construction, over a dozen people volunteered their time to help us light up DevCon.\n   Post-DevCon LiteBrite has occasionally made appearances around Kansas City, appearing front and center at Maker Faire for over 17,000 attendees to interact with.\n   LiteBrite was invited to Cerner Health Conference in October 2019.\n   Today, when LiteBrite isn\u0026rsquo;t traveling it lives at the entrance to Cerner\u0026rsquo;s Innovations Campus. It greets associates every morning with a hot cup of coffee, plays back select LiteBrite creations, and encourages associates to take some time to create.\nThis article includes contributions from Aaron Kaplan.\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/diy/",
        "title": "diy",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/makerspace/",
        "title": "makerspace",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/carl-chesser/",
        "title": "",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/we-are-back-w-new-look/",
        "title": "We’re Back with a New Look",
        "tags": [],
        "description": "",
        "content": "If you have been to our blog before, you might notice that it has a new look. Our past version of the site was one that we had used for several years. It utilized Jekyll for the static site generation, which is a popular project that leverages the Ruby ecosystem. Over the last few months, we recognized that our site needed an update. This change wasn’t only with its look and feel, but also with what we used in building the site.\nToday, we have uplifted our site to use Hugo. We have been using this project internally for project documentation, and have found it to be a powerful, yet simple, static site generation tool. In this uplift, we wanted to make sure we could simplify our site generation to make it easier for other Cerner engineers to contribute to our site. To get this started, Alex Bezek and I decided to leverage Cerner’s last virtual ShipIt event to get through a first pass of the site. We were able to get through the first round of moving to Hugo, which greatly simplified generating the site, and it removed the additional burden of managing Ruby gem dependencies. We were also able to start reducing some issues we discovered in our site around accessibility, something that sparked our interest after listening to Jennifer Luker’s talk on this topic.\n   Thank you Alex Bezek for your help and the Engineering Culture team for giving feedback along the way. We look forward to sharing new blog posts in this updated site format!\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/harish-pendyala/",
        "title": "Harish Pendyala",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/jordan-bush/",
        "title": "Jordan Bush",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/sharynne-azhar/",
        "title": "Sharynne Azhar",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/shipit/",
        "title": "shipit",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/shipit-xvii/",
        "title": "ShipIt Day XVII",
        "tags": ["engineering", "shipit"],
        "description": "",
        "content": "ShipIt Day, Cerner Engineering’s 24-hour hackathon, provides associates an environment to be creative, work on new projects, and network with other associates. This event is one of the many things that sit at the core of our Engineering Culture at Cerner, and is rapidly growing- now being hosted globally in Romania, Sweden, India, and the UK. In October, our team had the chance to participate in the seventeenth ShipIt Day at Kansas City’s Innovations Campus and managed to take home the Golden Keyboard trophy as the first place team!\nLeft to Right - Sharynne Azhar, Vu Tran, Harish Pendyala, and Jordan Bush\n   Overview During this ShipIt challenge, our team\u0026rsquo;s goal is to create a dashboard that tracks newly discovered CVEs and displays any affected Cerner artifacts. The Z3R0 D4Y dashboard is a centralized place where teams can quickly go to identify and remediate any new security vulnerabilities and threats.\nThe Problem New security vulnerabilities come up every day and keeping up with the latest updates can be tough. To take a more proactive response, we needed a way to get real-time updates of the latest vulnerabilities and identify which Cerner artifacts are affected.\nBut where can we get this data?\nOur Solution: Z3R0 D4Y We created the Z3R0 D4Y dashboard (shown below) to help solve this problem. The dashboard gives an overview of different CVEs and which products are affected.\n   It then allows you to drill in and see details about the affected products. Below is an example of a test app to show a vulnerability.\n   How It Works The project is comprised of two components: the backend engine and the dashboard site.\nThe engine subscribes to the National Vulnerability Database (NVD). The NVD provides a collection of data feeds of published vulnerabilities which are updated approximately every two hours. The Z3R0 D4Y engine runs nightly to retrieve the list of most recently discovered CVEs. The data then is parsed to identify details of the new CVEs and the list of vulnerable dependencies. Using the GitHub API, the engine then checks all of Cerner\u0026rsquo;s artifacts available in Github for matches based on that project’s dependency file. In Ruby, this would be the Gemfile.lock, for example. Finally, it publishes the findings to the Z3R0 D4Y dashboard.\nThe dashboard site is a simple Github page that displays all CVEs, their description, publish date, and affected artifacts. Its content is updated automatically nightly whenever the engine produces new data. The site also supports sorting and searching across the CVEs and Cerner artifacts data.\nOther ShipIt Day Winners Second Place: nudgers The nudgers teamed up to create a ‘Nudge’ behavior-changing native iOS or Android application that can be quickly delivered to Cerner Associates to begin using. The app has users select a single behavior they’d like to improve upon (drink more water, less screen time, etc), they will manually enter their day’s total, and the app will begin ‘nudging’ them to make small changes based on their entries\nLeft to Right - Pat Walsh, Anna Luckey, John Moses, Justin Eddings\n   Third Place: Stop Trying To Make \u0026lt;Cameras\u0026gt; Happen This team worked on part two of a previous project they had worked on in a ShipIt Day, which is a virtual ICU Mobile Cart. In this hackathon they were able to implement device event listeners, encounter and location device association, conference calling integration, and data calls to the eHospital API.\nLeft to Right - Taylor Clay, Bilal Ahmad, Duncan Dufva\n   "
    },
    {
        "uri": "https://engineering.cerner.com/authors/vu-tran/",
        "title": "Vu Tran",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/matt-boveri/",
        "title": "Matt Boveri",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/shipit-xvi/",
        "title": "ShipIt Day XVI",
        "tags": ["engineering", "shipit"],
        "description": "",
        "content": "Finishing our 16th occurrence of the series, our quarterly ShipIt Day hackathon invites the makers and innovators to come out and share new ways to think about our daily work here at Cerner. Whether it\u0026rsquo;s experimenting with new features or improving our developer workflows, ShipIt Day gives our engineers a chance to try and learn new things. This ShipIt day in particular had some exciting projects that melded hardware and software. Here are the top teams.\nShipIt Day XVI Winning Project Left to Right - Matt Boveri, David Crowder, Sam Livingston, Ian Kottman, Anthony Ross\n   During this 24-hour event, the team created a capacity meter for Cerner’s DC/OS cluster. The focus was to map out the potential for failure within a DC/OS cluster based on hardware constraints: CPU / Memory / Disk space. Every hour the meter will sample the usage and display the currently most used resource. During the live demo seen below, CPU is the most utilized at just under 70%. Colorful LEDs were used to signify which hardware type is currently displayed. As the DC/OS cluster hits 95% a trigger will flash the red LED to alert that the cluster is nearly at capacity.\nThe code as well as additional notes on the project can be found in the public GitHub repository: https://github.com/mboveri/dcos_meter.\n   Meter Metrics  0 - 70% = Green 71 - 85% = Yellow 86% - 100% = Red At 95%+ Red LED is illuminated  Raspberry Pi Configuration       Other ShipIt Day Winners 2nd Place- Sparkling Darlings: Aakash Pydi, Gunjan Kaphle, James Freeman, Jokongir Rikhsiboev, \u0026amp; Cory Tenbarge\n In this project, the team worked on prototyping their team\u0026rsquo;s user experience monitoring (UEM) jobs using Apache Spark.\nThe UEM jobs currently run a series of giant aggregation summarizing SQL queries on our Vertica cluster. This leads to issues such as (i) significant lag in data availability, (ii) giant, unintuitive SQL challenging to support and debug, (iii) the addition of significant load to our Vertica cluster, (iv) the load added to Vertica being \u0026lsquo;inefficient\u0026rsquo;. If these jobs were in Spark, it would be much more efficient, have near real time data availability, and be significantly easier to debug and support. It also could potentially leverage Spark libraries for machine learning and graph processing.\nThe more general goal is to set up an incremental development and adoption strategy, for a Spark powered aggregation, summarization, and analytics layer in the team\u0026rsquo;s data architecture. This will be an immensely valuable tool in the team\u0026rsquo;s arsenal.\n 3rd Place- Syug Looc: Eric Vue, Minhaz Abdullah, Zach Miller, Tarun Kolla, Sydne Anchutz \u0026amp; Zachary Herridge\n This project focused on making the Dev Academy process more accessible to new hires at Cerner. While going through training, there is a very specific development process that associates must follow. The documentation for different parts of this process are currently hosted in multiple locations and sometimes contradicts itself. To fix this, the team developed a Chrome extension. The Chrome extension modeled a checklist to keep track of each step of the process per development task. It also cataloged the instructions and requirements for each step of the process and provided useful links to documentation to minimize errors. There were also quick links that allowed the user to jump between Jira/Crucible/GitHub/Jenkins for each task assigned to them. The use of this extension will make development in the Dev Academy much more efficient and reduce user error.\n Best Presentation- Shippie-ki-yay Mother FHIR: Steven Goldberg, Ryan Neff, \u0026amp; Jean Fernandez\nBest Team Name- ThunderCats: Chris Wheeler, Saranath Govindaraju, Vinayak Tare, \u0026amp; Nageswara rao Nandigam\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/brett-jankord/",
        "title": "Brett Jankord",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/terra-ui/",
        "title": "Terra UI: A Health-Care focused UI Component Library",
        "tags": ["engineering", "oss"],
        "description": "",
        "content": "   Back in 2013, like most other companies, Cerner was heavily invested in building user interfaces with Twitter Bootstrap. Around that time, Dave Rupert wrote about the concept of Responsive Deliverables and touched on a key concept, creating “Tiny Bootstraps, for Every Client”. Along with this, Brad Frost had started promoting the idea of Atomic Design. We saw a lot of value in these ideas and saw that we needed to evolve how we were developing UI at Cerner. We could see that Bootstrap was no longer meeting our needs and we felt we needed to start the process of building our own \u0026ldquo;tiny bootstrap\u0026rdquo;.\nA small team was formed to begin work on a project named “Terra” to establish a component library for reusable UI patterns needed in our health care applications. We begin work building responsive, accessible, and internationalized components. The work was very similar to Twitter Bootstrap, but tailored to our specific needs at Cerner. We found great success and adoption with this new project but as tech evolved, we found our jQuery based UI components were showing a bit of age.\nFast-forward a few years, ReactJS and ES2015 had started to grow in popularity. At Cerner, we started to see more and more teams developing solutions with ReactJS. We could see that if we wanted to continue to provide consistent UI solutions that incorporated responsive design, accessibility, and internationalization concerns, we needed to evolve our component library.\nWe took this as an opportunity to build something new and share it with a wider audience. We decided to build a new version of our component library and base it on ReactJS. We also decided we wanted to make the project open source so anyone could help contribute to it. Today, we have a wide offering of UI components with a focus on health care applications in our open source component library, Terra UI.\nWho is Terra UI for? Terra UI is developed with the goal of helping consumers (including many app teams across Cerner, as well as external consumers in the open-source community) focus their efforts towards higher-level app concerns. We\u0026rsquo;ve leveraged the expertise of our UX team to create attractive and intuitive UI components that provide a consistent look and feel backed by usability research. Additionally, we\u0026rsquo;ve put a heavy focus into abstracting styling, accessibility, responsive design, cross-browser support, and internationalization considerations into our components so that consumers can get their projects up and running quickly. Cerner is utilizing Terra UI for healthcare web applications at scale with great success. We\u0026rsquo;re proud to make it available to other via open source and we hope you\u0026rsquo;ll check it out. Keep reading below to learn more about how you can consume and/or contribute to Terra UI.\nTerra UI Ecosystem The Terra UI ecosystem covers three types of components.\nTerra Core A repository containing a collection of common UI components ranging from buttons, to alerts, to form components, and more needed for building accessible, responsive, and internationalized applications.\nTerra Framework A repository that contains higher-order and composable UI components that build on top of terra-core components that help with application layout concerns and progressive disclosures.\nTerra Clinical A repository that includes clinically focused UI components that build on top of terra-core.\nAdditional Terra Packages Along with our component repositories, we provide webpack configuration and testing utilities via terra-toolkit, linter configs for stylelint and eslint, a component doc site webpack plugin via terra-dev-site, and tooling to help aggregate application translations.\nGetting Started To get started, we recommend checking out our guide to installing your first Terra UI component.\nContributions We welcome contributions to Terra UI. Check our Github issues for ways you can get started with contributing, and be sure to check out our contribution guidelines.\n NPM: https://www.npmjs.com/search?q=keywords%3ACerner%20Terra Site: https://engineering.cerner.com/terra-ui/  "
    },
    {
        "uri": "https://engineering.cerner.com/blog/introducing-f-twelve-an-open-source-dev-console/",
        "title": "Introducing F-Twelve, an Open Source Dev Console",
        "tags": ["engineering", "oss"],
        "description": "",
        "content": "   Many modern web browsers come with tools that can help developers debug their websites. If you are using Google Chrome or Firefox for example, try pressing F12. A new panel should open containing various tools. These tools provide a look “under the hood” for the current page. Common tools include a JavaScript console, JavaScript debugger, DOM explorer, network request viewer, performance profile, local storage manager, and more.\nUsually these tools are just a keypress away, but some environments don’t have browser tools. Alternate options exist such as Windows F12 chooser (which inspired the name F-Twelve) and Firebug Lite. These are nice but neither of them is a perfect solution. The former requires a certain version of Windows and the latter has not been updated in over 6 years. The kicker is, neither work inside an embedded IE frame. We ran into this issue while developing a SMART on FHIR application. Local development in a web browser would go smoothly but then we’d deploy to our test environment which used an embedded IE frame and it would not work. This was frustrating because it gave absolutely no indication of what the issue could be. It was typically either a blank white screen, or an eternal loading icon. The only way to troubleshoot was making a guess, adding alert calls, redeploying, and hoping it had something useful. This situation was occurring frequently and very inhibiting to development. We didn’t have access to F12, so we wrote our own JavaScript tool, F-Twelve.\nThe initial version was simply a div at the bottom of the page that would print window.onerror events and anything sent to console.log. The functionality was very limited, but it solved our problem of being able to identify errors. Since then we have cleaned up the UI and added functionality.\nCurrent features include:\n Display console output (log, info, warn, error) Evaluate expressions from console input Hide or show the tool via keyboard shortcut  For security reasons the console input does not execute arbitrary code, it can only parse and evaluate variables (e.g. window.location).\nPotential features for the future include:\n DOM explorer Network request viewer Debugger (Any other Dev Tools features)  The project is still very young and the functionality that it has now is just the tip of the iceberg. The end goal is ultimately to provide all the functionality of modern browser’s Dev Tools without the need for a modern browser.\nIf you want to contribute or learn more about the tool, check out the code or try the live demo.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/patrick-gross/",
        "title": "Patrick Gross",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/abhijit-rao/",
        "title": "Abhijit Rao",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/carbon-graphs-open-source-visualization-api/",
        "title": "Carbon Graphs: An Open Source Visualization API",
        "tags": ["engineering", "oss"],
        "description": "",
        "content": "We are pleased to announce Carbon Graphs as an open source code project!\nCarbon is a lightweight, vanilla JavaScript visualization API built with D3 that integrates well with any consumer\u0026rsquo;s tech stack. It provides a variety of graph types that are framework agnostic and responsive out of the box.\n   Motivation Over the past few years, we have gradually updated our user interface to use modern JavaScript libraries such as mithril.js and React. Our components needed a graphing solution that would work well with our current framework while providing visualizations based on leading industry solutions.\nWe did an extensive audit of various Cerner solutions already available, the graph types they were using, and the libraries used to plot the data. We discovered that most solutions had graph implementations baked into their product such that they were modifying or extending open source libraries in an effort to support Cerner\u0026rsquo;s unique clinical and accessibility needs. These implementations, however, could not be broadly reused. As such, we saw an opportunity to collaborate with our User Experience team to create an attractive, modern, and flexible graphing solution that not only would meet Cerner\u0026rsquo;s design standards, but that could be open sourced to give back to the engineering community for their own graphing needs.\nGraph Types With Carbon, you get:\n Line graph Multiline graph Spline Line graph Non-Contiguous Line graph Paired Result graph Timeline graph Bar graph Pie chart Gantt chart  Graphs come with following settings that be customized:\n Legend Labels Grid lines (Vertical and Horizontal) Axes (X and Y, or Y2) Regions (Horizontal only)  Carbon also supports functionalities that are not provided with popular open source libraries such as:\n Support for custom SVG shapes as data points Support for different standard shapes (dark or light) as data points Locale support for axes ticks  Drawing a Line Graph Let’s see how easy it can be to get started!\nTo create a line graph, first create an HTML element that will hold the graph. Here, we are specifying a main element with an id of root.\n\u0026lt;main id=\u0026#34;root\u0026#34;\u0026gt;\u0026lt;/main\u0026gt; From there, we will initialize a JavaScript object that configures various aspects of the graph, including where the graph will be drawn and how the axes should appear.\nconst graphConfiguration = { bindTo: \u0026#34;#root\u0026#34;, axis: { x: { type: Carbon.helpers.AXIS_TYPE.TIME_SERIES, label: \u0026#34;Datetime\u0026#34;, lowerLimit: new Date(2016, 0, 1, 9, 0).toISOString(), upperLimit: new Date(2016, 0, 1, 15, 59).toISOString() }, y: { label: \u0026#34;Temperature (degF)\u0026#34;, lowerLimit: 90, upperLimit: 106 } } }; Next, we’ll configure the dataset we want to plot.\nconst dataset = { key: \u0026#34;uid_1\u0026#34;, label: { display: \u0026#34;Oral Temperature\u0026#34; }, shape: Carbon.helpers.SHAPES.RHOMBUS, color: Carbon.helpers.COLORS.BLUE, values: [ { x: new Date(2016, 0, 1, 10, 5).toISOString(), y: 96.5 }, { x: new Date(2016, 0, 1, 12, 15).toISOString(), y: 98.7 }, { x: new Date(2016, 0, 1, 14, 15).toISOString(), y: 97.4 } ] }; And to wrap it all up, we’ll call loadContent to draw the content.\nconst graph = Carbon.api.graph(graphConfiguration); graph.loadContent(Carbon.api.line(dataset)); That’s it!\n   Contribute! We are continuously working on improving Carbon to support our ever-increasing clinical needs! Help us make it better by reporting issues using the GitHub issues queue or feel free to contribute with pull requests.\nNPM: https://www.npmjs.com/package/@cerner/carbon-graphs\nSite: https://engineering.cerner.com/carbon-graphs/\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/ccl-unit-and-ccl-testing-released-to-open-source/",
        "title": "CCL Unit and CCL Testing released to open source",
        "tags": ["engineering", "oss"],
        "description": "",
        "content": "We are pleased to announce the release of CCL Unit and CCL Testing as open source code projects, as well as the release of the CCL Testing plugin artifacts to Maven Central.\nCerner Command Language (CCL) is a database query and scripting language used with Cerner Millennium databases. CCL Unit is a unit testing framework for CCL written in CCL. The CCL Testing plugins generate reports for viewing the results of CCL Unit tests and static analyses of CCL programs and for generating code documentation from comments in the source code for CCL programs. These tools were created to help developers improve the quality of their CCL programs.\nWe have released the code for these tools to open source to facilitate frictionless contributions of enhancements, corrections, and documentation improvements by the CCL developer community at large and to allow CCL developers to opt in for automatic notifications about version updates by subscribing to the CCL Unit and CCL Testing source repositories. We have released the CCL Testing plugins to Maven Central to eliminate the need to configure the location and access credentials for a proprietary plugin repository in order to use them.\nWe invite you to visit the change log of each repository to see the many improvements made since the original releases. Most notably:\n CCL Unit tests can now be executed directly from a CCL command prompt. A Maven installation is not required. CCL Unit now sports a mocking framework which makes it a snap to create and access mock database tables seeded with test specific data without affecting the integrity of any existing database data.  "
    },
    {
        "uri": "https://engineering.cerner.com/authors/fred-eckertson/",
        "title": "Fred Eckertson",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/brandy-poiry/",
        "title": "Brandy Poiry",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/culture/",
        "title": "culture",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/devacademy-six-years-later/",
        "title": "DevAcademy Six Years Later",
        "tags": ["culture", "engineering"],
        "description": "",
        "content": "Cerner hires a lot of software engineers. In fact, we have hired over 1,600 engineers since our early career onboarding program, DevAcademy, went live almost six years ago. In particular, we use DevAcademy to onboard early career software engineers.\nDevAcademy is a three part onboarding program built by engineers for engineers. The first part of DevAcademy is DevEssentials which covers a lot of the basics new engineers need to know about development ecosystems in our development environment. After spending a few days hearing from the software engineers who teach DevEssentials, our new hires move into DevCenter. DevCenter is a performance based, hands-on training opportunity. Engineers dig into real projects on small teams with the help of experienced mentors from teams across Cerner’s tech stack. They work through their projects, adding new functionality to existing applications, and honing their craft with the help of the full-time engineering instructors who keep DevAcademy running as well as their mentors. Over 700 engineers have participated as mentors providing feedback in code reviews, office hours, and scrum on more than 120 different projects. One engineer said of DevAcademy, “I was able to (and encouraged to) work independently, make decisions and find solutions without always having to seek approval for them. I also got feedback when those decisions turned out to be bad, so I could make better decisions in the future.” As soon as new engineers are writing, testing, and documenting their code in a way that meets the criteria Cerner has established for new hires, they go to teams across all areas of the company.\nNew engineers working on their projects in the DevCenter\n   Engineering managers report that DevAcademy engineers are able to make meaningful contributions to their new teams very quickly, usually within two months. Before DevAcademy existed, it would often take six months before a new engineer could provide valuable contributions to their codebase. Engineers go to their new teams already equipped with knowledge of the process, tools, and best practices that are standard across engineering teams at Cerner.\nNew engineers working on their projects in the DevCenter\n   After joining their teams, new software engineers very quickly dive into the work, learning the architecture and tooling as they go. When they do need additional training to help level-up their skills, they utilize the third part of DevAcademy, DevElectives. DevElectives focus on practices and tools engineers have identified as needs for their teams and architectures. Like DevEssentials, DevElectives are taught by engineers and provide opportunities to hear from more experienced people while digging into new technology.\nIn addition to providing an optimized orientation to writing code at Cerner, DevAcademy helps people cultivate relationships and networks that they will utilize throughout their careers. Many groups continue to meet regularly to discuss the work they are doing and get pointers from their colleagues. One Cerner associate said, “When I first joined Cerner, I had just moved and was alone in a new city. I was apprehensive, but excited. I had no idea that DevAcademy would be so formative to the friendships I created and networks I joined. DevAcademy was where I met my friends, some of whom I can already tell will be lifelong, where I met mentors who became the start of my network, and became inspired to be a mentor to continue expanding my network.”\nA mentor helps a new associate set up his code review\n   In many ways, DevAcademy has become a staple of engineering at Cerner. It is where engineers have a safe space to learn and grow as they begin their career. It has become a proving ground for exploring new technologies and possibilities, and it gives more advanced engineers a way to improve their leadership skills. More importantly, it helps Cerner shape its engineering culture; focusing on the future state of engineering and cultivating an environment in which engineers are prepared to join a team and spread the knowledge they have gained during training to improve engineering as a whole.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/jake-kramer/",
        "title": "Jake Kramer",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/pi/",
        "title": "pi",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/pi-day-2019-build-your-own-alarm-system/",
        "title": "Pi Day 2019: Build your own alarm system",
        "tags": ["pi", "raspberry_pi"],
        "description": "",
        "content": "At Cerner Engineering, we love to celebrate Pi Day. This day is not only a fun time to enjoy eating pie and reflecting on mathematical properties, but we also share big announcements internally for our developers conference, DevCon.\nDrew a crowd to celebrate Pi Day this afternoon with snacks, lightning talks, and our DevCon 2018 theme reveal! #314day #cernerdevcon pic.twitter.com/wyy5eLKHVl\n\u0026mdash; Cerner Engineering (@CernerEng) March 14, 2018  For this post, we thought it would be fun to share a simple example of how you can hook your existing monitoring system to a physical alarm system powered by a Raspberry Pi. This alarm will be a red spinning light, the universal symbol of \u0026ldquo;something is going wrong.\u0026rdquo; We will build a program that will integrate with New Relic to determine if there are issues in our environment. If there are any issues, it will trigger the alarm and your monitoring system will come alive!\nParts List  Jumper wires - Wires that we will use to trigger the switch from your GPIO pins. Alarm light that is AC powered - There are several different types of fun alarm or party lights you could use. The important characteristic that we are seeking is an AC powered light, as we will use this to control its state. Many lights you will find are battery powered, which will not work for this example. PowerSwitch Tail or alternative - I have had this setup for quite some time, so I still have a PowerSwitch Tail, but looks like they may not be available by all providers. You can get other alternatives out there, where you can control an AC power switch safely through GPIO interactions. Raspberry Pi - I have an older model, but anything with a powered ethernet connection works well (ex. Raspberry Pi 3 Model B+), as we will be making remote HTTPS calls. If this is your first time getting a Raspberry Pi, there are great kits that include your power adapter and SD card (as you will need that too). RJ45 ethernet cable (optional) - We won’t leverage WiFi for this example, and simply use a ethernet cable for maintaining a network connection for the Raspberry Pi. You don’t need this if you already know how you plan to connect your Raspberry Pi to your existing network.  The Alarm Once you have this setup, we are going to use the Raspberry Pi to control your AC output by communicating to the PowerSwitch Tail through our GPIO pins. A Go program will control the logic of flexing the alarm on or off by polling a monitoring system: New Relic. New Relic is a real-time monitoring platform that gives you powerful insights about the applications you are operating. One of the features of New Relic, is that you can build alerts about different indicators of your application (ex. high memory utilization of a service). These can be rolled up to an \u0026ldquo;incident\u0026rdquo; concept, when you have a violation on an alert condition. For this physical alarm, it made sense to pair it with this concept that we use from New Relic. Therefore, if you build something that would trigger human engagement with your alerts (like an incident), this alarm can generically pick these up, without you having to manage anything else.\nThe Code Here is the code snippet of what we will implement. It is a Go program, which will interact with the GPIO pins using go-rpio. It will essentially run in a loop, and poll New Relic’s Alert API every minute. To ensure we aren’t running the alarm in the after-hours, we will also flex when this can trigger (ex. Mon - Friday, 9 - 5 PM).\nFirst, we will build something that can invoke the New Relic Alerts API. This will offer a single function (hasOpenIncidents) that will dictate if there are any open incidents when checking with New Relic.\n// New Relic Incident API type (just using including two of the fields as an example) type Incident struct { Id int `json:\u0026#34;id\u0026#34;` OpenedAt int64 `json:\u0026#34;opened_at\u0026#34;` } // New Relic Incident API response type, which we will assess on having any items in the array for the alarm. type IncidentsResponse struct { Incidents []Incident `json:\u0026#34;incidents\u0026#34;` } // Documented here: https://rpm.newrelic.com/api/explore/alerts_incidents/list func hasOpenIncidents(apiKey string) bool { url := \u0026#34;https://api.newrelic.com/v2/alerts_incidents.json?only_open=true\u0026#34; spaceClient := http.Client{ Timeout: time.Second * 15, } req, err := http.NewRequest(http.MethodGet, url, nil) if err != nil { log.Fatal(err) } req.Header.Set(\u0026#34;X-Api-Key\u0026#34;, apiKey) res, getErr := spaceClient.Do(req) // If there is a failure to calling New Relic (ex. timeout), simply logging and returning back \t// for a later retry \tif getErr != nil { log.Printf(\u0026#34;Failed to get a response: %s\u0026#34;, getErr) return false } incidentsResponse := IncidentsResponse{} jsonError := json.NewDecoder(res.Body).Decode(\u0026amp;incidentsResponse) if jsonError != nil { log.Fatal(jsonError) } return len(incidentsResponse.Incidents) \u0026gt; 0 } We will then manage the GPIO pin state in a simple loop which will check to see if there are any open incidents. If so, it will set the pin to High, which will trigger the light switch. Otherwise it will set it to low. We will also include a handler for setting the pin to low when we terminate the application (ex. via a SIGTERM). Example of managing the state:\n// If there are any open New Relic incidents, set the pin to high \tif hasOpenIncidents(apiKey) { log.Print(\u0026#34;Incidents detected, setting alarm.\u0026#34;) pin.High() } else { log.Print(\u0026#34;No incidents detected.\u0026#34;) pin.Low() } When you put it all together, the full picture of code looks like this (alarm.go):\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;github.com/spf13/viper\u0026#34; \u0026#34;github.com/stianeikeland/go-rpio\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/signal\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;time\u0026#34; ) // New Relic Incident API type (just using including two of the fields as an example) type Incident struct { Id int `json:\u0026#34;id\u0026#34;` OpenedAt int64 `json:\u0026#34;opened_at\u0026#34;` } // New Relic Incident API response type, which we will assess on having any items in the array for the alarm. type IncidentsResponse struct { Incidents []Incident `json:\u0026#34;incidents\u0026#34;` } // Documented here: https://rpm.newrelic.com/api/explore/alerts_incidents/list func hasOpenIncidents(apiKey string) bool { url := \u0026#34;https://api.newrelic.com/v2/alerts_incidents.json?only_open=true\u0026#34; spaceClient := http.Client{ Timeout: time.Second * 15, } req, err := http.NewRequest(http.MethodGet, url, nil) if err != nil { log.Fatal(err) } req.Header.Set(\u0026#34;X-Api-Key\u0026#34;, apiKey) res, getErr := spaceClient.Do(req) // If there is a failure to calling New Relic (ex. timeout), simply logging and returning back \t// for a later retry \tif getErr != nil { log.Printf(\u0026#34;Failed to get a response: %s\u0026#34;, getErr) return false } incidentsResponse := IncidentsResponse{} jsonError := json.NewDecoder(res.Body).Decode(\u0026amp;incidentsResponse) if jsonError != nil { log.Fatal(jsonError) } return len(incidentsResponse.Incidents) \u0026gt; 0 } // Simple handler to set the pin to a LOW signal when terminating the application func closeHandler(pin rpio.Pin) { c := make(chan os.Signal, 1) signal.Notify(c, os.Interrupt, syscall.SIGTERM) go func() { \u0026lt;-c log.Print(\u0026#34;SIGTERM detected, setting pin off\u0026#34;) pin.Low() os.Exit(0) }() } func main() { // Load configuration \tviper.SetConfigName(\u0026#34;config\u0026#34;) viper.AddConfigPath(\u0026#34;.\u0026#34;) configErr := viper.ReadInConfig() if configErr != nil { log.Panicf(\u0026#34;Fatal error config file: %s \\n\u0026#34;, configErr) } apiKey := viper.GetString(\u0026#34;new_relic.api_key\u0026#34;) // Setup GPIO pin \tlog.Print(\u0026#34;Opening GPIO\u0026#34;) err := rpio.Open() if err != nil { log.Panic(\u0026#34;Unable to open GPIO\u0026#34;, err.Error()) } defer rpio.Close() // Mapping to BCM2835 pin 18, which is the physical 12 pin. For more information about pin mapping, you can see  // how these are being mapped: https://github.com/stianeikeland/go-rpio/blob/v4.4.0/rpio.go#L35-L59 \tpin := rpio.Pin(18) pin.Output() closeHandler(pin) for { // If it is after-hours, don\u0026#39;t worry about triggering the alarm (trigger during: Mon - Fri, 9 - 5) \tt := time.Now() if (t.Weekday() \u0026gt;= 1 \u0026amp;\u0026amp; t.Weekday() \u0026lt;= 5) \u0026amp;\u0026amp; (t.Hour() \u0026lt; 9 || t.Hour() \u0026gt; 17) { pin.Low() log.Print(\u0026#34;After hours, sleeping for 5 minutes before continuing...\u0026#34;) time.Sleep(5 * time.Minute) continue } // If there are any open New Relic incidents, set the pin to high \tif hasOpenIncidents(apiKey) { log.Print(\u0026#34;Incidents detected, setting alarm.\u0026#34;) pin.High() } else { log.Print(\u0026#34;No incidents detected.\u0026#34;) pin.Low() } time.Sleep(1 * time.Minute) } } Building For my example, I have this in a alarm.go file within my nr-pi-alarm directory. I then issue the following command:\nenv GOOS=linux GOARCH=arm GOARM=5 go build This will produce a nr-pi-alarm you can then transfer to your Raspberry Pi for execution. One example of doing this is with scp:\n# Will transfer it to the alarm directory for the pi user on the Raspberry Pi scp nr-pi-alarm pi@INSERT_YOUR_RASPBERRY_PI_IP_ADDRESS:alarm Wiring it up For this example, we are using the physical pins #12 and #14 (GPIO18 and GND). We will use our jumper wires to then hook this to the AC switch:\n     \nYou will then hook your alarm light to the AC switch (PowerSwitch Tail). If the alarm light has its own on/off switch, turn it to on, as we don\u0026rsquo;t want this manual switch to block what our Raspberry Pi is going to control based on the flow of power.\n   Run it After you have transferred the build to your pi, you can then configure the alarm to use your New Relic account. This is achieved by creating a config.yml file in your alarm directory which currently hosts the nr-pi-alarm program (set to 600 for file permissions):\nnew_relic: api_key: INSERT_NEW_RELIC_API_KEY_HERE After you have configured it, simply invoke this to run the alarm:\n$ ./nr-pi-alarm 2019/03/14 10:04:12 Opening GPIO 2019/03/14 10:04:26 Incidents detected, setting alarm.    Happy Pi Day We hope you are having a great Pi Day and maybe this example will give you other ideas of what you can build to bring your systems alive. 😀\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/raspberry_pi/",
        "title": "raspberry_pi",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/caitie-oder/",
        "title": "Caitie Oder",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/hackathon/",
        "title": "hackathon",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/shipit-xiv/",
        "title": "ShipIt XIV",
        "tags": ["culture", "engineering", "hackathon", "shipit"],
        "description": "",
        "content": "It is winter time in Kansas City, which also included our fourteenth edition of ShipIt, our routine hackathon competition we host at Cerner Engineering. Although the snowy weather may have kept participants between the warm walls of our Innovations Campus, the winter storm did not disturb the initiative and enthusiasm of the fourteen participating teams. Check out the video below to hear from ShipIt Day participants and supporters as to why ShipIt Day has become an engineering-wide program that supports Cerner’s development culture.\n  The Winning ShipIt Day XIV Teams 1st Place- Bravo Avocado (Max Schroeder, Jacob Zimmermann, Jan Monterrubio)\n    “We created \u0026ldquo;HCCgle\u0026rdquo; (pronounced \u0026ldquo;WHO-gull\u0026rdquo;), a search application for looking up HCC information by ICD-10 diagnosis code (e.g. - \u0026ldquo;E13.11\u0026rdquo;) or by term (e.g. - \u0026ldquo;Diabetes mellitus\u0026rdquo;), including across physician-friendly terminologies like IMO and SNOMED CT. The app presented which HCC categories the requested term belongs to (if any) and shows how its categorization in the HCC specification has changed over time between revisions. This was created using Java, React, and DropWizard as well as an Oracle database. Our primary use case for this project is for aiding support troubleshooting, as there was not a good way to find the HCC codes used in production.” – Bravo Avocado\n 2nd Place- Risky Salt (Kevin Eilers, Ryan Rickard, Pepper Pancoast)\n    “We created an innovation/strategic roadmap prototype for Cerner\u0026rsquo;s clients that shows past, current and future projects, along with crucial data points for each. The data was directly from a Microsoft SQL server that houses all of our project management data, and the application was built on Ruby on Rails and React.”– Risky Salt\n 3rd Place- All the Data (Taylor Clay, Bilal Ahmad, Eric Ringle)\n    “Our team worked on a prototype of a real-time flow sheet, that was a customizable data visualization tool for patient device data. In the real world, this would be used to view trends in a patient’s health to proactively identify risks. The goal of this project was to build a prototype UI support a dense flow sheet and graphical view of critical care data. This would include Cerner CareAware iBus and Cerner Millennium data sources, as well as, understand complexity to better gauge what functions to bring into the solution. This was created using Node-RED (created a mock service that published data to the app) and React (the application UI).” – All the Data\n People’s Choice Award Winners Best Team Name - :(){:|:\u0026amp;};: (Ian Kottman, Sam Livingston, Paul Dennis, Bobby Ryterski, Anthony Ross)\n   Best Presentation - Ship O\u0026rsquo;Holics (Sowmya Mathukumalli, Shrutha Kashyap, Yasho Jhamvar, Tejendra Velaga)\n  \nThank you to our talented judges Ben York, Adilson Ribeiro, and Scott Julius. Thank you to Kyle Harper for taking awesome pictures throughout our event.\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/conference/",
        "title": "conference",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/devcon-recap/",
        "title": "DevCon: A tech conference for engineering, by engineering",
        "tags": ["culture", "engineering", "conference"],
        "description": "",
        "content": "In 2010 a group of Cerner engineers went to their leadership with an idea: a Cerner tech conference just for the internal development community, built and run from the ground up. The idea for a conference like this had been brewing at Cerner for several years. It was seen as an opportunity to create a large scale, sharing and learning experience for associates in Cerner\u0026rsquo;s technical workforces.\nFrom the beginning, buy-in from leadership was essential for the conference to succeed. In their original pitch, these engineers demonstrated how DevCon would create opportunities for knowledge sharing, networking, and collapsing long standing organizational silos by placing the teaching and learning opportunities into associates’ hands. The idea of associates getting together to share experiences and lessons learned around topics they were genuinely passionate about resonated right away as an intrinsic benefit to the company. Leadership’s support combined with the efforts of associates driving the conference forward year to year have pushed DevCon to an entirely new level since 2011.\nThe first DevCon had 600 attendees, all in Kansas City; now DevCon spans three locations - Kansas City, MO; Malvern, PA; and Bangalore, India - and in 2018 hosted over 7,500 attendees. The three experiences included 25 corporate sponsors, over 150 unique, associate-led sessions, and 5 keynote sessions delivered by industry thought leaders. Each year involves a new quirky visual theme, ranging from 8-bit vintage video games to large film and book franchises like Star Wars and Marvel Comics.\nIn addition to the keynotes and breakout sessions, attendees are given the opportunity to mingle and network with coworkers through trivia night, a social, lightning talks, and quick breaks for lunch and snacks throughout the day. Also, DevCon is fun. It’s two days to step away from your desk and become immersed in a creative, accepting, and open environment.\n  DevCon is a great example of the work that can be done across a wide variety of groups collaborating, allowing us to create something amazing. Some of these groups include:\n Presenters. Cerner has an amazing community of people who step up and share their stories, trials, ideas, and new information. The presentations are the core of what makes this conference a success and topic tracks range from devops and experiences with bleeding edge programming languages/frameworks, to people skills and UX design. A group of user experience designers that put an impressive level of thought and consideration into the aesthetics of the space, the flow of the event, and the promotional materials used before, during, and after the event. Organizers and volunteers who put their heart into choosing the right talks, training presenters, recruiting keynote speakers, choosing menus, organizing activities, marketing sessions, and helping attendees be at the right place at the right time. It takes a lot of work on the ground to get this thing going, then to successfully recreate it year after year.  See some of our favorite DevCon talks over the years on our CernerEng YouTube channel:\n  The Power of Pranks with Carl Chesser and Cornel Codrea\n  Leagues of Sea \u0026amp; Sky with Dr. Jeff Norris\n  Live Coding the Intersection Between the Arts, Research and Education with Sam Aaron\n  Don\u0026rsquo;t Blink - a Pursuit in Cognitive Bias with Brandy Poiry \u0026amp; Michelle Brush\n  Less Risk Through Greater Humanity with Dave Rensin\n  "
    },
    {
        "uri": "https://engineering.cerner.com/authors/melanie-taylor/",
        "title": "Melanie Taylor",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/2-to-the-5th-coding-competition-2018/",
        "title": "2^5 Coding Competition 2018",
        "tags": ["culture", "engineering"],
        "description": "",
        "content": "At Cerner, associates are always encouraged to develop tools and methods to improve their productivity. With this year\u0026rsquo;s theme being \u0026ldquo;Engineering Productivity,\u0026rdquo; Cerner\u0026rsquo;s fourth annual 2^5 Coding Competition provided associates another opportunity to dive into this subject and get involved in Cerner\u0026rsquo;s development culture.\nAs usual, this year\u0026rsquo;s 2^5 Coding Competition was kicked off alongside Programmer\u0026rsquo;s Day festivities on September 13th. The competition lasts 32 days, and associates are encouraged to submit code every day. During this year\u0026rsquo;s competition, over 200 code snippet submissions in a total of 59 different languages marked another successful run of the competition. This year the participants were judged based on the following categories:\n Best Representation: What captures the concept of \u0026ldquo;engineering productivity\u0026rdquo; the best? Most Obfuscated: Which code snippet was the most difficult to understand? Greatest Variety in Languages: Which repository contained the most variety in programming languages across submissions?  Even though each submission had to be 32 lines of code or less, it didn\u0026rsquo;t stop the participants from brainstorming and executing great ideas ranging from Jenkins Pipelines, creating shell commands, and automating tasks for the tools we use every day. The many entries received exemplified how easy it is to increase productivity in an engineer\u0026rsquo;s daily life with just a few lines of code.\n   My Experience As a participant, my biggest takeaway from this event was the learning experience it provided me. I was able to not only expand my skill set by working on the projects that I was passionate about, but also learn about different technologies. I found myself often inspired by reading through other participants' submissions. The restriction of 32 lines of code drove all of us as participants to write clean and concise solutions. It also made learning from each other\u0026rsquo;s code snippets fun and straightforward. This event also helped me understand how easy it is to find new project ideas. Some ways participants could easily get inspired include discovering a new library heard about in a Tech Talk, a cool new framework that your team started using, or even a complaint about how difficult it is to use a tool. All these things can inspire and drive participants to develop projects that are going to enhance their own personal knowledge about the new technologies or even help improve other people\u0026rsquo;s work efficiency.\nBy combining the newly learned technologies and the inspirations I found, I was able to develop tools that make my daily work easier. By learning how to develop chrome extensions, I automated some steps in our timesheet submission system including automatically adding holiday timecode. Furthermore, by researching the ins and outs of shell scripting, I simplified various tasks in command line that would otherwise be complicated to execute. During the competition, I benefited from code snippets submitted by other 2^5 participants such as a bat file that enables users to open the GitHub repository URL from the local git repo. 2^5 provides a platform for anyone who wants to learn and share ideas, and it doesn\u0026rsquo;t require big time commitments. I believe it is a perfect opportunity for our engineers to get involved and innovate.\nThe Winners Best Representation: Jenny Saqiurila The judges agreed that Jenny\u0026rsquo;s collection of submissions were most closely aligned to the 2018 2^5 theme of \u0026ldquo;Engineering Productivity.” Specifically, the judges noted the high usability of her automated timesheet submission tool.\nMost Obfuscated: Mithun Singh Mithun captured the award for the Most Obfuscated submission using an esoteric programming language that was comprised of symbols that outputted \u0026ldquo;Hello World.” None of our judges were familiar with this language and found his submission impressive, yet confusing.\nGreatest Variety in Languages: Susmitha Anandarao As the 2^5 competition runs for 32 days, the maximum number of languages that could be used by a participant is 32 languages. Susmita was named this category winner because she submitted all 32 days worth of code and used 32 different languages.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/jenny-saqiurila/",
        "title": "Jenny Saqiurila",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/devculture-team/",
        "title": "DevCulture Team",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/shipit-xiii/",
        "title": "ShipIt XIII",
        "tags": ["culture", "engineering", "hackathon", "shipit"],
        "description": "",
        "content": "What does it take to be awarded the ShipIt Day Golden Keyboard?    ShipIt Day XIII concluded last week after an intense 24-hours of brainstorming, building, and presenting. Various projects were completed by Cerner associates across twelve diverse organizations. ShipIt Day XIII\u0026rsquo;s projects were some of the most technically advanced this competition has ever seen.\nAwarding the Golden Keyboard is no easy job for our judges. Back in 2014, the trophy was created to provide the winning team with a memento that was unique and grew with the competition. Every competition the new winning team adds a trinket to the golden keyboard, so their ShipIt Day victory carries on forever. If you are wondering what it takes to bring home the golden keyboard, look no further than ShipIt Day XIII\u0026rsquo;s first place team, Dreamy Cloudy People.\nLearn from the Winners! Dreamy Cloudy People is a team of five individuals: Brett Heroux, Murtuza Syed, Niranjan Kumar, Abhijit Rao, and Swati Kode. The team spent the 24-hour hackathon on a real time push notification mechanism for patient chart web application views using the WebSocket API. Traditionally, data retrieval had been on demand, using AJAX and XMLHttpRequest. Previously, clinicians had to refresh their browser to receive updates to clinical data when reviewing a patient\u0026rsquo;s chart.\nWith this project, the WebSocket API was leveraged to allow a server to push updates to clients as events were triggered. The web application then updated its views without needing a user to manually refresh the page. They stood up a Java service for web clients to create WebSocket connections and send/receive data. It was also used to broadcast events involving patient data, such as new lab results or orders. The client application uses an existing JavaScript framework, part of Cerner Millennium MPages, to create socket connections and update the state of the application in real time.\n   During their presentation, Dreamy Cloudy People successfully pitched their improvement to the judges. After the presentation, one of our judges mentioned:\n Improving communication is always impactful in a clinical setting and improving to near real time is a great enhancement!\n There are several criteria that teams must showcase to prove their project is worthy of the Golden Keyboard:\n The project is well designed and captured everything that needed to be included. The project is technically difficult and tested the team\u0026rsquo;s technical knowledge, often leading to learning a new skill. The presentation shares the value potential and impact that the improvement/creation has to Cerner, associates, or our stakeholders.  Last but not least, a winning ShipIt Day team always makes sure to enjoy the event. One Dreamy Cloudy People team member, Niranjan Kumar, stated his favorite part of ShipIt Day was having \u0026ldquo;the opportunity to focus on projects not directly related to my primary role\u0026rdquo; and the \u0026ldquo;collaboration with peers.\u0026rdquo; Congrats to our first-place team!\nAdditional ShipIt XIII Winners 2nd Place: No amount of Pepto can stop this ingestion! Team members: Kevin Dunn, Scott Levander, Jeff Koehler, Dave Morgan, Rushabh Shah\n   This team designed a MS SQL Server database to track files received from clients into HealtheIntent and determine the latency since the last file arrived, as well as file size. They also created configuration tables to define expected file latency, acceptable thresholds before triggering an alert, and tables to track the people who want to receive an alert. Feeding off of that data, the team also developed an email alerting service (in C#), Tableau visualization dashboards for internal project investigations, and a Web UI to manage configurations (using PHP/SQL Server).\n3rd Place: Overnight Shippers! Team Members: Varun Kumar Chepuri, Tejaswi Gade, Ed Jalili, Neil Pathare\n   Currently, there are issues that are being reported on Splunk and there is a lot of manual effort to log JIRAs for them and then, developers need to track those issues while making any code changes. The JIRA backlog is building up and there is very little to no effort in cleaning up that backlog or working on fixing those issues. Issues logged in Splunk are not being fixed and these errors or defects are not caught early in the lifecycle. These errors or defects are shipped out to clients impacting their workflow. Therefore, Overnight Shippers! created a service bot which uses Splunk to find errors or exceptions and creates an issue in GitHub. This service bot creates a comment on a Pull Request if a Splunk issue exists for the files that are changed. Other capabilities include the capability to configure the bot per Github repository, log GitHub issues periodically, and support various project types.\nPeople Choice Awards Best Project: Centurions\n   Best Presentation: Team Rhinos\n   Best Team Name: Overnight Shippers!\n  \nTime to start preparing for ShipIt Day 2019! Thank you to our talented judges Chuck Schneider, Justin Morrison, and Jim Dwyer for serving as our judges.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/brittni-kingston/",
        "title": "Brittni Kingston",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/devcon/",
        "title": "devcon",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/devculture-team/",
        "title": "Our DevCulture Team: Connecting Communities &amp; Empowering Teams to Invent the Future",
        "tags": ["engineering", "culture", "devcon"],
        "description": "",
        "content": "The tech industry is constantly evolving; for better or worse, there\u0026rsquo;s no way around it. Now, more than ever, having a culture that fosters collaboration, empowers associates, and supports innovation is vital to a company\u0026rsquo;s success in the face of such constant change. However, an effective culture must be grown organically while being supported by senior leadership. At Cerner, we recognize the challenges involved in growing an effective, rewarding, and fun culture.With this in mind, we\u0026rsquo;ve made intentional strategic investments to overcome them. We’ve taken a unique approach by establishing a team that focuses on growing our culture across dev and ops specifically. Our team is called the DevCulture Team.\n   Our Mission  We exist as a strategic lever to establish Cerner as a destination for top tech talent, helping shift workforce behavior to catalyze the change Cerner needs for the Next.\n Cerner associates consider themselves privileged to work on challenging problems in an important space, healthcare. In 2010-2011, associates across dev and ops began to realize how much more they could learn from and contribute to the broader tech industry. We started attending conferences and bringing back new insights on the opportunities available when companies focused on culture as a driver for hiring, retaining, and leveling up associates working on software.\nThis led to establishing DevCon (our internal Developers Conference) in 2011, sponsoring local tech meetups as well as large scale tech conferences, sharing our work through this Engineering Health blog platform, and establishing the DevCulture Team officially in 2012. Prior to 2012, all efforts were completed by our first team member as extra projects while gaining the justification for a full time position on this new team. Not long after, we shifted our focus from awareness to associate industry engagement, encouraging associates to speak at conferences and to contribute to open source. As the work continued to increase, the DevCulture Team and our Culture Councils (described in more detail below) have grown around the globe as we work to align our values and advance together.\nCurrently, we focus on 10,495 associates across Cerner globally which represents a third of the company.\n   So who do we target and how do we develop our strategies? We focus our programs and work on both sides of the Dev/Ops equation, including user experience and product strategy. We also uniquely report through development executives which gives us the ability to advocate for their perspectives, represent engineering, and align strategies as we partner with other organizations (such as Recruiting, Talent Development, Learning, Finance, etc.) to implement our programs and values.\nCulture Council In effort to have buy-in, perspectives, and engagement from all sides of the business, we established Culture Councils to help us define our strategic direction. Each year, members are selected from an open application process. These Councils around the globe meet regularly to discuss where we are, where we are going, and where we need to be in order to be successful culturally. From existing programs to identifying cultural gaps, the Councils’ support of our team makes all of our work possible.\nCurious about what type of programs we run? We thought you might be. Check out some of our favorites!\nDevCon Embodying our values, DevCon is the foundation of our development culture. Providing opportunities to learn, share, network, and grow, DevCon is an internal conference and now takes place in three locations each year: Kansas City, Philadelphia, and Bangalore.\nFrom industry leader keynote speakers to hours of associate led sessions and a geek trivia night, associates look forward to DevCon all year round. Watch DevCon highlight videos, keynote sessions, and some associate sessions on YouTube to get a feel for what this awesome conference entails.\nEach year, the DevCulture team partners with associates from various organizations and roles to form the Core Planning Team that guides the overall direction of the conference, decides the theme, selects the talks, and plans engaging activities for associates to participate in at the conference.\n   Engaging with the Industry We believe we have a lot to learn from and share back with the industry and local tech community. From agile methodologies to big data processing, we bring in industry experts on a monthly basis in our Tech Talk Series to share their knowledge and perspectives that help our associates learn and grow. You can subscribe to our YouTube Channel to get notified anytime we post one!\nWhile attending and sponsoring conferences helps us bring essential knowledge back to Cerner, we’re privileged to send associates out to speak at conferences each year around the world. Being accepted to these conferences recognizes our associates for their complex work and talent while sharing more about what we’re working on at Cerner. Follow us on Twitter to find out what conferences we’re attending, sponsoring, or speaking at and come say hello! As a fun example of one of our talks, check out Kevin Shekleton’s talk at Strange Loop 2017 on ‘The Security of Classic Game Consoles’.\nEngineering Director Jenni Syed and Engineering Vice President Kevin Shekleton present at FHIR DevDays 2018.\n   When we consume open source in our software, we have the opportunity to leverage the talent from a broad cross section of the industry to make our solutions better. We also encourage associates to contribute to open source software both internally and externally. Learn more about the work we’re doing in Open Source on our Blog.\nShipIt Day Modeled after Atlassian’s ShipIt Day, we frequently host these 24 hour hackathons throughout the year in Kansas City, Malvern, Bangalore, and Brasov. We believe this encourages ingenuity and collaboration by promoting cross-pollination of ideas and teams as they solve problems together. Teams in KC compete for the coveted Golden Keyboard and Golden Mouse trophies that travel the campus, each winning team adding a trinket to the trophy representing their unique winning project. Learn about our most recent ShipIt Day winners on this Engineering Health blog.\nTeams present their projects to a panel of judges at the conclusion of the hackathon to determine the winners.\n   Our Work Can Sometimes Be Quite Odd. Amidst the strategic work in advancing our culture, we have found ourselves completing work for things most people would never consider to be a small part of their full time adult day job. Some of our favorite odd job memories from the past six years include:\n Ordering over 34,000 Lego bricks for DevCon 2014 (following multiple strategy meetings and spreadsheets to determine how many would be necessary for our 3,000+ attendees) Researching backstories on Marvel characters for DevCon 2017 Watching Doctor Who, Star Trek, Star Wars, Harry Potter, Lord of the Rings, etc. as part of our Training Requirements. We’re also required to pass a test covering Marvel vs. DC Characters. This is all to help ensure our team understand various cultural reference points that are popular amongst our target audience. Building an elaborate patchwork of Lego shapes on a giant Lego board to serve as a background for an all company meeting Renting a uHaul to transport old couches, coffee tables, TV stands, and TV’s we purchased from thrift stores in order to create retro living room spaces at DevCon 2013 where associates played video games between sessions.  Prior to moving into our brand new Kansas City Innovations Campus in 2017, our team made a video to demonstrate all the ways the desks can be configured to showcase the new setup. To avoid it being boring, we dressed up with various costumes we had on hand from other programs. See: Innovations Campus\n  \nWe believe culture is a journey, not a destination, and something we will always have to invest in and improve upon. These programs and efforts mentioned just scratch the surface of what we’re working on, with other projects ranging from how we can increase manager effectiveness to influencing our growth in continuous delivery models. Culture doesn\u0026rsquo;t change overnight, we\u0026rsquo;re iterating over ours with new programs based on regular feedback, that must be supported from within, and/or that we feel it\u0026rsquo;s important enough to truly invest in it.\nInterested in starting your own culture team? Great idea! We started out with smaller programs and projects to prove their effect and as they grew, documented the amount of work and any measurements we could to provide justification to have our first full time team member in 2012. Measuring cultural advancement can be challenging. We have done this by pulling engagement rates across different roles and organizations to gauge what is or isn’t effective, surveys following program events, and collecting anecdotal evidence of effect of programs (i.e. our team implemented these new SRE approaches/strategies because of Dave Rensin\u0026rsquo;s keynote at DevCon last month).\nJoin us on Twitter to stay up to date with all our cultural programs.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/andy-nelson/",
        "title": "Andy Nelson",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/cybersecurity/",
        "title": "cybersecurity",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/culture-of-cybersecurity/",
        "title": "Infusing a Culture of Cybersecurity within Cerner Engineering",
        "tags": ["engineering", "cybersecurity"],
        "description": "",
        "content": "With October being Cybersecurity Awareness month, we thought it would be a good time to reflect on some of the things we do in engineering to educate our associates and infuse security into our culture. We have over 28,000 associates worldwide supporting hundreds of solutions with millions of lines of code. Each associate has a specialization, such as software development, system support, and consulting. Keeping everyone up to date on the latest in security is a difficult task.\nSo how do we do it? We have teams dedicated to security that work directly within engineering. These teams have various responsibilities such as ownership of scanning tools and vulnerability tracking. My team\u0026rsquo;s goal is to bridge the gap by injecting security as a first-class citizen in the software development lifecycle. When working with developers, you have to make the right thing to do the easy thing to do. This is no different when it comes to security. In order to make security easy, we scan, assess, and create a plan for our developers to remediate their vulnerabilities. We promote the tools for scanning, help teams understand the results, and identify fixes for vulnerabilities. We run a monthly cybersecurity meetup which we use as a venue for associates to speak and learn about varying security topics.\nAndy Nelson opening the September edition of the Cybersecurity meetup\n   Sebastian Brown presenting at the July edition.\n  \nWe also take advantage of opportunities like Cybersecurity Awareness month. We bridge organizational gaps to host a variety of security focused events, engaging associates in development, security, and operations to facilitate better relationships and collaboration. Events like these lower the barrier to entry for our developers to learn more secure practices, and embrace and celebrate the progress we are making in our security journey. We kicked off the activities this month with an external tech talk from Britney Hommertzheim. Britney, the Director of Information Security at AMC Theatres, presented on how we can better integrate security teams and developers. It was a great talk and you can watch the talk on our YouTube channel. We invited another external speaker for our Cybersecurity meetup a few weeks ago too. Caleb Christopher, a Technical Business Adviser at Allegiant Technology, gave a great talk titled \u0026ldquo;Defeating Email Fraud with DMARC\u0026rdquo;. Along side those 2 events, we have held lunch and learns throughout the month, and are wrapping it all up with an hour of security focused lightning talks tomorrow.\nBritney Hommertzheim giving an external tech talk about integrating security across organizations\n      Security is not easy and we always have to strive to get better. Our development, operations, and security teams must work together, so we are doing our best to provide a forum for collaboration and sharing.\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/community/",
        "title": "community",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/denisse-osorio-de-large/",
        "title": "Denisse Osorio de Large",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/girl-scouts-cybersecurity/",
        "title": "Girl Scouts earn Cybersecurity Badge at Cerner",
        "tags": ["engineering", "cybersecurity", "girlscouts", "community"],
        "description": "",
        "content": "As a software engineering executive from Colombia, I thought that the best way to honor Hispanic Heritage Month (September 15-October 15) was to help the local Girl Scout Hispanic troops earn one of the new cybersecurity badges. October is Cybersecurity Awareness Month, having the activities nearing October seemed very fitting.\nGirl Scouts has been focusing on STEM badges for all ages with several new initiatives in the past couple of years. The cybersecurity badges are some of the latest additions to the growing list of STEM badges that girls can earn. I was excited to help the troops earn their badge and I wanted to make it very special and appealing to these girls, so I decided to run the event entirely in Spanish. I thought that this approach would be a great way not only to engage the girls, but also their family members who often accompany them to the events.\nThe badge was designed to be very interactive and engaging. The first activity was a game that helped the girls think about all the different kinds of electronic devices that exist, such as Fitbits, iPads, cell phones, TVs, etc, and how they interact with them in their daily life. The activity then focused on how security in everyday life can translate to security in cyberspace.\n   The next part of the event focused on helping the girls understand how much of our life is connected to the internet and how different our life would be if it wasn’t so. The girls had fun describing how they would live without internet. They all agreed that life without internet would be challenging but they would still be able to read and go camping!\nTo help illustrate the concept of how layers of security are important, the girls drew on whiteboards how they could defend a castle. The girls were quite inventive on this interactive activity!\nThe girls secured their “castles” effectively.\n   One of the most technologically advanced castles included dragons, a force-field and a keypad for the password.\n   The subsequent area of emphasis was regarding how electronic devices communicate. It was interesting to break down networking concepts like TCP/IP, network topology, and firewalls to a group of 1stand 2ndgraders, but I made these concepts relatable. IP packets were explained by building key chains using beads that spelled “brownies.” The activity involved the girls acting as senders, messengers and receivers, along with individual beads acting as packets with headers, contents, and other metadata.\nOne scout displays her finished IP packet keychain!\n   Safety rules were an integral part of the badge; the Brownies formed groups and made posters to help others understand the importance of the following internet rules:\n Don’t talk to strangers Stop and think before you act Don’t believe everything that you see or everything that people say  We wrapped up by touring our Innovations campus and highlighting how technology helps us improve our health and the health of our communities. Overall, this was another phenomenal opportunity for the Girl Scouts to engage with Cerner. It exposed them to key technology concepts and it expanded the prevalence of STEM education throughout the community.\nTo see more about Denisse’s involvement in the Hispanic Girl Scout community, check out this blog from 2017. Denisse has played a vital role in bringing STEM opportunities to the local troops. In fact, she was recently recognized by the Hispanic Chamber of Commerce of Greater Kansas City for her philanthropic efforts by receiving the STEM and Education Nuestra Latina 2018 Award. Congratulations, Denisse!\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/girlscouts/",
        "title": "girlscouts",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/jenkins/",
        "title": "jenkins",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/naresh-rayapati/",
        "title": "Naresh Rayapati",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/ssh-steps-for-jenkins-pipeline/",
        "title": "SSH Steps for Jenkins Pipeline",
        "tags": ["engineering", "jenkins"],
        "description": "",
        "content": "Pipeline-as-code or defining the deployment pipeline through code rather than manual job creation through UI, provides tremendous benefits for teams automating builds and deployment infrastructure across their environments.\n Source of image\n   Jenkins Pipelines Jenkins is a well-known open source continuous integration and continuous deployment automation tool. With the latest 2.0 release, Jenkins introduced the Workflow plugin that implements Pipeline-as-code. This plugin lets you define delivery pipelines using concise scripts which deal elegantly with jobs involving persistence and asynchrony.\nThe Pipeline-as-code\u0026rsquo;s script is also known as a Jenkinsfile.\nJenkinsfiles uses a domain specific language syntax based on the Groovy programming language. They are persistent files which can be checked in and version-controlled along with the rest of their project source code. This file can contain the complete set of encoded steps (steps, nodes, and stages) necessary to define the entire application life-cycle, becoming the intersecting point between development and operations.\nMissing piece of the puzzle One of the most common steps defined in a basic pipeline workflow is the Deploy step. The deployment stage encompasses everything from publishing build artifacts to pushing code into pre-production and production environments. This deployment stage usually involves both development and operations teams logging onto various remote nodes to run commands and/or scripts to deploy code and configuration. While there are a couple of existing ssh plugins for Jenkins, they currently don\u0026rsquo;t support the functionality such as logging into nodes for pipelines. Thus, there was a need for a plugin that supports these steps.\nIntroducing SSH Steps    Recently, our team consisting of Gabe Henkes, Wuchen Wang and myself started working on a project to automate deployments through Jenkins pipelines to help facilitate running commands on over one thousand nodes. We looked at several options including existing plugins, internal shared Jenkins libraries, and others. In the end, we felt it was best to create and open source a plugin to fill this gap so that it can be used across Cerner and beyond.\nThe initial version of this new plugin SSH Steps supports the following:\n sshCommand: Executes the given command on a remote node. sshScript: Executes the given shell script on a remote node. sshGet: Gets a file/directory from the remote node to current workspace. sshPut: Puts a file/directory from the current workspace to remote node. sshRemove: Removes a file/directory from the remote node.  Usage Below is a simple demonstration on how to use above steps. More documentation can be found on GitHub.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  def remote = [:] remote.name = \u0026#34;node\u0026#34; remote.host = \u0026#34;node.abc.com\u0026#34; remote.allowAnyHosts = true node { withCredentials([usernamePassword(credentialsId: \u0026#39;sshUserAcct\u0026#39;, passwordVariable: \u0026#39;password\u0026#39;, usernameVariable: \u0026#39;userName\u0026#39;)]) { remote.user = userName remote.password = password stage(\u0026#34;SSH Steps Rocks!\u0026#34;) { writeFile file: \u0026#39;test.sh\u0026#39;, text: \u0026#39;ls\u0026#39; sshCommand remote: remote, command: \u0026#39;for i in {1..5}; do echo -n \\\u0026#34;Loop \\$i \\\u0026#34;; date ; sleep 1; done\u0026#39; sshScript remote: remote, script: \u0026#39;test.sh\u0026#39; sshPut remote: remote, from: \u0026#39;test.sh\u0026#39;, into: \u0026#39;.\u0026#39; sshGet remote: remote, from: \u0026#39;test.sh\u0026#39;, into: \u0026#39;test_new.sh\u0026#39;, override: true sshRemove remote: remote, path: \u0026#39;test.sh\u0026#39; } } }   Configuring via YAML At Cerner, we always strive to have simple configuration files for CI/CD pipelines whenever possible. With that in mind, my team built a wrapper on top of these steps from this plugin. After some design and analysis, we came up with the following YAML structure to run commands across various remote groups:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40  config: credentials_id: sshUserAcct remote_groups: r_group_1: - name: node01 host: node01.abc.net - name: node02 host: node02.abc.net r_group_2: - name: node03 host: node03.abc.net command_groups: c_group_1: - commands: - \u0026#39;ls -lrt\u0026#39; - \u0026#39;whoami\u0026#39; - scripts: - \u0026#39;test.sh\u0026#39; c_group_2: - gets: - from: \u0026#39;test.sh\u0026#39; to: \u0026#39;test_new.sh\u0026#39; - puts: - from: \u0026#39;test.sh\u0026#39; to: \u0026#39;.\u0026#39; - removes: - \u0026#39;test.sh\u0026#39; steps: deploy: - remote_groups: - r_group_1 command_groups: - c_group_1 - remote_groups: - r_group_2 command_groups: - c_group_2   The above example runs commands from c_group_1 on remote nodes within r_group_1 in parallel before it moves on to the next group using sshUserAcct (from the Jenkins Credentials store) to logon to nodes.\nShared Pipeline Library We have created a shared pipeline library that contains a sshDeploy step to support the above mentioned YAML syntax. Below is the code snippet for the sshDeploy step from the library. The full version can be found here on Github.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  #!/usr/bin/groovy def call(String yamlName) { def yaml = readYaml file: yamlName withCredentials([usernamePassword(credentialsId: yaml.config.credentials_id, passwordVariable: \u0026#39;password\u0026#39;, usernameVariable: \u0026#39;userName\u0026#39;)]) { yaml.steps.each { stageName, step -\u0026gt; step.each { def remoteGroups = [:] def allRemotes = [] it.remote_groups.each { remoteGroups[it] = yaml.remotes.\u0026#34;$it\u0026#34; } def commandGroups = [:] it.command_groups.each { commandGroups[it] = yaml.commands.\u0026#34;$it\u0026#34; } def isSudo = false remoteGroups.each { remoteGroupName, remotes -\u0026gt; allRemotes += remotes.collect { remote -\u0026gt; if(!remote.name) remote.name = remote.host remote.user = userName remote.password = password remote.allowAnyHosts = true remote.groupName = remoteGroupName remote } } if(allRemotes) { if(allRemotes.size() \u0026gt; 1) { def stepsForParallel = allRemotes.collectEntries { remote -\u0026gt; [\u0026#34;${remote.groupName}-${remote.name}\u0026#34; : transformIntoStep(stageName, remote.groupName, remote, commandGroups)] } stage(stageName) { parallel stepsForParallel } } else { def remote = allRemotes.first() stage(stageName + \u0026#34;\\n\u0026#34; + remote.groupName + \u0026#34;-\u0026#34; + remote.name) { transformIntoStep(stageName, remote.groupName, remote, commandGroups).call() } } } } } } }   By using the step (as described in the snippet above) from this shared pipeline library, a Jenkinsfile can be reduced to:\n1 2 3 4 5 6  @Library(\u0026#39;ssh_deploy\u0026#39;) _ node { checkout scm sshDeploy(\u0026#39;dev/deploy.yml\u0026#39;); }   An example execution of the above pipeline code in Blue Ocean looks like this:\n   Wrapping up Steps from the SSH Steps Plugin are deliberately generic enough that they can be used for various other use-cases as well, not just for deploying code. Using SSH Steps has significantly reduced the time we spend on deployments and has given us the possibility of easily scaling our deployment workflows to various environments.\nHelp us make this plugin better by contributing. Whether it is adding or suggesting a new feature, bug fixes, or simply improving documentation, contributions are always welcome.\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/shipit-xii/",
        "title": "ShipIt XII",
        "tags": ["culture", "engineering", "hackathon", "shipit"],
        "description": "",
        "content": "   This month we held our quarterly hackathon event- called ShipIt Day. We would like to thank all the participants for making ShipIt Day XII a huge success. With 75 participants, we had associates representing 13 different organizations across Cerner. These associates were given 24-hours to meet the requirements of this hackathon which included making something that was innovative, usable, and value-adding. As the clock ticked down, our associates worked in their teams to complete a project while taking some time to enjoy the fun activities happening throughout ShipIt day.\n   It is an unwritten rule that the balance between fun and project time during ShipIt days should remain even. Associates are encouraged to enjoy the collaborative spaces of Cerner\u0026rsquo;s Innovations campus and participate in various activities such as ping-pong, eating snacks, playing games, and building relationships with other Cerner associates. Participants were given vouchers to use in the Innovations Café for breakfast and lunch. They were also served a fully-catered taco bar to stay energized for dinner.\nThis year, WarGames and ColecoVision gameplay were brought in for participants to enjoy throughout the evening. As said best by Cerner associate, Carl Chesser (@che55er):\n Software creation requires additional breaks… ShipIt at Cerner is no exception of this software craftmanship approach.\n The morning crept up on the teams quickly, and they had to begin preparing for their 4-minute presentation in the Assembly at Innovations campus. The goal of the team presentations is to explain the project, how it was implemented, and show a brief demo to the judges and audience. All the teams delivered competitive pitches, which made judging very difficult.\n   Huge thanks to Micah Whitacre, Nick Smith, Jon Miller, and Jared Moore for serving as our rockstar judges!\nOverall Winners The judges gathered together after the completion of the presentations to decide the top three performing teams. Congratulations to the following teams on your outstanding performance!\nFirst Place: Let\u0026rsquo;s Take Suggestions Team Members: Scott Grissom, Alex Harder, and Matt Nelson\n   This team was awarded the Golden Keyboard for their first-place achievement. They will defend the traveling trophy for the upcoming months and add a symbolic trinket to the collection for their team to be remembered in ShipIt history.\nThe focus of this project was to provide a mechanism for customized cloud-based ACL testing. Utilizing serverless functions through the OpenFaaS framework, the team enabled rapid development and deployment of serverless functions which test a specific type of network access, e.g. using cURL to access a ReST endpoint or a JDBC client to access a database instance. Deploying these functions to the same cloud instance which hosts services provides a more accurate testing of network dependencies from the perspective of a service as opposed to testing from a developer machine.\nSecond Place: #crowdercrowd Team Members: Ian Kottman, Heather Boveri, Robert Ryterski, and David Crowder\n   #crowdercrowd created a dashboard showing cluster utilization of applications across both on-premise and public cloud clusters. The metrics show how much CPU and memory an application is using versus how much it is requesting, along with an estimated yearly cost to run the application. This dashboard will be used to identify what applications can be scaled down to better fit their actual resource needs.\nThird Place: Bravo Avocado Team Members: Jan Monterrubio, Maximilian Schroeder, and Jacob Zimmermann\n   Bravo Avocado created a Maven plugin that starts a Docker container of a ReST service. It also allows the integration tests to dynamically connect to the correct URL and port of the container. This lets users run integration tests directly against the working code as part of the Maven lifecycle. Before the team wrote the plugin, they had to manually stand up the service and update the test configuration to validate any changes. By tying these eight steps to an existing command, users can streamline development for ReST services.\nPeople\u0026rsquo;s Choice Aside from our talented official judges, many associates join in on the ShipIt Day fun to act as peer judges in the People\u0026rsquo;s Choice category. The large audience during the ShipIt presentations was asked to get involved and vote for their favorite teams in certain categories.\nFavorite Team Name: Chef BoyarDeployments could be better (Steven Goldberg, Ryan Neff)\n   Favorite Project: Audio Bot (Mitali Paygude, Kunal Suryavanshi, Vinay Datta Pinnaka)\n   Best Presentation: Team Rhinos (Venkata Adusumilli, Veda Bhaskar Bhamidipati, Naga Prashanth Chanda, Sunand Kumar Matam, Prashanth Gajula)\n  \n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/internship/",
        "title": "internship",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/pratik-vaidya/",
        "title": "Pratik Vaidya",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/summer-of-engineering-health-intern-story/",
        "title": "Summer of Engineering Health: A Software Intern&#39;s Story",
        "tags": ["engineering", "internship"],
        "description": "",
        "content": "Pratik will be starting his Junior year at the University of Michigan this Fall studying Computer Science and Biomedical Engineering. This summer he’s a Software Intern on Cerner’s LightsOn Network team. This is the story of his summer experience:\nAs a college student, it\u0026rsquo;s not every day that you get to work on cutting edge technology to transform healthcare as we know it. Every morning, I walk past a wall on the way to the elevator: \u0026ldquo;Health care is too important to stay the same.\u0026rdquo; Simple enough, huh?\nHealthcare has been one of the few fields that has yet to fully embrace the possibilities new technology has afforded. Throughout my internship, there\u0026rsquo;s been an overall culture where associates understand they wouldn\u0026rsquo;t be working on something unless it was important. As a result, associates are visibly passionate and driven about the problems they\u0026rsquo;re working on, constantly striving to improve and add value. In my experience thus far, the environment at Cerner has been both engaging and supportive, allowing me to grow both personally and professionally as I strive to help improve healthcare and better the lives of others.\n   As interns, we had the opportunity to attend our two-day developer conference (DevCon) where we learned from talks on topics ranging from development technologies, innovating for maximum value, and understanding the perspectives of our clients when they use our solutions. I found it interesting to reflect on the differences between the \u0026ldquo;best solution\u0026rdquo; from a technology/development standpoint versus that of the end-user, maneuvering the product every day. More importantly, we discussed how we as engineers can provide users with opportunities to streamline their workflow and increase efficacy, without causing frustration among clients accustomed to a more traditional process. Changes need to be implemented gradually and seamlessly in order to get the most buy-in. Introduce change too fast and the users will be frustrated, regardless of the overall improvement.\nSmall Data, Big Picture This summer, I have had the opportunity to work as a Software Intern on the LightsOn Network team, which develops a solution that provides both internal associates and external clients with the pertinent information necessary to make organizational decisions, whether that be in workforce experience, system configuration optimization, or organizational value. I\u0026rsquo;ve been contributing to a project aimed at providing users with additional clarity about the status of their data at the metric, dashboard, and data source levels. The status indicators update in real-time to provide clients with the most up to date information about their data. Users can then examine the dashboards with this information in mind or drill down to identify what specific feed may be causing the issue to troubleshoot. The ability to immediately make such a large impact on clients around the world as an intern has been a humbling experience.\n   The internship experience thus far has not only helped me learn about useful development languages including Django, Angular, and JQuery, but also opened my eyes to the considerations taken into account when delivering a successful analytics product and software application in general. I\u0026rsquo;ve experienced the importance of breaking down the product into individual problems to target and then working on telling a clear story in the final product. All around me, the decisions made are not based on technology alone, but first truly consider the value added to the individual providers we aim to serve. By reducing inefficiencies in healthcare, we allow providers to do their jobs more effectively and in turn, allow more people access to healthcare.\nKey Takeaways  Built great connections with our team members and fellow interns Learned new programming languages, but more importantly, became more experienced with engineering practices in a highly regulated industry, as well as, the agile development framework Improved time management skills, especially optimizing my time usage when managing code reviews and future development tasks Learned first-hand the value of diving in deep and taking advantage of every opportunity possible Became inspired to continue applying computer science to the field of healthcare and constantly look for avenues to strengthen our solutions to ultimately benefit each patient Realized that improvements to healthcare technology doesn’t only affect a specific group of people, but each and every one of us - the impact is enormous.  "
    },
    {
        "uri": "https://engineering.cerner.com/tags/bunsen/",
        "title": "bunsen",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/fhir/",
        "title": "FHIR",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/ryan-brush/",
        "title": "Ryan Brush",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/data-engineering-with-bunsen/",
        "title": "Scalable Data Science with FHIR",
        "tags": ["engineering", "spark", "FHIR", "bunsen"],
        "description": "",
        "content": "The FHIR standard started as a better way to exchange healthcare data, but it also provides a solid basis for deep analytics and Machine Learning at scale. This post looks at an example from the recent FHIR DevDays conference that does just that. You can also run the interactive FHIR data engineering tutorial used in the conference yourself.\nOur first step is to bring FHIR data into a data lake \u0026ndash; a computational environment where our analysis can easily and efficiently work through petabytes of data. We\u0026rsquo;ll look at some patterns for doing so, with concrete examples using the open source Bunsen and Apache Spark projects.\nFHIR StructureDefinitions Define the Schema The schema for every dataset you see here was generated from a FHIR StructureDefinition. There is a big gap between building a FHIR-based schema by hand and generating it directly from the source. Every field in every query here is fully documented as a FHIR resource, making the FHIR documentation itself the primary reference to our datasets. This means the data is well-defined, curated, and familiar to those who have used FHIR.\nData Catalogs over Filesystems Organizing data in files and directories is convenient, but it becomes unwieldy when working with a large number of complex datasets. Data catalogs can meet this need \u0026ndash; and to offer a foundation for further data governance. The Apache Hive metastore is the most common way to catalog data in Hadoop-based environments and has native integration with Spark, so we organize data as one FHIR resource per table. Here\u0026rsquo;s an example from the tutorial used at FHIR DevDays:\n1 2  spark.sql(\u0026#39;use tutorial_small\u0026#39;) spark.sql(\u0026#39;show tables\u0026#39;).toPandas()   Which prints a table like this:\n   database tableName isTemporary     tutorial_small allergyintolerance false   tutorial_small careplan false   tutorial_small claim false   tutorial_small condition false    \u0026hellip;and so on. This makes it trivial to use intuitive database metaphors like use tutorial_small and select * from condition.\nFirst-class ValueSet Support FHIR ValueSets \u0026ndash; collections of code values for a specific purpose \u0026ndash; are essential to querying or working with FHIR data. Therefore they should be a first-class construct in our healthcare data lake. Here\u0026rsquo;s a look at using some FHIR valuesets in our queries as supported by Bunsen.\n1 2 3 4 5 6 7 8  from bunsen.stu3.valuesets import push_valuesets, valueset, isa_loinc, isa_snomed push_valuesets(spark, {\u0026#39;ldl\u0026#39; : isa_loinc(\u0026#39;18262-6\u0026#39;), # Loads LOINC descendants \u0026#39;hdl\u0026#39; : isa_loinc(\u0026#39;2085-9\u0026#39;), \u0026#39;hypertension\u0026#39; : isa_snomed(\u0026#39;38341003\u0026#39;), # Loads SNOMED descendants # Loaded from a FHIR ValueSet resource \u0026#39;chd\u0026#39; : valueset(\u0026#39;http://engineering.cerner.com/bunsen/example/chd\u0026#39;, \u0026#39;201806001\u0026#39;)});   Now we can use these valuesets in our SQL queries via the in_valueset user-defined function:\n1 2 3 4 5 6 7 8  spark.sql(\u0026#34;\u0026#34;\u0026#34; select subject.reference, code.coding[0].system system, code.coding[0].code code, onsetDateTime from condition where in_valueset(code, \u0026#39;chd\u0026#39;) \u0026#34;\u0026#34;\u0026#34;).limit(10).toPandas()      reference system code onsetDateTime     urn:uuid:f88c\u0026hellip; http://snomed.info/sct 53741008 2014-09-14T07:45:47   urn:uuid:d9ac\u0026hellip; http://snomed.info/sct 53741008 2017-05-22T06:56:19   urn:uuid:7460\u0026hellip; http://snomed.info/sct 53741008 1974-08-06T06:50:32   urn:uuid:5a28\u0026hellip; http://snomed.info/sct 53741008 2015-08-28T01:17:20    It\u0026rsquo;s worth looking at what\u0026rsquo;s going on here: in a few lines of SQL, we are going from the rich (but somewhat complicated) FHIR Condition data model to a simple table of onset times of Coronary Heart Disease conditions.\nFHIR Data in Columnar Storage Users see a clear catalog of FHIR datasets, but something important is happening behind the scenes. Most data stores or serialization encodings like JSON keep data in a row-wise format. This means all columns from a given record are physically adjacent on disk, like this:\n   This is a good fit for many workloads, but often not for analysis at scale. For instance, we may want to query the \u0026ldquo;code\u0026rdquo; column of several billion observation rows, and retrieve only those in a certain valueset. This is more efficient if columns are grouped together, like this:\n   This is completely transparent to the user; she simply sees FHIR data from the specification.\nSo while users see the FHIR data model, it is encoded in a columnar file like Parquet. In such files, all of these \u0026ldquo;code\u0026rdquo; columns next to one another, allowing the queries to do tight scans over columns of interest without expensive seeking past unneeded data.\nCreating For-Purpose Views These are the building blocks that simplify otherwise complex analysis. For instance, if we want to identify people with diabetes-related risks, we can create a collection of simple views of the underlying data customized for that purpose. You can see the full example in the Bunsen data engineering tutorial, but we\u0026rsquo;ll start with a dataframe of people with diabetes-related conditions as defined by a provided ValueSet:\n1 2 3 4 5 6 7 8 9 10  diabetes_conditions = spark.sql(\u0026#34;\u0026#34;\u0026#34; select id condition_id, subject.reference person_ref, coding.system, coding.code, coding.display from condition lateral view explode(code.coding) nested as coding where in_valueset(code, \u0026#39;diabetes_risks\u0026#39;) \u0026#34;\u0026#34;\u0026#34;)      condition_id person_ref system code display     urn:uuid:9c72\u0026hellip; urn:uuid:5a28\u0026hellip; http://snomed.info/sct 44054006 Diabetes   urn:uuid:56d5\u0026hellip; urn:uuid:214f\u0026hellip; http://snomed.info/sct 15777000 Prediabetes   urn:uuid:69de\u0026hellip; urn:uuid:7f4d\u0026hellip; http://snomed.info/sct 15777000 Prediabetes    We can inspect and validate this dataframe, and then move onto the next part of our analysis. Let\u0026rsquo;s say we want to exclude anyone who has had a wellness visit in the last two years from our analysis. We just build a dataframe with them:\n1 2 3 4 5 6 7 8  wellness_visits = spark.sql(\u0026#34;\u0026#34;\u0026#34; select subject.reference person_ref, period.start encounter_start, period.end encounter_end from encounter where class.code = \u0026#39;WELLNESS\u0026#39; and period.start \u0026gt; \u0026#39;2016\u0026#39; \u0026#34;\u0026#34;\u0026#34;)      person_ref encounter_start encounter_end     urn:uuid:f88c\u0026hellip; 2016-08-21T07:45:47 2016-08-21T07:45:47   urn:uuid:f88c\u0026hellip; 2017-08-27T07:45:47 2017-08-27T07:45:47   urn:uuid:d9ac\u0026hellip; 2016-05-16T06:56:19 2016-05-16T06:56:19    Now that we\u0026rsquo;ve loaded and analyzed our dataframes, we can simply exclude those with wellness visits by doing an anti join between them:\n1 2 3  diabetes_without_wellness = diabetes_conditions.join(wellness_visits, [\u0026#39;person_ref\u0026#39;], \u0026#39;left_anti\u0026#39;)   The result is a simple table containing the cohort we\u0026rsquo;re looking for! Check out the complete tutorial notebook for the full story.\nReproducible Results from Immutable Data Repeatability is an essential property for deep analysis. Re-running the same notebook in the future must load exactly the same data and produce exactly the same results. This gives us the controls needed to build on and iteratively improve previous analysis over time. Fortunately, using immutable data partitions are a common pattern in this type of system. We won\u0026rsquo;t go into depth here, but will touch on a couple good practices:\n Data is never mutated. Updates coming into our data lake are appended to previous data, and we can reproduce previous results by only working with data that was available at a given processing time. If necessary, a policy to archive or remove previous views of data from the data catalog is used to manage size.  Finally, building on such a FHIR-based data lake enables portability. The predictive model or analysis output is fully captured starting with portable data \u0026ndash; which means it can be more easily deployed into other systems. FHIR has made great progress in exchanging data in online systems, and we see a lot of promise for data science at scale as well.\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/spark/",
        "title": "spark",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/cerners-open-source-contributions-for-interoperability-developers/",
        "title": "Cerner&#39;s Open Source Contributions for Interoperability Developers",
        "tags": ["engineering", "open source", "interoperability"],
        "description": "",
        "content": "Open source and open standards encourage collaboration around innovation that advances the health care industry through improved interoperability. Developers across health care can come together and use open source code to share information, as well as develop and continually improve apps to support better health outcomes for patients.\nAt Cerner, developing open platforms that support interoperability standards like SMART® and FHIR® is integral to our mission of transforming health care. In addition to implementing these standards in our platforms, we also participate in organizations like HL7 and the Argonaut Project to help shape and develop these standards. Cerner attends and speaks at conferences on interoperability like FHIR DevDays this week in Boston, MA.\nIn addition to all of this work, we\u0026rsquo;re continually developing open source projects and work for the benefit of all interoperability developers. As we speak on and support interoperability this week in Boston for FHIR DevDays, we thought it would be great to highlight our current open source interoperability work over the past couple of years.\nSMART on FHIR Tutorial A couple of years ago, Cerner developed a tutorial walking developers through the process of creating their first SMART on FHIR application. With this hands-on tutorial, developers can write and deploy their SMART on FHIR application, running it against Cerner\u0026rsquo;s code Console and our FHIR Sandbox. Because SMART applications are interoperable with other vendors, the tutorial also walks the developer through running that same app against other sandboxes like the one provided by the SMART Health IT project. Over 700 developers have completed this tutorial as of June, 2018.\nYou can find the source code for the tutorial at https://github.com/cerner/smart-on-fhir-tutorial\nSMART on FHIR Apps in Embedded Browsers Over the years, Cerner has contributed to several changes to the popular open source JavaScript library (fhir-client.js, maintained by the SMART Health IT organization), used by many SMART app developers for launching and interacting with the FHIR API from their web-based SMART application. Running these types of applications within an embedded IE browser in a Windows application (like Cerner\u0026rsquo;s PowerChart® EHR) requires a bit of added effort to ensure SMART apps running concurrently do not exhibit critical patient safety issues. Cerner released an open source library, used in conjunction with fhir-client.js, to prevent these issues from happening. Our open source library is in use by SMART app developers running their apps not only within Cerner EHR environments, but also within other EHR vendor environments.\nYou can find the source code for this project at https://github.com/cerner/fhir-client-cerner-additions\nASCVD Risk Calculator SMART Application Through collaborative efforts, Cerner and Duke University Health System recognized a need for the development of a more clinically relevant cardiac risk calculator app. To solve for this, together we developed the open source ASCVD Risk Calculator SMART application. Not only is the app freely available to all Cerner clients, but all our work on the app is open source so that other hospitals and health systems can run and modify it to meet their needs.\nYou can find the ASCVD Risk Calculator source code at https://github.com/cerner/ascvd-risk-calculator\nCDS Hooks CDS Hooks is an emerging standard for interoperable clinical decision support. Similar to our SMART on FHIR tutorial, Cerner wrote an open source, hands-on tutorial that walks a developer through writing their first CDS Service.\nCerner is also the primary developer and maintainer of the CDS Hooks Sandbox, an open source Sandbox used by the CDS Hooks community. The CDS Hooks Sandbox has been proven to be incredibly valuable and helps developers test and demonstrate their CDS Services against any FHIR server of their choosing.\nYou can find the CDS Hooks Sandbox source code at https://github.com/cds-hooks/sandbox\nBunsen: FHIR and Big Data Big data can have a great impact on health care when used correctly. Health care organizations are analyzing big data to improve health care in a number of ways, from increasing revenue and improving efficiency to predicting diseases and improving patient care. As technology advance, health care organizations will continue to gather more data and developers will need to find ways to make that data useful and actionable.\nCerner has many years of experience working with big data. To support big data developers, we made Bunsen open source. Bunsen is a project that makes it easy for developers to explore, transform and analyze FHIR data with Apache Spark. In addition, Cerner recently released a tutorial for getting started with Bunsen. You can read more about Bunsen on our blog post here.\nYou can view the Bunsen source code at https://github.com/cerner/bunsen\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/interoperability/",
        "title": "interoperability",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/kevin-shekleton/",
        "title": "Kevin Shekleton",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/open-source/",
        "title": "open source",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/conferences/",
        "title": "conferences",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/jared-moore/",
        "title": "Jared Moore",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/srecon/",
        "title": "srecon",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/srecon18-recap/",
        "title": "SREcon18 Recap",
        "tags": ["srecon", "conferences", "engineering"],
        "description": "",
        "content": "This past spring, I had the pleasure of attending SREcon18 in Santa Clara, California. If you have never heard about SREcon or the term SRE then let me diverge for a moment to describe. SRE, or Site Reliability Engineer, was coined by Google employees back in 2003 when a team of software engineers were tasked with running a production environment. It\u0026rsquo;s the new hotness in the technology world, so an internet search will turn up a bunch of topics. If you are interested in learning more, I recommend sticking with the more trusted sources:\n Google (an obvious choice) SRE book SRE Wikipedia entry  Since SRE concepts have been codified, organized, and documented by the creators, the above resources are easy to explore without getting conflicting information from other entities who have taken the ideas and modified them to fit their organization. There is a lot of excitement in the industry around the SRE concept and there\u0026rsquo;s a lot of overlap between SRE and DevOps. Having been working in this industry for a while now, this is a much-needed evolution in the operations and development roles.\nSREcon started in 2014, so it is relatively new and is hosted by Usenix Association. Usenix has been hosting technical conferences since 1975 and one of their tenants is to host vendor-neutral events. I have been to a few vendor hosted conferences over the past few years and they are always heavy on the sales and light on the ideas. The sales aspect of these conferences got to the point where I was burned out and looking for something idea based. SREcon was the complete inverse, heavy on the ideas and light on the sales. It fit my need perfectly. My only complaint, and this is minor, was that most of the speakers selected this year were backed by big name tech companies, i.e. Google, Facebook, LinkedIn, Netflix, etc. A fellow attendee I was talking to mentioned that they enjoyed the company diversity this year over previous years, though. Apparently, the tone of the conference in previous years was Google heavy, i.e. \u0026ldquo;This is the Google Way\u0026rdquo; which is not surprising since that\u0026rsquo;s where it all started. The conference was being held in the heart of Silicon Valley therefore, the proximity made it reasonable to have many presenters come from companies in the valley. I fully expect to see more diversity in the coming years. SREcon19 was announced to be held in Brooklyn, NY, so I would not be surprised if there are many east coast companies represented. Additionally, I attended the SREcon Americas conference. Showcasing another aspect of diversity, there are SREcons in Europe/Middle East/Africa and Asia/Australia, so it\u0026rsquo;s become a world-wide conference.\nThe idea-sharing phenomenon of SREcon played an important role in my decision to attend, since I\u0026rsquo;m leading an automation development team in CernerWorks℠. CernerWorks is Cerner\u0026rsquo;s managed services organization responsible for hosting, managing, and monitoring our client\u0026rsquo;s systems in order to provide the most reliable, highest performing and cost effective delivery of technology services for healthcare. My team is focused on automation in the client aligned systems management space. That\u0026rsquo;s just a fancy way of saying that our focus is on the non-cloud solutions. We are focusing on solution provisioning, configuration management, and upgrades. The work we are doing aligns well with the tenants of SRE, DevOps and other philosophies that have been discussed in the past few years. I am very interested in these philosophies simply because change is hard, especially when it comes to changes in a culture with an established mentality of doing things a certain way. The technical part of our job is easier in my opinion since technology is always changing.\nSome of the more interesting sessions I attended talked about patterns and behaviors that we can learn from other industries. For example, the way that firefighters and other first responders react to incidents and the command structures they put in place outline ways that we can improve how we deal with incidents. Having a plan and structure when engaging with an incident has many benefits. It allows you to define what normal operations for your team look like versus what emergency operations looks like. Determining the chain of command and defining pre-set roles that anyone can fit into can help streamline the time to respond and the emergency command hierarchy of the team. Other industries, such as oil and gas refineries, must have strong contingency plans in place in order to not go boom when things go south. There are a lot of patterns, behaviors, and philosophies that we can learn from almost any industry out there if we open our eyes.\nA key topic for the conference is learning from failures and mistakes. The idea of a blameless postmortem was discussed at length in multiple talks and conversations. The need for such a thing stems from our human nature to want to assign blame and fault when dealing with problems. But in many ways this type of behavior causes actions to be hidden out of fear of retribution. A blameless postmortem comes from the idea that learning and preventing is more important than assigning blame. Etsy, for example, has an annual award for the employee who\u0026rsquo;s made the biggest mistake. The culture they are building is one where accidents are a source of learning rather than a source of embarrassment. This is quite an interesting concept.\nWorking in the technology industry and never having been to Silicon Valley was surreal. Seeing big company names on the side of buildings and realizing those were the corporate headquarters of companies whose software I have interacted with for years was an experience. One of the nights, I took a trip to the Googleplex to walk around the campus. It was a beautiful campus with trees and flowers everywhere and the weather was amazing. The campus was abuzz with people and activity that you would expect to take place at Google late at night. While walking a thought started forming in my mind around the type of work we do at Cerner. A thought that continued to form later at the conference when I spoke with development teams from various big-name tech companies. It was rewarding to talk about how we are using technology to make healthcare better. These conversations would usually end with the team from the big tech company expressing admiration, respect, and praise for the work we are doing. The Googles, Facebooks, Twitters, and the like are doing interesting things, and it\u0026rsquo;s easy to get distracted by those names, but the problems that we are solving have real impact on the lives of our fellow humans and we shouldn\u0026rsquo;t take that for granted.\nSREcon18 was worth the trip, but if you couldn\u0026rsquo;t attend this year\u0026rsquo;s, previous years, or future SREcons you don\u0026rsquo;t need to worry about missing out. Usenix has an open access policy which means that slides, videos, and audio of all the talks are available after of the conference. I strongly recommend reviewing the program for this year\u0026rsquo;s conference and watching any of the talks that catch your fancy.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/andie-young/",
        "title": "Andie Young",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/interns/",
        "title": "interns",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/jennifer-lambert/",
        "title": "Jennifer Lambert",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/joe-geris/",
        "title": "Joe Geris",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/say_hello_to_the_2018_summer_interns/",
        "title": "Say Hello to the 2018 Summer Interns",
        "tags": ["engineering", "interns"],
        "description": "",
        "content": "Throughout the month of May, teams across Cerner’s Engineering space have welcomed 197 software interns to our campuses in Kansas City and Malvern. Ranging from rising seniors in high school to rising seniors in college, these interns will be working alongside Cerner’s full time engineers to enhance and expand upon their current skillsets.\nWhile interning at Cerner, these associates will experience firsthand the look and feel of a day in the life of a full time software engineer. Potential projects include:\n working with data ETL and web technologies to produce analytical dashboards working on features and capabilities of our solutions to improve the overall experience of Cerner’s consumer solutions and/or development processes, and adding functionality to some cloud-based applications to provide more troubleshooting capabilities.  The variety in projects interns will be working on allows for work in a robust development ecosystem using tools like JIRA, Crucible, Github and languages like Java, Javascript, and Ruby. Our interns have opportunities to work on real projects with real world impact in the healthcare space.\nIn addition to their work, interns will attend a variety of culture and networking events like our internal developers conference, DevCon, a bi-weekly Tech Speakers Series, an intern-only hackathon, and a family day where interns are encouraged to bring their families to see their work environment and projects. Cerner has welcomed three intern groups in Kansas City and one in Malvern, with one more group to start in Kansas City next week. Stay tuned this summer for an in-depth look at a few of the projects our interns work on.\n        \n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/chris-fagyal/",
        "title": "Chris Fagyal",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/code-review/",
        "title": "code review",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/code-reviews-there-is-a-better-way/",
        "title": "Code Reviews: There is a Better Way",
        "tags": ["engineering", "code review"],
        "description": "",
        "content": "At Cerner, peer review of code is an important job. We have found it to greatly improve the quality of code and improve a team’s knowledge of the codebase. Through code review discussions, newer engineers are able to learn on valuable areas to question or challenge when something is being changed. While we heavily embrace the practice of peer code review, we also recognize it can make an engineer’s life challenging when you are overwhelmed with code to review. Furthermore, it can be large source of project delay when points of coordination are not managed well. In this blog, we will cover some tactics that you can apply to improve how you approach code reviews.\nSmaller code reviews are better code reviews    The first step of making a code review inviting for others to provide feedback, is by making the scope of the review small. This generally requires thought and planning on how to incrementally present code for review, allowing each review to be smaller in size which can be safely included into the codebase in isolation. This can ultimately help morale of a team by avoiding monster-size reviews from being stagnate, and never reaching a point of completion due to amount of work necessary to review or the length of feedback that exists. By having the review small, a laser set of focus can be achieved to promptly and effectively complete the review, providing a positive reinforcement of being able to also complete the work.\nHave machines filter what code qualifies for review We recently had a \u0026ldquo;GitHub Day\u0026rdquo; at our Innovations campus, where GitHub and other Cerner engineers shared how to improve your use of source code management. In these presentations, a quote was shared that rang true:\n We are more receptive to feedback from pedantic robots than pedantic people.\n When it comes to a code review, it is easy to point out simple flaws in the code which relates to formatting or basic anti-patterns in a given language. However, should humans be spending time on these types of issues in a code review? Machines are well suited for testing code through linters and static analysis, where formatting rules and known bad patterns can be quickly identified. By focusing on having your CI system do these types of tests on your code, and failing your build when formatting problems emerge, you now have a filter which blocks a large set of noise from entering into a code review. As a result, humans don’t engage in a code review until the code actually builds (which would encompass both static and dynamic forms of tests). Furthermore, it is easier to receive these minor points from a machine (a bot which is communicating the results of your build), versus having a team member spend time pointing out all the minor things you forgot to check or format correctly. In the end, humans apply their creative mind on discussing deeper topics on the design and intentions of the code, versus getting distracted with noise of simple things that can be addressed earlier by machines.\n   Agreed upon conventions Building upon the previous point, teams should have agreed upon conventions on things like style and code formatting, so that those things can be automatically found (and even fixed with the proper tooling). This eliminates the propensity for style/formatting comments in code reviews which detract from the actual purpose of finding substantive issues within the code. If you find code styling questions or debates are occurring in the code review, ensure the team comes to an agreement on the convention, and that styling rule gets fed into your linter (ex. CheckStyle, Rubocop, IDE formatters) to avoid it from entering into a code review again. Furthermore, this helps reinforce the “boy scout rule”: Always leave the campground cleaner than you found it. By having your linter now embrace this new formatting rule, existing codebases that you start changing in the future will be invited to be updated as well, versus allowing the drift to continue to occur.\n   \u0026ldquo;How I would have done it\u0026rdquo; When comments emerge on a code review which are being conveyed as opinions, like “I’d have done it this way,” this can detract the focus of the code review away from correctness and more on personal preferences. In this section we will discuss two challenges when including personal preferences in code reviews: the distractions and how they can be interpreted.\nDistraction on personal preferences While personal preferences may help the readability of the code (based on the viewer), it is also important to provide the larger “why” when sharing opinions. When multiple opinions are being shared within the review, it may invite for a larger debate that might simply derail the ultimate focus of the code review. As a result, the code review may languish and not be completed due to the debate of opinions.\nAccidental preference becoming \u0026ldquo;law\u0026rdquo; When opinions like this are shared, especially when communicated from someone more senior on the team, their opinion may be viewed as “law”. In that, it is simply accepted by the engineer to apply the change because the source of the opinion carries a large amount of weight in the discussion. However, it is expected that no personal preference would simply be applied without additional thought and reason. When personal preferences are brought up in a review, it is best to capture and understand the “why” versus assuming the opinion is simply true or accepting it as its face value.\nModerator All code reviews should have a moderator for the review to ensure the review doesn’t get off track (discussing items not applicable to the review, circular discussions, personal preference comments, etc). The moderator isn’t necessarily an engineer reviewing the code, but could be the technical project manager for the project where the code resides. The moderator is also responsible for ensuring comments are addressed and the review is closed when appropriate.\nReview agility Code reviews should be given priority, as the author is being blocked waiting for the review to be completed. In order to achieve review agility, code reviews must be small (as mentioned above), so that reviewers are able to process the content of the review and provide substantive feedback easily. In order to give reviews priority, your team should actively work to ensure people are planning accordingly when they are assigned to reviews. Based on the expertise on the team, it is not uncommon for some team members to become a bottleneck on the team due to amount of code reviews that are assigned to them. It is important for the team to identify this issue, and ensure the team is growing other team members to shed the load of reviews by building up the expertise.\nConclusion While peer code reviews are a crucial component of our software development process, they also require evaluation on how they can be improved. These are simply a few tactics that can be applied to help improve how you approach code reviews. As with the Agile process, we want to continuously improve how we build software, and by focusing on how to improve this process, you can ultimately improve the quality and delivery time of your software.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/kyle-harper/",
        "title": "Kyle Harper",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/shipit-xi-spring-2018/",
        "title": "ShipIt XI Day: Spring 2018",
        "tags": ["culture", "engineering", "hackathon", "shipit"],
        "description": "",
        "content": "   On March 8th and 9th, 17 teams of Cerner associates competed for the Golden Keyboard during our 11th ShipIt Day. During this 24-hour hackathon, associates were challenged to create something innovative, usable, and value-adding. This was the fourth ShipIt held at our new Innovations Campus, leveraging the wonderful collaborative spaces available.\nWith Shipit XI, we expanded upon a time-honored tradition of this event, the traveling trophy Golden Keyboard. This keyboard, adorned with symbolic trinkets from past winners, provides an opportunity for winning teams to defend the trophy and leave their mark in ShipIt history.\nGolden Keyboard\n   However, we realized as ShipIt grew, the category for winning the \u0026ldquo;People\u0026rsquo;s Choice Award\u0026rdquo; (the favorite as voted by the audience agnostic of the judges’ selection) needed to have something more special to symbolize its importance. As a result, we introduced the Golden Mouse! This trophy, while separate from the Golden Keyboard, is another peripheral that a team can win by vote of their peers. With ShipIt XI, the Golden Mouse is now added to the tradition.\nGolden Mouse\n   This ShipIt featured Reliability as its theme. This theme highlighted the work that is continually done to improve the reliability our systems. Reliability, it is the most fundamental feature! By including this theme, we also introduced a new winning category for best project that improved reliability.\nLike all ShipIts, signs of creativity, collaboration, and intense problem solving were abundant all night.\nRegistration and kick-off\n   Strategy planning\n   Focused concentration\n   Abundant caffeine\n  \nIn addition to café vouchers for breakfast and lunch, ShipIt XI featured a taco bar for dinner. Participants enjoyed unlimited tacos and delicious churros for dessert!\n   ShipIt XI also hosted a ping-pong “March Madness” tournament to provide an opportunity for the participants take a break and flaunt their athletic prowess.\nThe dreaded Jake Kramer power serve\n   Demos always arrive quicker than anticipated. Ready or not, 9am rolled around and demos were in full swing. Teams were given four minutes to present their projects to a panel of judges, consisting of Julie Schlabach, Kevin Shekleton, Jeff Dittmer, and Chad Todd. Special thank you to those folks for spending their morning with us and supporting ShipIt Day!\nTeam SPACES RUNNER – 1st Place\n   Coming in first place was Team SPACES RUNNER (Ryan Scheidter, Darius Washington, Miguel Fernandez, Aarthi Gunda, and Sundeepa Godavarti).\n“Space Runner is a visualization tool to show previously run, currently running, and scheduled to run Apache Spark jobs. It also provides some color coding for jobs that take over the average amount of time of previous runs, along with links to the Spark UI where we can view more details about the job. This tool is being used as a visualization of my team’s (Spaces Platform) dev domain to see what ran at what time, what is running, and what is about to start. This is a huge asset in terms of seeing how the domain is running, when it has too many things scheduled, and when it has free time coming up to manually run a job. This tool takes far less time than manually looking at the frequency jobs are scheduled and determining if the domain is free or overbooked.”\nCongratulations SPACES RUNNER!\nTeam PSA – 2nd Place\n   Second place went to Team PSA (Aaron Noll, Bryce McDonald, Jesse Gilbert, Bennet Lovejoy, and Snehil Wakchaure).\n“Team Patient Safety Analytics (PSA) developed a SMART on FHIR app with the aim of making hospitals safer and more reliable. This goal of this app is to decrease the time required to report patient safety events while also increasing accuracy by 1) Populating basic demographic fields automatically for a report (currently manually entered) and 2) Embedding the app within PowerChart to reduce the number of clicks necessary to create a report. Over time, the app could also enable Cerner to add more value by helping hospitals improve through analysis of their data and comparison against peers while also providing reporting utilities for clients to monitor and improve their processes.” Way to go PSA!\nTeam Pedantic Beasts – 3rd Place\n   Team Pedantic Beasts (Robert Ryterski, Paul Dennis, Ian Kottman, and Anthony Ross) took third place.\n“MCop is a linter for Marathon app definitions. It statically analyzes definitions for problems such as syntax errors, unstable deployment configuration, and departures from best practices.”\nGreat job Pedantic Beasts!\nTeam Reliabilibuddies – Best Reliability Themed Project and People’s Choice: Best Team Name\n   The winner of the sponsored project theme of “Reliability” was Team Reliabilibuddies (Siri Varma Vegiraju, Nathan Schile, Vu Tran, Lucas Chandler, and Venkatesh Sridharan). Reliabilibuddies also won the people’s choice Best Team Name!\nNicely done Reliabilibuddies!\nTeam The Centurions – People’s Choice: Best Project\n   Winning the brand new Golden Mouse for people’s choice Best Project was team The Centurions (Mohan Chamarthy, Sanket Korgaonkar, Saranth Govindaraju, Vamsi Krishna Guntupalli, and Shyam Gopal Rajanna). Congratulations and enjoy your cheeseballs!\nTeam Whalenado – People’s Choice: Best Presentation\n   Winning a glorious tub of cheeseballs is Team Whalenado (Mithun Singh, David Crowder, Nick Overfield, and Michael Rzepka) for people’s choice Best Presentation.\nWinning teams walked away with some awesome prizes such as wireless headphones, TileMates, Echo Dots, and drones.\n   ShipIt continues to be a great way for associates to explore new solutions to problems, build relationships within different organizations, and most importantly, enjoy themselves. ShipIt Day XI was a great success. The projects were creative and have potential to bring value Cerner’s associates and clients.\nSave the Date: ShipIt Day returns August 9th and 10th! If you are interested in learning more about other ShipIt Days, see these earlier posts:\n ShipIt Day X Winter 2017 ShipIt Day IX Summer 2017 ShipIt Day VIII Spring 2017 ShipIt Day VII Winter 2016 ShipIt Day Winter 2016 Highlight Reel ShipIt Day Fall 2016 ShipIt Day Spring 2016  "
    },
    {
        "uri": "https://engineering.cerner.com/tags/big-ip/",
        "title": "big-ip",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/cerner-and-icontrol/",
        "title": "Cerner and iControl",
        "tags": ["engineering", "big-ip", "f5"],
        "description": "",
        "content": "At Cerner, we manage our own infrastructure and a big part of that is managing F5 load balancers. If you are a Ruby developer or a network engineer and regularly work with F5s, then I have good news! I\u0026rsquo;ve created a gem that abstracts iControl\u0026rsquo;s REST API. It makes working with the API easy to understand and eases the effort required to automate changes across many devices.\n1 2  api = IcontrolRest::Client.new(host: \u0026#39;1.2.3.4\u0026#39;, user: \u0026#39;user\u0026#39;, pass: \u0026#39;pass\u0026#39;, verify_cert: false) =\u0026gt; \u0026lt;Icontrol::Client:0x007fb953ab7750 @options={ ... }\u0026gt;   After logging in, one can build a query via an instance method:\n1 2 3 4 5 6 7 8  api.get_sys_dns # returns an object like this: =\u0026gt; { \u0026#34;kind\u0026#34; =\u0026gt; \u0026#34;tm:sys:dns:dnsstate\u0026#34;, \u0026#34;selfLink\u0026#34; =\u0026gt; \u0026#34;https://localhost/mgmt/tm/sys/dns?ver=11.5.4\u0026#34;, \u0026#34;description\u0026#34; =\u0026gt; \u0026#34;configured-by-dhcp\u0026#34;, \u0026#34;nameServers\u0026#34; =\u0026gt; [\u0026#34;1.2.3.72\u0026#34;, \u0026#34;1.2.3.73\u0026#34;], \u0026#34;search\u0026#34; =\u0026gt; [\u0026#34;domain.com\u0026#34;] }   The gem builds REST queries given a method delimited by underscores. The first word is the REST method and the following words are the path you\u0026rsquo;d like to send your request to.\nFor example, if you\u0026rsquo;d like to get the version of the F5, given this iControl REST API doc\nYou\u0026rsquo;d call a method that looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  api.get_sys_version # returns an object like this: =\u0026gt; { \u0026#34;kind\u0026#34; =\u0026gt; \u0026#34;tm:sys:version:versionstats\u0026#34;, \u0026#34;selfLink\u0026#34; =\u0026gt; \u0026#34;https://localhost/mgmt/tm/sys/version?ver=12.1.2\u0026#34;, \u0026#34;entries\u0026#34; =\u0026gt; { \u0026#34;https://localhost/mgmt/tm/sys/version/0\u0026#34; =\u0026gt; { \u0026#34;nestedStats\u0026#34; =\u0026gt; { \u0026#34;entries\u0026#34; =\u0026gt; { \u0026#34;Build\u0026#34; =\u0026gt; { \u0026#34;description\u0026#34;=\u0026gt;\u0026#34;0.0.249\u0026#34; }, \u0026#34;Date\u0026#34; =\u0026gt; { \u0026#34;description\u0026#34;=\u0026gt;\u0026#34;Wed Nov 30 16:04:00 PST 2016\u0026#34; }, \u0026#34;Edition\u0026#34; =\u0026gt; { \u0026#34;description\u0026#34;=\u0026gt;\u0026#34;Final\u0026#34; }, \u0026#34;Product\u0026#34; =\u0026gt; { \u0026#34;description\u0026#34;=\u0026gt;\u0026#34;BIG-IP\u0026#34; }, \u0026#34;Title\u0026#34; =\u0026gt; { \u0026#34;description\u0026#34;=\u0026gt;\u0026#34;Main Package\u0026#34; }, \u0026#34;Version\u0026#34; =\u0026gt; { \u0026#34;description\u0026#34;=\u0026gt;\u0026#34;12.1.2\u0026#34; } } } } } }   So in practice you could put all this together to do something like audit the versions of F5s in a collection:\n1 2 3 4 5 6  [\u0026#39;hostname1\u0026#39;, \u0026#39;hostname2\u0026#39;, \u0026#39;hostname3\u0026#39;].each do |host| api = IcontrolRest::Client.new(host: host, user: \u0026#39;user\u0026#39;, pass: \u0026#39;pass\u0026#39;, verify_cert: false) result = api.get_sys_version version = result[\u0026#39;entries\u0026#39;][‘https://localhost/mgmt/tm/sys/version/0’][\u0026#39;nestedStats\u0026#39;][\u0026#39;entries\u0026#39;][\u0026#39;Version\u0026#39;][\u0026#39;description\u0026#39;] puts \u0026#34;host: #{host}| version: #{version}\u0026#34; unless version == ‘12.0.0’ end   This would print out the hostname and version of any F5s that aren\u0026rsquo;t on version 12.0.0. This is just one example, I encourage you to look through the iControl REST API docs to get a feeling for what\u0026rsquo;s really possible.\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/f5/",
        "title": "f5",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/jerrod-carpenter/",
        "title": "Jerrod Carpenter",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/lindsey-bromberg/",
        "title": "Lindsey Bromberg",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/one-year-calling-innovations-home/",
        "title": "One Year Calling Innovations Home",
        "tags": ["culture", "engineering"],
        "description": "",
        "content": "One year ago this month, Cerner development associates in Kansas City moved into a new place to call home for their workdays, Innovations Campus. The first two towers that stand on this campus provide 805,000 square feet of workspace for more than 3,000 associates. Eventually, the campus will grow larger, including 16 buildings and 16,000 Cerner associates. As that growth approaches, it is important to reflect on what these first two towers represent for associates and their development careers at Cerner. The investment in this workplace is a testament to Cerner’s dedication to and appreciation of associate values. It is a reflection of who the people at Cerner are and what they strive to be.\nMany of the features in the building were designed with input by associates, for associates. Other spaces and elements of the building were voted on by associates. A Momentum Wall, a series of installations that change and grow with Cerner’s culture and development platforms, capturing architectural history, open source projects and commits, a giant monthly calendar of events, and a continuously changing installation showcasing Cerner’s commitment to usability and solid design. Distinct “Neighborhood Nodes” were designed by committees of development associates to provide opportunities for associate to relax, learn, and create. The game rooms Bowser’s Castle and Flynn’s Arcade are a nod to the sci-fi and gaming culture popular among many development associates at Cerner. The Harry Potter and Lord of the Rings libraries house books and quiet work space for associates to get away and learn. The MakerSpace provides space, machinery and materials for associates to work on personal projects and think about work projects in a different way. Associates voted on various options for workstations and landed on a configurable option that provides flexibility in terms of work space and style. The Assembly provides a space for associates to share and learn, and creates opportunities for ambient conversations for those passing through a meetup or knowledge transfer session. On a normal workday at Innovations you can see how the physical elements embedded in the buildings contribute immensely to our development culture, from the collaborative layout of the campus with unique seating and whiteboards around every corner to unique representations of Cerner’s values and history composed in binary ascii. The functionality and aesthetics of the campus have enhanced the cultural experiences associates have had over the past year. In honor of the one year anniversary in Innovations, we’ve asked a few of our associates for their perspectives on the parts of the campus they interact with daily.\nAssociate Reviewer: Ann Dickey, Software Engineer  I love the MakerSpace Neighborhood Node. Not only because it’s given me a chance to learn and use tools I have never used before, but for the networking experience it has turned out to be. I’ve met dozens of associates across Cerner while babysitting a 3D print, or attending the Makerspace learning events the admins host.\nWe put together a group for ShipIt IX, where we used the MakerSpace to design and create a 3D printed notification template that hooked up with an Arduino and LED light strip we programmed to give the user notifications from their desired apps. ShipIt was a first for all of us, and we wouldn’t have had a project without the availability of the MakerSpace to do 3D printing and soldering. We all had a blast, learned a lot, and plan on doing another ShipIt in the future. Plus, now I have some great embarrassing photos of my team feeding each other ice cream and failing to properly use the outdoor exercise equipment because of the ShipIt scavenger hunt challenge. The funny thing is the MakerSpace is the one neighborhood node I did not vote for when the new Innovations was being built. I had never used a 3D printer, CNC, or laser cutter and, not knowing how, I wasn’t able to imagine a personal use for those tools. But the MakerSpace team put together great tutorials in their training and connected me to resources like Thingiverse and other associates co-using the space that now I am using the space at least once a month to create decorations or make utility pieces for around the house or office. The entire MakerSpace community has been a wonderful addition to my overall experience working here at Cerner. I’ve enjoyed the people, the projects, and the enthusiasm to help each other and share ideas.\n    The MakerSpace node serves as an associate space for innovation and exploration equipped with Ultimaker 3D printers, a soldering station, a CNC Carvey and an Epilog Laser.   A community of MakerSpace admins host regular workshops for associates to learn new skills or machinery such as this one featuring a laser project.Associate Reviewer: Caleb Meyer, Senior Software Engineer  The new Innovations campus shows us that Cerner is a company you can grow with. The old Innovations campus (I’m still refusing to call it Realizations) wasn’t designed for programmers, and certainly not for as many as we have. My team is as collaborative as ever, but now we have space to focus and get good work done. The all wifi floors perfectly represent everyone’s preference to remain wireless (untethered) during the day.\nMy favorite feature of the campus is the Assembly. I’ve run the Python Developers’ Meetup since its inception across the road (at the former campus), and I remember how ill-suited the game room (8 byte café) was for it. The Assembly is perfect for meetups of any size, and the self-service microphones and projector rock. The DE Tech Talks we’ve had there have all been fantastic, and the external speakers’ Tech Talks are always good as well. When there’s not something going on, the furniture surrounding the Assembly is comfortable, and there’s plenty of outlets for working away from your desk without your battery dying.\n Spanning two floors in the heart of Innovations, the Assembly is home to many cultural events such as meetups, Tech Talks (be sure to check out our YouTube channel for recordings of our impressive speakers), lightning talks, ShipIt presentations, and more!\n     \nAssociate Reviewer: Ryan Grass, Senior Software Engineer  In many ways I feel the new campus is a reflection of the cultural shift Cerner took over half a decade ago to create a fun, creative, and interactive development culture. The cafeteria is practically a food court, offering a range of diverse choices that reflect our many individual associates’ tastes. The work areas are open, and modular, allowing for easy communication between associates; and creativity in customizing your work environment. The many nodes on campus offer creative outlets for innovative minds. Revenue Cycle uses Bowser’s Castle for monthly after work board game meetups. These are not only fun, but great ways to network within our organization. My team has even decided a few times to stay on campus rather than going out for team lunches and play board games over lunch. These have been great opportunities to bond, and become more relaxed around each other.\n    Classic and Super Classic Nintendo stations are in both of our game rooms, Flynn’s Arcade and Bowser’s Castle. Associates gravitate to these retro games when they need a short break to look at problem differently. Additionally, you’ll find many associates huddled around our foosball tables during lunch.   Pinball is one of the many arcade games we have on-hand in our two game rooms at Innovations. Game of Thrones seems to be a crowd favorite as we wait in anticipation for the final season!   Table tennis is a necessity at Innovations. On a daily basis you can find these tables full of associates looking for some mid-day competition during lunch.   We have two quaint library nodes at Innovations each with a unique theme: Harry Potter and Lord of the Rings. These spaces are designed for associates who need to escape their desks for a change or want to check out the hundreds of books Cerner offers ranging from technical, UX, medical and management.Innovations has become an integral part of our development culture here in Kansas City. It has also become a showstopper in the Kansas City workplace architecture and development realm as it recently won the Special Judges’ recognition for the Kansas City Business Journal’s 2018 Capstone Awards. We are very proud of our newest campus and can’t wait to see how it grows and transforms physically and culturally over the new few years!\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/isabella-kuzava/",
        "title": "Isabella Kuzava",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/shipit-x-day-winter-2017/",
        "title": "ShipIt X Day: Winter 2017",
        "tags": ["culture", "engineering", "hackathon", "shipit"],
        "description": "",
        "content": "   On December 14th and 15th, 18 teams of Cerner associates competed for the Golden Trophy during our tenth ShipIt Day. Being this was our tenth, it was fun to reflect back on how this event started and how much it has grown.\n It has been great to see ShipIt Days start from small hackathons organically run by individual teams to now an engineering-wide quarterly event that brings together engineers from disparate organizations, allowing for a better cross-pollination of ideas and team members. - Kevin Shekleton\n    This was our third ShipIt Day hosted at our new Innovations campus. It continues to be a great location for ShipIt because of our awesome game rooms and collaboration spaces. As part of this ShipIt, we had a sponsored theme, which focused on building React applications using Kaiju that are based on Cerner\u0026rsquo;s consistent Terra UI theme. Team members that support these projects were available to help participants if they wished to use these technologies, helping to gain more feedback on the projects, and to expand the exposure of these projects to ShipIt teams. By including this theme, we also introduced a new category for someone to win by using these projects.\nShirt for participants.\n   Whiteboard action happening bright and early.\n  \nTeams checked in bright and early for kick-off. Vouchers to the cafe were provided so participants could fuel their hard work throughout the day. Workflow was constant with lots of collaboration.\n   Teams planning out their strategy.\n  \nThe Assembly filling up before presentations.\n   Demos always arrive quicker than anticipated. Ready or not, 9am rolled around and demos were in full swing. Teams got 4 minutes to present their projects to a panel of judges, consisting of Michelle Brush, Robert Farr, Tim Erdel, Chad Moats, and Chuck Schneider. Special thank you to those folks for spending their morning with us and supporting ShipIt Day!\nTeam Delayed Gratification    Coming in first place was Team Delayed Gratification (Russ Star, Ben Hemphill, Morgan Patterson).\n Our ShipIt team, Delayed Gratification, worked to complete an existing side project called the \u0026ldquo;RUSS test,\u0026rdquo; aka Really Useful Speed Service. The goal is to allow service owners to make smart decisions about where to geographically place their services by providing response time, bandwidth, and other key network metrics.\n Congratulations Team Delayed Gratification! A much deserved win.\nTeam ShipIt X Files    Second place went to Team ShipIt X Files (Santosh Manughala, Carlos Salazar, Matt Horn, Seada Ali).\n Our project was to create an application for discovering related database rows that let\u0026rsquo;s the user quickly navigate back-end data in Millennium, (e.g. look up lots of useful info for a given personnel id, person id, or event id). Each bit of data could be clickable to do a new search on that id (search the person you found on a clinical event in one click). The app displays information so that it is easy to navigate.\n Way to go, Team ShipIT X Files!\nTeam A Near Field Far Far Away    Team A Near Field Far Far Away (Aurko Ghosh, Aaron Kaplan, Vijay Ayilavarapu, Nikhil Agrawal, Neil Pathare) took third place. Their project utilized our MakerSpace!\n For ShipIt, we assembled a 3D printed light bridge that provides 180% of bright light for the soldering workbench. In addition to assembly, we designed and attempted to implement an RGB LED controller using a Raspberry Pi 3 and mosfet chips. The controller uses both a physical switch, as well as a mobile app to control the lighting. The mobile app is developed using React Native, and communicates wirelessly with the controller via Near Field Communication (NFC).\n Judge Chuck Schneider noted, \u0026ldquo;I like that they took on a lot of different pieces, which adds to the complexity of the project.\u0026rdquo;\nTeam Flying Mongoose    The winners of the sponsored project theme was a veteran team, Flying Mongoose (Mahesh Acharya, Taylor Clay, Cihan Kaynak, Matt Stramel). They were able to start the migration of Cerner\u0026rsquo;s RxStation to a React application utilizing Terra UI. They were able to utilize Kaiju to build the prototype UI, which had dynamically loaded tables and mock buttons. By using ShipIt to start this transition, they were able to learn a lot of what it would take to make these next steps.\nNicely done Team Flying Mongoose!\nPeople\u0026rsquo;s Choice Awards     Favorite Team Name: Keep it on the DL (Eric Fung, Ian Kottman, Sam Livingston, Abhi Mahabalshetti, Will Pruyn) Favorite Project: A Near Field Far Far Away (Aurko Ghosh, Aaron Kaplan, Vijay Ayilavarapu, Nikhil Agrawal, Neil Pathare) Favorite Presentation: If It Doesn’t Ship, You Must Acquit (Steven Goldberg, Ryan Neff, Jason Xu)  Winning teams walked away with some awesome prizes. Wireless headphones, TileMates, portable chargers, Echo Dots, drones, Google Cardboards, and Lego tape were all up for grabs.\n   Each ShipIt Day, the projects get more intricate and more creative. The projects presented show off the talent we have. After surveying the participants on their projects, 45% of the projects were identified to improve daily work at Cerner. ShipIt Day remains one of our most popular dev culture events at Cerner.\nFirst time participant, Ted Segal said, \u0026ldquo;I\u0026rsquo;m a new developer, I\u0026rsquo;m still in the DevAcademy, and I really appreciated the chance to learn about tools and ideas that I had not been exposed to yet from more experienced developers.\u0026rdquo;\nVeteran participant, Alex Harder said he likes participating in ShipIt Day because he enjoys exploring new technologies and applying them to a problem. ShipIt Day continues to be successful among Cerner associates. Whether you want to work on something you don’t normally have time to, or you’re interested in meeting new people, ShipIt Day is a winner!\nSave the Date: ShipIt Day returns March 8th and 9th! If you are interested in learning more about other ShipIt Days, see these earlier posts:\n ShipIt Day IX Summer 2017 ShipIt Day VIII Spring 2017 ShipIt Day VII Winter 2016 ShipIt Day Winter 2016 Highlight Reel ShipIt Day Fall 2016 ShipIt Day Spring 2016  "
    },
    {
        "uri": "https://engineering.cerner.com/authors/bryan-baugher/",
        "title": "Bryan Baugher",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/cerner-open-sources-its-kafka-utilities/",
        "title": "Cerner Open Sources its Kafka Utilities",
        "tags": ["engineering", "kafka", "ingestion"],
        "description": "",
        "content": "At Cerner, we often make use of many open source projects in our infrastructure. I work on a team responsible for Cerner\u0026rsquo;s Ingestion Platform, a critical piece of infrastructure that takes in TBs of data and over a billion messages per day. The platform’s responsibility is then to make this data available for downstream teams to consume. When designing the Ingestion Platform, we felt Apache Kafka was perfect for ingesting and consuming these massive streams of data. We originally built the Ingestion Platform in 2014 and it has grown with Kafka ever since.\nAlong the way, we’ve become experienced with Apache Kafka and how it works. We know how to monitor it, perform common administrative operations, and more. We’ve even made contributions to Kafka for various improvements [1][2][3] and Apache Crunch, our processing framework, to add Kafka support. We have also given talks about our usage of Apache Kafka at Kafka Summit and we’ve written a blog post about it as well.\nWe are still interested in supporting the Kafka community, and today we hit another milestone as we open source some of our utilities for Kafka. This includes:\n Lightweight wrappers over producer and consumer providing additional functionality A client to simplify admin operations A Kafka connect sink for delivering data to another Kafka cluster Kafka test infrastructure for integration testing  Try it and let me know what you think! You can log issues for improvements, or email me your thoughts: bryan.baugher@cerner.com. We hope to continue to improve the library and always welcome contributions and ideas from others.\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/ingestion/",
        "title": "ingestion",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/kafka/",
        "title": "kafka",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/agile/",
        "title": "agile",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/cerner-and-the-sdlc/",
        "title": "Cerner and the SDLC",
        "tags": ["engineering", "agile", "learning"],
        "description": "",
        "content": "Cerner has a very compelling story that many want to learn from. Cerner’s Agile Champions are regular presenters at local, national, and global conferences. We routinely host calls and site visits for interested companies around the world who want to figure out how we were able to adopt Agile so quickly and sustain it so successfully.\nDespite our success, there has been a general feeling at the engineering leadership and Agile Champion tiers that we aren’t getting the benefits from Agile that we once were. We talk Agile well, but have far too many teams that could be classified as \u0026ldquo;cargo cult\u0026rdquo; - going through the motions rather than having an Agile mindset. We were great teachers, but somehow had gone stagnant in our learning and growth.\nTwo events have ignited Cerner on a renewed learning journey that has already impacted Cerner in many business areas and allowed Cerner to help shape the future direction of business agility across the globe.\nThe Invitation Ironically, the first came shortly after delivering a presentation at Agile 2014 that included a section on avoiding cargo cult Agile. Ahmed Sidky, CEO of ICAgile, invited me to connect for a brief meeting. When our \u0026ldquo;brief meeting\u0026rdquo; stretched over several hours, we knew we had a strong partnership opportunity. Thus, was born the certification of Cerner’s Agile courses against an independent, internationally recognized curriculum and Cerner as one of ICAgile’s first Member Organizations.\nThe second came shortly after Agile 2015 while hosting a call for CH Robinson and their delivery lead, Vanessa Adams. As the call wrapped up, Vanessa disclosed that she served on the board of directors for a newly formed group under the leadership of Steve Denning and Ahmed Sidky that was bringing together forward looking Agile companies to discover best practices in enterprise agility and share them back into the community. After talking with us, she felt that Cerner was an ideal candidate given our story and our willingness to learn and share.\nI was intrigued yet skeptical.\nI honestly didn’t know who Steve Denning was so being a part of the \u0026ldquo;Steve Denning Learning Consortium\u0026rdquo; held no appeal. I respected Ahmed as a voice in the Agile community that understood how to address real business issues with an Agile mindset.\nSome quick research on Steve exposed his background in the business and banking world. He is an author and regular contributor to Forbes magazine and Harvard Business Review. While he had written books on Agile management and was on the Scrum Alliance board, my mental image was of a business school professor, not a practicing agilist.\nI voiced my concerns. Vanessa felt confident they would be resolved and invited us to attend an upcoming visit (May 2016) at Riot Games and Microsoft that would be partially hosted by Ahmed who had joined Riot Games as head of Delivery.\nConverting the Skeptic One of the keys to Cerner’s successful transformation was leveraging the idea of Connectors, Mavens, and Salesmen along with Skeptics as articulated by Malcolm Gladwell in our early pilot groups so we could address potential issues head on and build momentum. We were especially successful as some of the biggest Skeptics were also Connectors or Mavens and they have been crucial to our broad adoption and continued sustainability. Our results were our best Salesmen.\nOn Cerner’s first site visit to meet with the SD Learning Consortium, the roles played out again. Microsoft, Riot Games, Ericsson, CH Robinson, Spotify, Steve Denning, and a representative from the Scrum Alliance were all in attendance. Over the four days in LA and Seattle, we got to see inside Riot and Microsoft and see what was working as well as what wasn’t. The open and frank conversations as well as the \u0026ldquo;No BS\u0026rdquo; attitude among all participants was extremely refreshing and productive.\nAt conferences, messages are polished and even \u0026ldquo;learning\u0026rdquo; (a.k.a. failure) is always painted in the best possible light. This was completely different. Failures that were failures were shared and discussed. Current experiments were both admired and challenged, often by the same individual.\nProbably the most significant moment for me was sitting next to Joakim Sunden from Spotify, who co-authored what I still consider to be the best introductory book on Kanban for software development (Kanban in Action), and having him explain how the Spotify videos were created and what really goes on. (Hint – they are Marketing videos and highlight models that Spotify does not rigidly follow. They don’t know what the Spotify Way is that many talk about.) Spotify is great, visionary and different, but they aren’t a unicorn or perfect. A future site visit to Spotify solidified that fact and made me respect them even more.\nI found that each existing member was a Connector in their organizations, a Maven in terms of enterprise agility, and a healthy Skeptic towards the bulk of commercialized Agile. Ahmed was the Salesman. Steve was a little bit of the professor, but clearly demonstrated an Agile mindset and a vision to change the world of work. I caught his vision. It was refreshing and for the first time in several years, I learned several new things that I could try at Cerner as soon as I got back. The Skeptic was converted.\nAssessing and Applying at Cerner As I took a hard look at Cerner after returning from the visit, I realized that our success with things like DevCon and the DevAcademy where we were truly industry trend setters had allowed us to be complacent in other areas that the other SDLC members had as strengths. In some cases, entropy was setting in and some of the benefits we realized from going Agile were no longer as prevalent. We weren’t in the danger zone, but some trends were heading in the wrong direction and merited a fresh look.\nIn other cases, SDLC companies were branching out with experiments in areas like HR, Finance, or Marketing that were bearing fruit. Their success led to a desire to see what we could do at Cerner.\nOver the past two years, site visits have been held at Microsoft (Seattle), Ericsson (Stolkholm and Athelone Ireland), Barclays Bank (London), BMW (Munich), Riot Games (LA), Spotify (New York), Vistaprint (Boston), Fidelity Investments (Boston), CH Robinson (Chicago), and Cerner. Without exception, from each set of site visits, at least three action items have been taken by the Cerner attendees to improve Cerner. There is a renewed vigor towards an Agile mindset that had waned over the past 3-4 years. At the beginning, Shahzad Zafar and I were the only representatives for Cerner. Now we have four Cerner associates at each site visit and have many other associates engage in deep dive conversations on special topics like DevOps, Shadow IT, or HR.\nLearning and Sharing Part of the SDLC is to learn from each other and then share those messages back into the broader community. There is deep, rich learning that can be gained from blending large companies like Ericcson with over 100K employees with a mid-tier company like Cerner (20K+), smaller companies like Vistaprint (5k+) and \u0026ldquo;born Agile\u0026rdquo; start-ups like Riot Games.\nWhile each SDLC member has software components as part of the services that we offer to our clients, we operate in different verticals. The lessons we learn are universal and are not limited to a specific market or niche.\nAt the end of 2016, the SDLC members got together and summarized our learnings from the year in a paper that Steve then presented at the 2016 Drucker Forum.\nPersonally, I have gone from the Skeptic to a very vocal evangelist, sharing the combined Cerner and SDLC story at the 2016 KC PMI Conference, 2016 LeanAgileKC, 2017 Business Agility Conference, Agile 2017, 2017 Global PMI PMO Symposium as well as 2 of Cerner’s 2017 DevCons. Barclays Bank and Riot Games have been equally as active in sharing the SDLC learnings across the globe.\nSteve Denning recently published an article in Forbes with the findings that are starting to be cited as the core fundamentals for business agility across the globe with Cerner listed as a prevalent contributor.\nLike the Agile Manifesto, the principles are simple, yet powerful:\n Delighting Customers Descaling Work Enterprise-wide Agility Nurture Culture  The Future The SDLC is currently viewed as the leader in business agility practices and is shaping the speakers and agendas for the Business Agility Conferences globally. We continue to expand membership to include new companies who meet the criteria of a leader in agility and have a story to tell. Current members have grown to include Barclays Bank, Vistaprint, Fidelity Investments, Target, American Express, and All State are all candidates to join in 2018.\nThe brightest future is not about the SDLC but about Cerner and what we can accomplish based on learning from other SDLC members. Our 2018 Vision aligns well with the SDLC learnings and several strategies to achieve our vision include models based on what other SDLC members have already proven to be successful.\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/learning/",
        "title": "learning",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/matt-anderson/",
        "title": "Matt Anderson",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/bad-design-is-bad-for-your-health-why-data-visualization-details-matter/",
        "title": "Bad Design is Bad for Your Health: Why Data Visualization Details Matter",
        "tags": ["ux", "data", "visualization"],
        "description": "",
        "content": "Presentation Abstract by Jody Butts, Sr. User Experience Designer Given on August 11, 2017 at the UX Australia Conference in Sydney\nFull presentation audio and slides are available here: http://www.uxaustralia.com.au/conferences/uxaustralia-2017/presentation/bad-design-is-bad-for-your-health-why-data-visualization-details-matter/\nThe proper design of data visualization details is imperative for accurate and actionable data visualizations and dashboards. In the health care technology industry, even the smallest design detail on a graph of patient data can have a dramatic impact on patient safety and outcomes. For example, auto-adjusting y-axes in vitals graphs across patient charts in the EMR could cause change blindness which impacts the perception of patient data trends.\nPatient Safety As a user experience designer for Cerner, a health care IT company, patient safety is the foundation of my work. However, we see startling patient safety statistics around the world today. According to a Johns Hopkins study from 2016, medical error is the third leading cause of death in the United States, following heart disease and cancer. The World Health Organization (WHO) estimates that in developed countries across the globe, as many as one in ten patients are harmed while receiving hospital care.\nBased on research the User Experience team at Cerner has done, there are several challenges the health care industry faces to ensure patient safety. From workflow and communication to external rules and regulation, there is a complex and interconnected web of factors.\nWhile human error is unavoidable, there should be several defensive layers for every system. Despite the training, regulation, and process our health systems have, health care providers are still working in high-risk environments.\nUnderstaffed health systems are flooded by people in need of care. On a bustling hospital floor with alarms going off at all hours, care team members running between patient rooms, and the need to use a computer to document patient data to meet government regulations, it is no wonder that medical error occurs.\nAlthough we cannot ensure software will always work perfectly, we do have responsibility to ensure that the UI and UX layers of our systems are designed to protect our users and enable them to perform their tasks well.\nUser Experience at Cerner I work for Cerner, a company at the intersection of health care and IT. Cerner technologies support the full continuum of care including clinical, financial, and patient engagement solutions. We have products licensed at more than 25,000 facilities across 35 countries.\nAs a part of Cerner’s UX team, I spend most of my time designing in the clinical space, working on projects for our main product: the electronic medical record (EMR), mainly used by doctors and nurses. Our technology is subject to a long list of legislation and regulations. Cerner has an entire team dedicated to reviewing standards for each country to ensure we are meeting all health IT requirements.\nOur UX team also has training on how these standards impact our designs and research. Accessibility guidelines, patient privacy, and medication safety are just a few examples of regulation themes that guide our work. I focus on the rules that impact data visualizations and incorporate additional research to create safe and usable experiences.\nThe accurate graphical display of patient data has become my passion over the past two and a half years. Data visualization is a very powerful tool across industries, but I’m biased toward believing that its most amazing applications are in health care technology.\nData Visualization in Pediatric Health Care For many years, the EMR was simply an input tool for charting patient data–most data review was solely displayed in rows and columns. Now we are creating meaningful displays of information for clinicians to support their decisions.\nFor example, the pediatric growth chart has been used by health care providers to track children’s growth from birth into adulthood for almost 40 years, starting on paper. The WHO and Centers for Disease Control and Prevention (CDC) have standards for healthy growth of height, weight, head circumference and more based on age, sex, and so on.\nThese standards are displayed as percentile lines that create a unique curve for each measure. When patient data is plotted, it becomes easy to identify if a child’s growth is progressing normally, or if there are abnormal patterns. For the first 20 years of an individual’s life, the growth chart is essential for tracking healthy growth progression. It is also an invaluable tool for communication between parents and clinicians.\nAs I have worked on a redesign of Cerner’s interactive growth chart tools, I have had the opportunity to observe clinicians using the growth chart in hospitals. In a Neonatal Intensive Care Unit (NICU) setting, or neonatal intensive care unit, physicians are making life-saving decisions for premature infants based on this information. For specialists, such as pediatric endocrinologists, the pattern of the trend line is often the first indication of diagnoses such as hypothyroidism. Can you imagine being able to recognize a disease by looking at a line graph?\nData Visualization Design Details In a matter of clicks, we can take tables with thousands of numbers and transform them into graphics that provide immediate insights. From problem solving to storytelling, this information drives action across every industry.\nThere are countless examples of confusing and even misleading graphs–just try a Google search for “bad data visualizations,” and you’ll see what I mean. This assumed legitimacy is dangerous, especially when the design is sleek and appears trustworthy. The details that lead to accurate and impactful data visualizations fall into three categories: clarity, context, and creativity.\nClarity\nClarity is the foundation of data visualization design. We want to avoid all potential for confusion in order to empower our users to confidently make decisions. To achieve this, we must start with the basics. The title of a visualization should be succinct, yet provide the information needed to understand what data are being viewed and how they are being measured. If additional details are important to include, consider using supporting text below the title.\nA dangerous mistake I regularly see while consulting on visualizations and dashboards is improper or missing labels. Do not make users guess what data points are being shown or how they are being measured. All axes in visualizations should be labeled. It is also important to include the unit of measure, which I like to include in parentheses next to the label.\u2028While labels are important, I believe the most powerful element for clear data visualizations is proper scale. A simple mistake I often see is to start the y-axis of a bar graph at a value other than 0. Human eyes interpret the value of a bar to be equal to its length, so when the y-axis does not begin at 0, the values are not interpreted accurately. Similarly, the perception of data measured in percentages is also often skewed by scale. This sometimes occurs because a tool automatically adjusts the scale to fit to the data, but sometimes it is done to manipulate the data display. This example of an ad from Chevy illustrates the power scale has on perception (starting at 0:45).\nContext\nOnce there is a truthful and clear foundation for data, the following techniques can bring users quicker insights and increase their confidence in data interpretation.\nIf there is a normal, expected, or ideal range of values for the data, adding a target range is a simple but powerful way to add context. A light shaded region behind the grid lines can allow users to easily identify high and low values. Similar to a target range, a goal line can be added to indicate a discrete value that the plotted data should be achieving. A goal line and target range can be used together as well.\nIt’s important to mark visualizations and dashboards with the date it was published. I also recommend including the date or date range of the data as well in order to maintain trust and relevancy of your visualization.\nAnnotations are another simple way to provide your user with context. On-canvas notes and labels should be used sparingly, but can help highlight insights. These can be placed in context of a specific data point, a date or timeframe, etc.\nCreativity\nOnce we have created a clear visualization and provided our user with additional context, we are ready to polish our design. There is great potential for creativity in data visualization, but it can cause more harm than good if done poorly. Let’s look at visual encoding methods, colors, and other visual design details that can enhance your visualization or dashboard.\nQualitative and quantitative data is visually encoded, or translated, through a variety of graphical methods that make up each visualization type. Research has been done on these visual encoding methods, giving us insights for human graphical perception. Statistician William Cleveland and his colleague, Robert McGill, tested the visual encoding methods and ranked them by accuracy of perception. Their findings give us a scientific foundation for data visualization.\nPosition is the best encoding method for accuracy, followed by length, angle, area, volume, and finally, with color taking the last spot. This means that color is not reliable for communicating data information in a graph. This aligns to the WCAG 2.0 accessibility guidelines, which state that color cannot be used as the only visual means of conveying information, indicating an action, and so on. Second, bar, and line graphs are common for good reason–they are most effective for our brains to accurately understand. Ultimately, it’s always best to try a couple of options and compare their effectiveness.\nUse color sparingly. Think of it as a tool for drawing attention to the most important insights. If everything has color, especially on a dashboard, nothing will stand out. Since approximately 10% of the population is color blind, we should avoid using red and green together on a visualization. Color Oracle is a great color blindness simulator that you can use to see how your colors appear for users with these vision deficiencies.\nEnsure you maintain high contrast between colors by avoiding very light or very dark shades. The Delta-E calculation measures the distance between two colors. At Cerner, we ensure a difference of at least 3.5.\nThe best way to test the colors you are using is to print it out in grayscale. Make sure you can still match plotted data to its labels. Better yet, get additional eyes on it. This test ensures that color is not being utilized as the only way to communicate meaning.\nVisualizations are the Key to Understanding Data The world is overflowing with big, complex problems, but we also have big, complex data to solve them. This big, complex data exists as numbers that can go on forever. Data visualizations are the key for humans to understand it all. Data must be visualized clearly and effectively to enable professionals across industries to understand it, make decisions, and take action based on it.\nWe can revolutionize health care and other fields through the way we visualize data. The design details matter. Through clear, contextual, and creative data visualizations, we can drive action that will change the world and even save lives.\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/data/",
        "title": "data",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/jody-butts/",
        "title": "Jody Butts",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/ux/",
        "title": "ux",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/visualization/",
        "title": "visualization",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/2-to-the-5th-coding-competition-2017/",
        "title": "2^5 Coding Competition 2017: 32 lines or less",
        "tags": ["culture", "engineering"],
        "description": "",
        "content": "The Competition Part of Cerner\u0026rsquo;s engineering culture is to celebrate technology. This past fall, we had our annual celebration for Programmers’ Day. We celebrated the day with great food (taco bar and ice cream) and an evening of trivia.\nTopping off Programmers\u0026#39; Day with a taco bar and trivia night with @geekswhodrink pic.twitter.com/pSjqBVBHxK\n\u0026mdash; Cerner Engineering (@CernerEng) September 13, 2017  Happy Programmers\u0026#39; Day! Celebrating with an ice cream social. pic.twitter.com/bakBeOx20M\n\u0026mdash; Cerner Engineering (@CernerEng) September 13, 2017  With our celebrations, we also like to blend in challenges or activities that can mold desired behaviors. Therefore, as part of Programmers' Day, we also kicked off a programming competition (our third year of organizing the competition). For 32 days (September 13 - October 16), associates submitted small snippets of code (32 lines or less) in any language that represented the concept of engineering impact. After 32 days, our panel of judges sorted through the code and assigned a winner to each of the following categories:\n Best Representation: What captures the concept of \u0026ldquo;engineering impact\u0026rdquo; the best? Most Obfuscated: Which code snippet was the most difficult to understand? Greatest Variety in Languages: Which repository contains the most variety in programming languages across submissions? Most First Hand Submissions: Which participant submitted the most code that relates to First Hand Foundation and its mission to \u0026ldquo;impact\u0026rdquo; others?  As with anywhere, engineers at Cerner are busy. Asking them to do something additional, like a coding competition, imposes challenges on participation based on their availability of time. However, we found that by organizing the competition for engineers to explore and share what they are working on, in small and consumable bites of code, helped in keeping it a simple time investment and gained participation. Furthermore, it helped reinforce a routine behavior to continually look and evaluate different technologies and languages.\nJust like tweets, by having small code examples being shared, it was easy for others to also learn what they were doing. We facilitated sharing these tweets by making a dashboard which served up crawled results from GitHub. This was achieved by having participants including a specific phase in their code (like a hashtag), which could then be pulled from GitHub’s search and built into a dashboard. This dashboard was based on how we originally achieved this in our earlier 30 Days of Code competition.\nThe Winners Our winners from this year are as follows:\n Best Representation: Jan Monterrubio Most Obfuscated: Douglas Bailey (Honorable mention: Joe Huck) Greatest Variety in Languages: Afrin Subair Most First Hand Submissions: Jake Varness  Congratulations to the winners and thank you to everyone who submitted code. Until next year!\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/announcing-bunsen-fhir-data-with-apache-spark/",
        "title": "Announcing Bunsen: FHIR Data with Apache Spark",
        "tags": ["engineering", "spark", "FHIR"],
        "description": "",
        "content": "We\u0026rsquo;re excited to open source Bunsen, a library to make analyzing FHIR data with Apache Spark simple and scalable. Bunsen encodes FHIR resources directly into Apache Spark\u0026rsquo;s native data structures. This lets users leverage well-defined FHIR data models directly within Spark SQL.\nHere\u0026rsquo;s a simple query against a table of FHIR observations that produces a table of heart rate values:\nspark.sql(\u0026#34;\u0026#34;\u0026#34; select subject.reference person_id, effectiveDateTime date_time, valueQuantity.value value from observations where in_valueset(code, \u0026#39;heart_rate\u0026#39;) \u0026#34;\u0026#34;\u0026#34;).show() Which prints a table like this:\n   person_id date_time value     Patient/123 2016-12-27 54.0000   Patient/123 2017-04-18 60.0000    Notice that each field in the above SQL is fully defined by the FHIR Observation model. This is because the table schemas are generated directly from FHIR resource definitions, ensuring these queries exactly match other FHIR-based views of the same data.\nBunsen also provides a collection of helpful functions to make querying data easy. The above query includes the in_valueset user-defined function, allowing users to use code value sets directly in the query. You can see the Bunsen value set documentation for details.\nScalability and Performance Because Bunsen encodes FHIR resources in Apache Spark\u0026rsquo;s efficient binary format, we get all of Spark\u0026rsquo;s scalability and performance advantages. Simple queries across billions of FHIR resources typically return in single-digit seconds in internal clusters. Arbitrary joins and aggregations of complex datasets scale with your Apache Spark cluster. We take advantage of Spark\u0026rsquo;s built-in support for Apache Parquet to read and write FHIR with an efficient columnar data format that is readable by other systems as well.\nSophisticated Queries Spark SQL offers rich query semantics that can now be used directly over FHIR data models. For instance, here is a query that builds a timeseries-like table directly from a collection of observations by simply grouping items by the person and time period. This is just standard Spark SQL wrapped around our simple valueset-based function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  select subject.reference patient_id, year(effectiveDateTime) obs_year, month(effectiveDateTime) obs_month, avg(if(in_valueset(code, \u0026#39;glucose_level\u0026#39;), valueQuantity.value, null)) avg_glucose_level, avg(if(in_valueset(code, \u0026#39;bun\u0026#39;), valueQuantity.value, null)) avg_bun from observations group by subject.reference, year(effectiveDateTime), month(effectiveDateTime) order by patient_id, obs_year, obs_month   Typical queries may aggregate many other types of data and join to conditions, allergies, or other tables to build a more complete report. All of this can be done interactively over billions of records.\nJava Usage Bunsen uses the HAPI FHIR library to represent data in object form. Java users can convert their objects from the HAPI to Spark-native structures and back with a few lines of code. Here\u0026rsquo;s an example:\n1 2 3 4 5 6 7 8 9 10  FhirEncoders encoders = FhirEncoders.forStu3().getOrCreate(); List\u0026lt;Condition\u0026gt; conditionList = // A list of org.hl7.fhir.dstu3.model.Condition objects.  Dataset\u0026lt;Condition\u0026gt; conditions = spark.createDataset(conditionList, encoders.of(Condition.class)); // Query for conditions based on arbitrary Spark SQL expressions Dataset\u0026lt;Condition\u0026gt; activeConditions = conditions .where(\u0026#34;clinicalStatus == \u0026#39;active\u0026#39; and verificationStatus == \u0026#39;confirmed\u0026#39;\u0026#34;);   Users can also leverage Spark\u0026rsquo;s built-in functionality to write these datasets to tables and query these tables later.\nUsing and Contributing Bunsen is available under the Apache 2.0 license. The initial release targets Apache Spark 2.1 and FHIR STU3, and support for additional versions will emerge over time.\nCheck out Bunsen\u0026rsquo;s documentation and GitHub Repository for more on using or contributing to the project.\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/celebrating-world-usability-day-at-cerner/",
        "title": "Celebrating World Usability Day at Cerner",
        "tags": ["usability", "ux"],
        "description": "",
        "content": "World Usability Day is celebrated worldwide on the second Thursday in November every year. Across the globe, organizations host events and use this day to provide education on how to make products easier to use and simpler to access, as well as to celebrate progress in this pursuit.\nThe World Usability Day theme for 2017 was \u0026ldquo;Inclusion.\u0026rdquo; Inclusive design considers the range of abilities, language and other forms of human difference when creating a solution. Designing with these in mind from the start can create a better suited product for everyone.\nCerner\u0026rsquo;s Kansas City-based User Experience (UX) team celebrated by hosting a day filled with five speaker sessions and an afternoon workshop for Cerner associates on November 9, in the Innovations Campus event space. Following the theme, the speakers covered topics such as accessibility and universal design.\nLauren Bock, User Experience Designer on the UX team, kicked off the day with a presentation on Universal Design: The Equality of Experience. Lauren explained how designing only for the \u0026ldquo;average user,\u0026rdquo; or including only the minimum accessibility requirements, can result in something designed for no one. We each have unique needs and designing products, services and experiences designed in a way that honors those needs can result in a better outcome for everyone. She shared how designing universally from the beginning not only makes a better product, but does so for a larger number of users.\nFollowing the keynote, Dr. Ben Wilkerson, Physician Executive for Cerner\u0026rsquo;s Population Health organization, shared his take on why Healthcare Software is Failing at the Box Office. In his talk, Dr. Wilkerson drew comparisons between healthcare software and the movie industry. He highlighted how healthcare software can take cues from films in the way that production teams spend the majority of time in research, preparation, and planning, before the costly filming step. Dr. Wilkerson also compared the different roles of the production team to those designing software at Cerner and how they can work together to create a compelling result.\nIn the last session for the morning, Scott Wilmarth, Senior User Experience Designer on the UX team, shared his personal and real-world story on the importance of accessibility in Retinas, Accessibility, and More Stuff I Learned About on the Way to Get Thai Food. After an unexpected loss of vision in one of his eyes, Scott has become an accessibility advocate, sharing his story with development teams, Cerner Health Conference attendees and others at Cerner. Scott shared the impact of accessibility and shared ways for designers, developers and project teams at Cerner to integrate accessibility into their processes. He explained that designing for accessibility is not just required, but the right thing to do. Accounting for different abilities in design can ultimately create better products for all users.\nAfter lunch, Michael Turman, the Senior Strategist for the Reference Data Management team, gave his perspective on why We\u0026rsquo;re Going to Need a Bigger Boat when designing our solutions at Cerner. Coming from the Cerner Consulting organization, Michael highlighted the importance of the post-development, implementation and maintenance steps of Cerner software with our clients. He provided tips for teams to design for these user bases and with implementation in mind. He explained that by designing for users both upstream and downstream in the development process we can create better experiences.\nDr. Jeff Wall, Director of the Physician Strategy and Medical Specialties organization, concluded the speaking sessions with his talk Designing the Physician Workflow: Technical vs. Adaptive considerations for success. Dr. Wall summarized it perfectly, \u0026ldquo;change is hard.\u0026rdquo; How do we help users effectively handle change as we constantly improve our healthcare software? He explained that when designing software for our users, we must be considerate of adaptive challenges, making sure to distinguish between technical changes and changes to human behavior. With years of experience as a clinician, Dr. Wall explained methods for Cerner teams to prepare for and introduce change to our users.\nThe last activity for the day was a Virtual Reality Workshop sponsored by the Cerner Labs team, who focus on the investigation of emerging technologies and how to use those to create new ways of engaging with data. Workshop participants were able to experience a scenario using Virtual Reality (VR) headsets. Small groups led by UX team members replayed the scenario and used the technique of Journey Mapping to lay out the course of the experience, ultimately discovering areas of opportunity for improvement for the scenario. Groups reconvened and shared each of their maps to learn from each other.\n   The day concluded with a public evening meet-up hosted in partnership with the User Experience Professionals Association of Kansas City, a local non-profit organization focused on UX. Attendees were invited to tour the User Experience Lab where the Human Factors Research team conducts usability tests and other research studies. This tour was followed by a design challenge where attendees broke out into teams and collaborated on creating quick solutions to design prompts focused on inclusive design.\nJust around the corner from the User Experience Lab at the Innovations campus is a hand-drawn installation of epic proportion about World Usability Day. These 20 ft. long, 10 ft. tall installations of content are created and drawn by UX team members, and will rotate every quarter. Located next to the glass wall is an interactive puzzle also put together by the UX team, highlighting User Experience Vice President, Paul Weaver’s definition of UX: \u0026ldquo;UX is the nexus between the user and the machine; a combination of design and research that works to perfect human-computer interaction.\u0026rdquo;\n   Photos by Lai Xu and Kelsey Daly\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/lauren-bock/",
        "title": "Lauren Bock",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/usability/",
        "title": "usability",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/accessibility-and-usability-on-the-road-to-great-ux/",
        "title": "Accessibility and Usability: On the Road to Great UX",
        "tags": ["accessibility", "usability", "midwestux"],
        "description": "",
        "content": "In 2010, a group of designers from Columbus, Ohio set out to create a grassroots, cross-discipline, and creative conference. Since that first event, Midwest UX has been an ongoing tradition that attracts innovative ideas and provides an opportunity for regional professions to engage in discussion with other professionals. Last year, Amber Wilks and Dr. Becca Green were invited to take their talk from DevCon 2016 on the road to Midwest UX. On October 20, 2016, they presented Accessibility \u0026amp; Usability: The Equality of Experience in Louisville, KY.\nAt Midwest UX, the talk covered all of the great content from DevCon enhanced by examples of how accessibility specifically affects our solutions used by our consumer market or clinical staff. Each point covered demonstrated how UX and development work together to achieve an accessible software solution and the importance of crafting equally awesome experiences for all of our users. When people talk about accessibility, there are some common myths that go along with the topic. So the talks starts off by covering three myths of accessibility that have impacted the health care community, and Cerner in particular.\nThe first myth covered was that project teams look at accessibility as a minefield they must cross to achieve accessible software. The truth is that it is simply a blend of legislation and regulation that aims to eliminate barriers in technology for people with disabilities. In fact there are two main forms of standards used in the U.S. to define accessible content, Section 508 and W3\u0026rsquo;s Web Content Accessibility Guidelines. While they serve similar purposes, there are some very key differences. They different in terms of the technology they cover, the organization of the guidelines, and their application. However, they are just a small part of a landscape of legislation and regulation that make up accessibility.\nThe second myth covered the assumption that accessibility is all about screen readers and alt tags. In reality, screen readers are just a small subset of assistive technologies. Low-vision or blind users only make up a small slice of all U.S. citizens with disabilities. There are a variety of assistive technologies to aid every type of disability. The third and final myth we covered is specific to the healthcare industry; that clinicians aren\u0026rsquo;t disabled so accessibility isn\u0026rsquo;t a priority for our users. The truth is that clinicians are not immune to disabilities. In fact, a study in the British Journal of General Practice that found that the proportion of male doctors with color vision deficiency was at 8%, in line with the percentage of the general population.\nAs the guidelines for accessibility have evolved, so has the organization of these guidelines such that they can now be organized into 4 tenant principles: perceivable, operable, understandable, and robust.\nPerceivable Information and UI components must be presented in ways users can perceive. In other words, information can\u0026rsquo;t be invisible. We do that by making information distinguishable and by providing alternative forms of content.\nTo make information distinguishable, the foundations involve color math and text zoom. To create distinguishable information using color there are two mathematical calculations that can be used to ensure the content is both legible and discernible. To ensure the content is legible, Cerner follows WCAG\u0026rsquo;s (Web Content Accessibility Guidelines) color contrast guidelines include Guideline 1.4.3, which states that \u0026ldquo;the visual presentation of text and images of text has a contrast ration of at least 4.5:1.\u0026rdquo; To check if your information is legible, the WCAG recommends using the WebAIM Color Contrast Checker. This tool simply requires that you enter in your text, or foreground, color and background color and it will tell you to what degree you meet the contrast guidelines.\nThe second calculation ensures users can perceive changes in the content. While this calculation is not part of accessibility guidelines, we leverage the calculation as a baseline metric for interaction indications (i.e., hovers, click states, icons, etc.). This numerical value starts at 0 and the higher the number, the greater the difference between two colors. We recommend a minimum value of 3.5, which represents an obvious difference.\nThe other part of having distinguishable content is allowing for text zooming. In the U.S. alone, 8.1 million people have a vision disability. In March 2013, WebAIM conducted a survey of users with low vision, which found that the bulk of accessible technologies used by these individuals included some form of text resizing. Text zooming is a method used to scale text up and down while retaining the screen formatting and layout. A successful text zoom is when the content can be resized up to 200% and everything on the screen scales uniformly and scroll bars are provided if needed.\nThe second part of providing perceivable content is to provide content that supports all the senses. A video may require a paired audio supplement. An audio file may require a written transcript. Just like the transcript we provided with our talk. While people with vision disabilities are a small slice of the population, those with hearing disabilities make up a larger portion and are more likely to be employed. So it\u0026rsquo;s far more likely that a user would need supplemental text than they would be using a screen reader.\nOne of the most common ways to provide alternative content is through captioning. Captioning is the transcription of speech and important sound effects. Not to be confused with subtitling which is a written translation of dialogue.\nOperable UI components and navigation must be operable. In other words, the UI cannot require an interaction that a user cannot perform. Most assistive technologies used by people with disabilities emulate how a keyboard functions. So a good rule of thumb is that if it can be navigated with a keyboard, then it is accessible. Operability is about giving the user control with controls to pause, stop, and hide moving, blinking, or scrolling content.\nUnderstandable Information and the operation of the user interface must be understandable. In other words, the content and operation cannot be beyond a user\u0026rsquo;s understanding. Users consume content in a variety of ways and we must be flexible to accommodate all of the different ways they might access information. Therefore, it\u0026rsquo;s important to ensure that content is housed in a structured framework to facilitate navigation.\nAs part of the talk, an example of the experience a screen reader user might have while navigating a well known and popular website is presented. This set of videos demonstrates some of the common problems found with navigation such as duplicate links, ambiguous link names, and long navigation. The results of an automated accessibility tool are presented to highlight that not only are automated tools important for assessing accessibility but that it also requires a manual component.\nRobust Content must be robust enough that it can be interpreted reliably by a wide range of user agents. In other words, user must be able to access the content as technologies advance. Since there are a lot of different types of assistive technology, as these technologies change, so does the landscape of accessibility. Being future friendly isn\u0026rsquo;t as hard as keeping up with technology. It\u0026rsquo;s actually as easy as sticking to \u0026ldquo;the basics.\u0026rdquo;\n Write valid code Optimize navigation Stick to standards Stay away from known hazards  The key to crafting equally compelling experiences for all is the concept of Universal Design. Universal design is the design and composition of buildings, products, and services so they can be accessed, understood, and used by all people. It\u0026rsquo;s an inclusive approach to design. When you design for the average person, you\u0026rsquo;re actually designing for no one. We are each unique and have different needs. We want to encourage you to not think of accessibility as a flat, one-size-fits-all solution but instead as a multifaceted, configurable space. It\u0026rsquo;s more than just what\u0026rsquo;s on the screen though. It\u0026rsquo;s about the environment and the users in that environment.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/amber-wilks/",
        "title": "Amber Wilks",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/midwestux/",
        "title": "midwestux",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/rebecca-green/",
        "title": "Rebecca Green",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/arvindo-kinny/",
        "title": "Arvindo Kinny",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/jwala-joins-cerner-open-source/",
        "title": "Jwala Joins Cerner&#39;s Open Source",
        "tags": ["engineering", "oss"],
        "description": "",
        "content": "Introducing Jwala, a Resource Manager for the Apache Community Apache Tomcat and Apache httpd are hardened, scalable, and reliable open-source solutions. Widely adopted, these solutions are commonly used to host and load-balance Java applications.\nConfiguring and managing a small set of elements is relatively easy, but once you start scaling out your application and platform, things get complicated. Questions such as \u0026ldquo;How do I update the heap or database pool-size across hundreds of Java Virtual Machines (JVMs)?\u0026rdquo; arise. Although commercial solutions exist to solve this and other similar problems, an open-source solution was not available - until now.\nOriginally designed and developed to accommodate Cerner\u0026rsquo;s complex topologies, Jwala was developed to automate deploying our web applications here at Cerner. Now Cerner is donating Jwala as open source. Making contributions to development communities is an important part of Cerner\u0026rsquo;s engineering culture. We love it when we\u0026rsquo;re able to give back to the community, and we\u0026rsquo;re sure you\u0026rsquo;ll love Jwala!\nA Cerner platform based on Apache Tomcat and Apache httpd, Jwala enables the configuration, deployment, and management of logical groups of Tomcat JVMs load-balanced by Apache web servers.\nWith Jwala you can configure, deploy, and manage your large-scale Tomcat and Apache httpd topology at an enterprise scale. You can create and persist definitions of Group instances that include web applications, JVMs, web servers, and other resources, and expose them through a RESTful interface, Jwala\u0026rsquo;s REST API, to perform management operations on the group.\nJwala utilizes a defined file system structure and Secure Shell (SSH) agents on each machine to manage running Tomcat instances on remote servers. Jwala\u0026rsquo;s application deployment model knows how to request current status from each Tomcat and httpd instance. Jwala also is able to update each instance as changes are made to the configuration and allows maintenance operations to be executed from a central console. Last but not least, Jwala maintains an audit log of all the operations or changes performed on any of its managed resources.\nSo, if you are someone in development, operations, or management looking for an easier way to configure and manage a large Tomcat topology, check out Jwala.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/dennis-patterson/",
        "title": "Dennis Patterson",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/engineers-overseas-for-smart-and-fhir/",
        "title": "Engineers Overseas for SMART and FHIR (FHIR DevDays 2016)",
        "tags": ["engineering"],
        "description": "",
        "content": "In a previous post, we discussed the need to provide education around our ecosystem and implementation of the FHIRⓇ and SMARTⓇ standards for developers and how we did so at our annual Cerner Health Conference. That same week, we were also a part of FHIR DevDays, which included a track entitled \u0026ldquo;Apps in the EHR.\u0026rdquo; We worked alongside track participants who were seeking to integrate their apps in a variety of EHRs, including Cerner.\nFHIR DevDays is a conference put on by Furore in Amsterdam. This year, over 250 participants from all over the world gathered to attend tutorials given by FHIR community leaders in the mornings and participate in hands-on sessions in the afternoons.\n   Cerner co-led the \u0026ldquo;Apps in the EHR\u0026rdquo; track. This track was comprised of roughly 20 people from across France, Germany, Ireland, Israel, Italy, Netherlands, Norway, Spain, Switzerland, the United Kingdom, and the United States. Josh Mandel kicked things off with a tutorial on SMART and how to get started building cross-vendor applications. Afterward, we met with track participants to show them how to get plugged into Cerner\u0026rsquo;s sandbox and ecosystem. We started out by covering our documentation launchpoint, registering applications in our code console, and going over our technical documentation. For those who didn\u0026rsquo;t come to the conference with an existing application to work on, we provided a tutorial. With those tools in hand, they were ready to get to work!\n     \nOver the course of the three afternoons, we assisted people as they used SMART and FHIR for the first time, but also people with experience in FHIR who were looking to get their technical and business questions answered about integrating with Cerner. Several of the projects were plugged into multiple sandboxes. The developers shared that once their app was working in one sandbox, it was easy to plug into others, which is exactly how it should be!\nWhen we gave our closing presentation, we actually had some technical difficulties loading up the slides. Little did we know that Grahame Grieve, the original architect of FHIR, snapped a picture and would use it shortly in his closing keynote on FHIR’s future.\n   With this image on screen, he stated: \u0026ldquo;[to us:] Well done. [to the rest of the audience:] Next time somebody in your acquaintance tells you that certain vendors aren\u0026rsquo;t invested in interoperability, I want you to think about this. That\u0026rsquo;s part of what the FHIR community is about, alright? Cerner and Epic on the stage together. It\u0026rsquo;s not special anymore, even. It\u0026rsquo;s great!\u0026rdquo;\nFHIR DevDays is Europe\u0026rsquo;s biggest FHIR conference and many attendees don\u0026rsquo;t typically make it to the tri-annual HL7 Working Group Meetings that focus on the formation and evolution of the specification. We were pleased to be a part of that gathering and to work alongside people interested in engaging with Cerner. But, we were also encouraged by the comments Grahame and others gave as they expressed appreciation that Cerner would be present and show our interest and dedication to the standard. We look forward to being back at DevDays 2017 this November to lead tracks on SMART \u0026amp; FHIR and CDS Hooks.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/ian-kottman/",
        "title": "Ian Kottman",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/shipit-ix-day-summer-2017/",
        "title": "ShipIt IX Day: Summer 2017",
        "tags": ["culture", "engineering", "hackathon", "shipit"],
        "description": "",
        "content": "   There is no better way to end summer than with ShipIt Day! We kicked off our ninth ShipIt Day on August 10th, with more than 75 Cerner associates signed up. Nineteen different teams worked for 24 hours straight to complete their projects. For those unfamiliar with the event, ShipIt Day is a 24 hour hackathon for internal associates to create something innovative, usable, and value-adding. This was the second ShipIt held at our new Innovations Campus. Associates love using our new campus for ShipIt because of the game rooms and collaboration spaces! One new addition to this quarter’s ShipIt Day was the scavenger hunt. Participants were given a list of crazy tasks to accomplish within the 24 hours. Some tasks included spoon feeding a fellow team member, completing the obstacle courses around the outdoor walking trail, and finding binary codes around campus and decoding them. Shoutout to the winners of the scavenger hunt: the Jumpy Woodchucks team!\nTeams were given lots of snacks and drinks to keep them fueled for the next 24 hours.\n   Collaboration is key to a successful project.\n   For dinner, we brought in some of the biggest pizzas Kansas City has to offer from Pizza 51. By dinner time the participants were ready to chow down and give their brains a break. Dinner entertainment included School of Rock and Futurama.\nChaos ensued as everyone runs to grab some pizza!\n   School of Rock was played during dinner.\n   Associates stayed overnight and worked around the clock to ensure their projects would be complete by presentation time. However, sometimes you just need a nap.\n   Presentation day is always an exciting one. After 24 hours of hard work, the teams have 4 minutes to show off their product to their peers and a panel of rock-star judges. A special thank you to Brian Wallace, Amon Woolridge, and Greg Whitsitt for judging this round.\nWe always start presentation day off with bagels for breakfast from Panera.\n   The Prize Table.\n  \nTeam Bravo Avocado ShipIt Day IX teams brought their A game. The projects get more intricate and more impressive each time around. ShipIt Day is a great way to show off the talent we have at Cerner. Team Bravo Avocado, made up of Thomas Clay, Jacob Zimmerman, Jan Monterrubio, Max Schroeder, and Elisha Boozer took first place! Judges were impressed by their live demo and humorous presentation style.\n Our project was an enhancement for Millennium Java services that removes certain required pieces of server implementation by adding custom annotations. Consumers will no longer have the same restrictions from traditional service generation (i.e. defining a generation file, method naming), because the annotation processor will generate that for them. Also, the framework can bind the structured request/reply objects to an annotated object of their choice. These annotations remove lots of redundant code and gives consumers more time to focus on what matters.\n Team Bravo Avocado with the Golden Keyboard.\n   Team E=MC Hammer Second place went to team E=MC Hammer, made up of Kyle Lipke, Nimesh Subramanian, James Boyd, Mahesh Sampekatte, and Megan McConnell.\n For ShipIt IX, our team E=MC Hammer built a tool that looks up and publishes Millennium OAuth keys. This tool was created to be an easy-to-use, easy-to-search, and updateable source of truth for all those that request and use Millennium OAuth keys. We set up an api that hosts only the data we wish to present, in a read-only \u0026ldquo;MAP\u0026rdquo; so users understand the structure of the data. We had a blast on the project and would recommend attending a ShipIt day yourself!\n Judge Greg Whitsitt noted that their project was:\n \u0026hellip;applicable across teams, solves a problem many face. A great idea and great execution.\n Team E=MC Hammer.\n   Team Jumpy Woodchucks Team Jumpy Woodchucks, made up of Hans Perera, Nathaniel Owen, Ann Dickey, James Thomas, and Pramoth Murali took third place. Their project was Chamberlain:\n Light notification system for all your notification needs that decorates your desk. This system currently supports alerts for Microsoft Outlook, Crucible, and Millennium domain health. Designed to be highly customizable and a project for teams to learn about electronics and use of Makerspace.\n Judges commented that this project was what the heart of ShipIt Day is all about.\nTeam Jumpy Woodchucks.\n   People\u0026rsquo;s Choice Awards We had a packed house for ShipIt Day presentations. Associates could watch live in the Assembly or catch it on the livestream website. One of the best parts of ShipIt Day is letting the audience join in on the fun and vote for their favorite team name, best presentation, and favorite project. Congratulations to the following teams\u0026hellip;\n Favorite Team Name: 🌝 - Venkatesh Sridharan, Ian Kottman, Anthony Ross, Karthik Nimmagadda, and Abhi Mahabalshetti Favorite Project: Message Therapist - Siddhartha Dutta, Ashish Nair, Noah Benham, and James Lawson Best Presentation: Bravo Avocado - Thomas Clay, Jacob Zimmerman, Jan Monterrubio, Max Schroeder, and Elisha Boozer  The Assembly during presentations.\n   Summary Each ShipIt Day, we see a handful of ShipIt \u0026ldquo;veterans.\u0026rdquo; It is always interesting to hear our associates take on the 24 hour event. Ian Kottman, a software engineer on the Population Health Genomics team, has a lot to say about his experience participating in ShipIt Day.\n I recently got to take part in one of my favorite events at Cerner: ShipIt Day. This was my fourth ShipIt and I was excited about the project my team had come up with. My team had named ourselves 🌝, which is one of the team member\u0026rsquo;s favorite emoji. We planned to add a UI to a tool we created last ShipIt to manage access to Cerner\u0026rsquo;s Makerspace. Our primary goal this time around was to have fun and win the coveted cheeseballs.\nIn the short 24 hours we had our team went through all of the stages of a typical software project: design, implementation, realization we overestimated, shedding unneeded features, and finally getting something working. Partway through the day we ran into issues getting Chrome to run in a Docker container and there was a brief incident involving internationalization breaking our error page, but after many whiteboard sessions and late night sodas we completed a working product. We even managed to have time for a few hours of sleep before the presentations. When it was all said and done we had accomplished our goal: we won cheeseballs for best team name! All in all, we had an excellent time collaborating and developing a tool that will be used for a long time to come. I\u0026rsquo;m already thinking about what to work on next ShipIt\u0026hellip;\n ShipIt continues to be a great way for associates to work on something they haven’t had a chance to work on, build relationships within different organizations, and most importantly, enjoy themselves. ShipIt Day IX was a great success. The projects were creative and have potential to be built upon and used in Cerner’s everyday work flow. We are already looking forward to our next ShipIt Day in December!\nIf you are interested in learning more about other ShipIt Days, see these earlier posts:\n ShipIt Day VIII Spring 2017 ShipIt Day VII Winter 2016 ShipIt Day Winter 2016 Highlight Reel ShipIt Day Fall 2016 ShipIt Day Spring 2016  "
    },
    {
        "uri": "https://engineering.cerner.com/blog/continuous-delivery-for-dcos-with-spinnaker/",
        "title": "Continuous Delivery for DC/OS with Spinnaker",
        "tags": ["engineering"],
        "description": "",
        "content": "Last fall our team (Mike Tweten, Trevin Teacutter, and Zameer Sura) started working on the problem of automating DC/OS deployments in a way that wouldn\u0026rsquo;t require multiple teams to duplicate effort and tie themselves directly to DC/OS APIs. While DC/OS certainly makes the act of deploying an application much easier than anything we\u0026rsquo;ve used previously, there are still many different ways you could choose to layer on deployment strategies and integrate with continuous delivery systems. Additionally, there may be lots of teams with very similar needs in this space and there are certainly more efficient uses of their time than making them all solve the same problems. Furthermore, while DC/OS is a good choice today we need to make sure that we don’t become locked in and can change our mind in the future without a big impact to those teams.\nBeing an engineering team, naturally our first approach was to try to write a service to manage a simple set of deployment strategies with an API that we could reimplement over other resource schedulers (like Kubernetes) in the future. However, after producing a prototype we really started to grasp that we were taking on a much bigger task than we should. Our API could handle some basic pre-built deployment workflows, but if it wasn\u0026rsquo;t flexible enough for teams we\u0026rsquo;d need to continue to adapt it. In addition, we would still have to work out how it would integrate with CI/CD tools like Jenkins, and we\u0026rsquo;d have to do a lot of that work again each time we wanted to support another resource scheduler as a deployment target.\nAt this point we decided to take a deeper look at Spinnaker, a continuous delivery platform that was open-sourced by Netflix. It had originally been developed to orchestrate AWS deployments and had expanded to other providers like Azure, OpenStack, and more recently Kubernetes was added as the first container based deployment target. Spinnaker allows for building deployment pipelines that can be triggered by things like Github commits or Jenkins builds and can then run stages to do things like start other Jenkins jobs, deploy new versions of an application, scale up or down applications, and disable old application versions. With these capabilities we would be able to use Spinnaker to provide a consistent deployment abstraction while still providing the flexibility to handle deployment workflows that we didn\u0026rsquo;t build ourselves.\nThe only minor problem was that it didn\u0026rsquo;t support DC/OS, our preferred deployment target. After taking some time to dive in and get familiar with the details it quickly became apparent that adding support for DC/OS would be much easier than continuing to build our own system, especially since we had already become familiar with the DC/OS APIs while developing our prototype. Not only would we then gain the flexibility to allow more deployment strategies than we originally anticipated, but it would also buy us integration with Jenkins and the ability to convert pipelines over to Kubernetes without much effort. We checked with the Spinnaker community to make sure that no one was already working on DC/OS support and also with Mesosphere (the company behind DC/OS) to make sure they weren\u0026rsquo;t planning to do it. When both of those came back negative we began working on the project in earnest, with the goal of building something that would benefit not only Cerner but also the entire Spinnaker and DC/OS communities.\nThis was the first time that any of us had tried to contribute significant changes to an established open-source project so there was some adjustment and learning along the way. We initially created our fork of the necessary repositories to our internal Github with the plan to finish the project before making anything visible publicly. Instead, we found that we wanted to more easily be able to share things with Mesosphere since they had agreed to give us any guidance we might need. It also became apparent that it was too easy to commit things like config that referenced internal resources, or comments that referred to our own JIRA issues. If we kept going that way we were only going to make more work for ourselves cleaning those things up when it came time to submit our changes. By moving our development to repos in Cerner\u0026rsquo;s Github organization we were able to solve both of these problems at once.\nIn spite of moving our code into the public Github we still maintained an insular development approach, determined to get everything just right before submitting a perfect polished gem to the upstream project. In hindsight, we should have been less concerned about trying to get everything right all at once and submitted more incremental changes. We thought it would be easier for everyone not to have to deal with our messy work in progress, but after our first pull request for one of the services ended up with over 130 files and a history of 100+ commits we realized that it\u0026rsquo;s too much to expect the maintainers to review so much at once, especially when they may not be that familiar with DC/OS. Ultimately, they wanted us to carve up that massive patch anyway so it would have been less work for us to do it from the start.\nAfter completing this effort we\u0026rsquo;ve found that we get even more benefit from Spinnaker than we initially expected. Spinnaker also helps us manage deployments to multiple DC/OS clusters, use Chaos Monkey to test the resiliency of our applications, and even to deploy DC/OS itself on AWS. We\u0026rsquo;re pleased to announce that our contribution has been accepted by the Spinnaker maintainers and is now available for anyone to use. The Spinnaker community has been great to work with and helped us by answering questions and inviting us to participate in the discussions with other companies that have contributed major functionality to Spinnaker. This is an exciting opportunity to contribute something back to the open-source community and we can\u0026rsquo;t wait to see where it goes from here.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/will-gorman/",
        "title": "Will Gorman",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/hbase/",
        "title": "hbase",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/michelle-brush/",
        "title": "Michelle Brush",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/structure-matters-how-cerner-handled-too-much-health-data-in-hbase/",
        "title": "Structure Matters: How Cerner Handled Too Much Health Data in HBase",
        "tags": ["engineering", "hbase"],
        "description": "",
        "content": "Background In order to manage the health of populations, Cerner builds data-driven solutions built on top of its HealtheIntent℠ platform. The platform transforms and standardizes data across disparate health systems. The data is then used in algorithms that aid healthcare providers and hospitals in managing and monitoring health both at a person and population level.\nThe Reference Record is one of the core components of the HealtheIntent platform. It represents the data known about a person\u0026rsquo;s health from a particular source of healthcare information. Downstream components in the platform aggregate information across multiple reference records to understand the full picture of a person\u0026rsquo;s health.\nTo create the record, the system performs both batch and stream processing. Some sources send the data to HealtheIntent once a day or once a month. That data is processed in batch, using Hadoop MapReduce jobs. For sources that send the data as streams of updates in near real time, we rely on a homegrown near real time framework that sits on top of Apache Storm. Both the pipelines persist their processed data in HBase tables.\nFor the data processed in batch, we write all the data about the person from a given source to the HBase table as one HBase row. We refer to this as a wide table. The HBase Rowkey is the source identifier followed by the person identifier: /source:\u0026lt;id\u0026gt;/person:\u0026lt;id\u0026gt;. Each row could have multiple columns, one per each healthcare entity like allergy, medication, procedure, etc. Another thing to note is this table is salted for better distribution of data across regions. That means a given person\u0026rsquo;s data will be in a single region since it\u0026rsquo;s all in one row, but the data for a given source will be distributed across the HBase cluster.\nFor streaming sources, we use a different table approach. We have a tall table, where each healthcare entity is written separately. The Rowkey in this approach would be built from the source identifier, person identifier, entity type, and then individual entity identifier. This allows the system to manage the information flowing from the stream more efficiently.\nTo make sure that the consumers of the record don\u0026rsquo;t have to know whether the data is coming from a streaming or batch source system, we offer a Crunch-based extract API that is capable of reading from either table structure. The API provides a single, consistent representation of the data.\nThe purpose of this post is to discuss what happened when a single person in our system received a little too much healthcare over the course of her fake life.\nThe Investigation We were bringing a new source of healthcare data onto the HealtheIntent platform using our batch processing pipeline. In the midst of this work, a small number of MapReduce jobs started failing. Our failure monitoring triggered and notified us that something wasn\u0026rsquo;t right.\nWe turned to the logs to investigate the failures. We saw this (not very informative) message:\n Task attempt_xxxx failed to report status for 601 seconds. Killing!\n Our first assumption was that the issue was related to memory, but we profiled the job and found that the heap utilization looked fine.\nQuerying across all the logs, we were able to see two key pieces of information. One, a job was more likely to fail if it was reading from a particular source of data, and two, failures were correlated with alerts on slow response times in HBase. Drilling deeper into our logs, we were able to find that sure enough, we were timing out trying to read from a particular region in HBase.\nThe next obvious step was to look at region metrics. We found the region had grown in size to 115 GB on a cluster configured to have a max region size of 10 GB. More logs showed the table had stopped compacting and splits weren\u0026rsquo;t working due to a 30 GB row in the table.\nAt this time, anything trying to read that row was failing, but some large writes were still succeeding. There would be attempts to bulk load new data to the region, but the loads would invariably fail due to the large row. When the job ran again, it would attempt to write the same data, over and over. Occasionally during these attempts, some of the data would get written to the region. This process piled more and more data onto the region, and the region wouldn\u0026rsquo;t split.\nAs the region was growing, the situation got worse. As previously mentioned, we salt the row keys to distribute data across the cluster. Jobs interested in other source data were also reading from this region, so we started seeing failures in other jobs. Then in the midst of our investigation, we saw more HBase region servers start failing. Then HBase bulk loads in other MapReduce jobs were also failing. We knew we had to get rid of the row and split the region.\nAs some team members worked on a plan to deal with the row, others moved on to answering the question, \u0026ldquo;How did this one row get so big that the bulk loads started failing?\u0026rdquo;\nWith healthcare data, it\u0026rsquo;s possible to imagine diseases that might lead someone to have so many encounters and activity within a single healthcare system that we could get to a gigabyte of health data about a person. However, we had never seen anything near this large before. It was a pretty significant outlier.\nGetting Rid of the Alternative Fact We decided to delete the person\u0026rsquo;s data. However, the person had grown so large, this proved difficult. (The naive approach to deleting a row in HBase failed spectacularly.) Splitting was also not possible.\nWe needed a better plan. First, all reads and writes to the region had to stopped. This meant suspending all jobs that any potential to read or write. Since several teams depended on our data for their processing, we started by notifying all our downstream consumers. Then we stopped all reads and writes to the affected region. We copied the affected Hfiles to a new table. This allowed compaction (and as a result, deletion) to succeed. Cleaning up this one bad record brought the region size down to 2 GB from 200 GB. As part of this process, the table was disabled and the compacted Hfiles from the fixed region were copied back to the old table before it was re-enabled.\nDamage When we stopped all the jobs loading into that table, we had a stale data period for downstream processing. Reports that needed the data showed old information during the length of the incident investigation and mitigation. The good news is that our near real-time algorithms such as our Sepsis alerts and readmissions risk assessments were built on top of the stream processing system which was not impacted by this incident.\nPrevention In our postmortem for the incident, we reviewed what we could have done better before, during, and immediately following the situation. A quick win for us was to add HBase region size monitoring \u0026amp; alerting. It is still reactive, but would help us react faster and minimize the duration of the event.\nUltimately, the root of the problem is that we have an approach that encourages a lot of data to accumulate in a single row. We had a design flaw. While this patient was outside the realm of anything we\u0026rsquo;d seen before, it wasn\u0026rsquo;t outside the realm of possibility. To quote an oldie, but goodie from Frederick Brooks, \u0026ldquo;Our first response should be to reorganize the modules' data structures.\u0026rdquo;\nThis event made us realize our batch and streaming representations of the data need to converge. That is the path we\u0026rsquo;re now taking which will introduce its own set of challenges, but we\u0026rsquo;re confident this will land us in a more stable, scalable place.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/vinay-koduri/",
        "title": "Vinay Koduri",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/managing-splunk-knowledge/",
        "title": "Managing Splunk&#39;s Knowledge",
        "tags": ["engineering", "splunk"],
        "description": "",
        "content": "When we first were given access to Splunk we were excited about all the functionality it could provide our team to help us monitor and debug our applications. We created alerts to email us if our applications are logging errors, dashboards to show health or metrics of our services, and field extractions as well as tags to make searching easier. As we created more and more of these Splunk knowledge objects we started to have issues. For regulatory and validation concerns, we have several Splunk instances that represent different environments. This required us to copy and paste or duplicate the Splunk knowledge object we created in one environment to all the others. This became very tedious and we saw all sorts of problems. Objects were only copied to some Splunk instances, updates were sometimes lost, and permissions weren\u0026rsquo;t consistent or someone would forget to make their object publically viewable. We set out to look for a solution.\nAfter we couldn\u0026rsquo;t find anything that could easily help us we created a tool called splunk-pickaxe. splunk-pickaxe is a Ruby gem that provides a command-line interface for synchronizing a directory or repository of Splunk knowledge objects written as files to a Splunk instance. So now when a developer creates a new Splunk object, they would create and test it in our Splunk dev instance and then open a pull request to our Splunk Github repository with the object\u0026rsquo;s configuration or contents . Other developers review the changes before it gets merged and automatically pushed out to all our Splunk instances using our splunk-pickaxe tool and Jenkins our continuous integration engine. This simplified things and provided consistency on these Splunk changes. We were also able to more effectively review the Splunk objects for efficient and proper Splunk usage.\nThe tool grew in popularity at Cerner and currently supports many Splunk objects:\n Dashboards Alerts Reports Field Extractions Tags Event Types  Check out the Getting Started section to try it out and feel welcome to contribute any ideas or improvements.\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/splunk/",
        "title": "splunk",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/grace/",
        "title": "grace",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/hopper/",
        "title": "hopper",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/internationalization/",
        "title": "internationalization",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/localization/",
        "title": "localization",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/polyglot/",
        "title": "polyglot",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/you-may-not-be-a-polyglot-but-your-code-needs-to-be/",
        "title": "You May Not Be a Polyglot, but Your Code Needs to Be",
        "tags": ["engineering", "polyglot", "localization", "internationalization", "grace", "hopper"],
        "description": "",
        "content": "Last October, I had the privilege to speak at Grace Hopper, the world\u0026rsquo;s largest gathering of women technologists. It was impressive to see 15,000 fellow female technologists gather together to share their experiences and technical expertise. To look into the audience, and see a room full of other female engineers was a great experience!\nMy talk was titled, You May Not Be a Polyglot, but Your Code Needs to Be (previously presented at Midwest.io). One definition of the term polyglot is \u0026ldquo;composed in many languages.\u0026rdquo; During my talk, I discussed common misconceptions about writing code for global audiences, and I provided tips to help position engineers for success in these types of projects.\nHaving worked in engineering, translation, and localization for more than over a decade, I have seen first-hand how projects struggle through the globalization process. A properly globalized solution is generally achieved by ensuring the appropriate internationalization, localization, and translation steps have taken place; however, as developers, we have misconceptions that make this process more costly and less effective than it needs to be.\nTo help set the stage, I began my talk by speaking in Spanish. I love watching the audience react to my comments, as I usually say something silly during the introduction, so those who understand react one way, while others in the audience tend to look puzzled.\nOne of the most common misconceptions when coding for a global audience is the infamous statement: \u0026ldquo;I\u0026rsquo;ll NEVER need to do this.\u0026rdquo; The most common rationalization for this misconception is that in the U.S., we only speak English, however, one in five U.S. residents speaks a foreign language at home. This is a record 61.8 million residents who speak languages other than English. Of these residents, 25.1 million (41 percent) told the Census Bureau that they speak English less than very well. Additionally, data from 2013 shows that there are seven languages with at least one million speakers in the US. These languages were Spanish (38.4 million), Chinese (three million), Tagalog (1.6 million), Vietnamese (1.4 million), French (1.3 million), and Korean and Arabic (1.1 million each). By only serving one audience, we are ignoring many customers and consumers of our solutions. Even in my adopted hometown of Kansas City, 6% of the population speaks Spanish as their primary language.\nAnother common misconception is that internationalization and localization are the same. Here are some definitions according to the Word Wide Web Consortium (W3):\n  \u0026ldquo;Internationalization is the design and development of a product, application, or document content that enables easy localization for target audiences that vary in culture, region, or language.\u0026rdquo; \u0026ldquo;Localization refers to the adaptation of a product, application or document content to meet the language, cultural and other requirements of a specific target market (a \u0026lsquo;locale\u0026rsquo;).\u0026rdquo;   I like to think of their relationship more like this: i18N + L10N == G11N While the W3 notes, \u0026ldquo;some people use other terms, such as globalization to refer to the same concept\u0026rdquo; as internationalization,\u0026quot; I see the appropriate execution of internationalization and localization together as providing a globalized solution.\nAmong examples of appropriately globalized solutions, TripAdvisor is one I like to highlight because it has a broad user base, and many of its users don\u0026rsquo;t realize it is used by others around the world in their own language. The following Trip Advisor elements help provide a positive user experience:\n Flexing dates to show in the appropriate locale Removing content that is not available in the target language (newsletters are a good example) Accurate currency conversion Allowing for text expansion to avoid truncation Flexing logos and the brand to ensure that it is appropriate for the locale  Another common misconception when creating globalized applications is that once something has been internationalized and translated, it can be shipped. The best way to highlight the folly of this perception is to look at some marketing mishaps. Below are some of my favorite ones:\n  A famous detergent from the Middle East, is white as snow, so it is called by the word for snow in Farsi. This doesn\u0026rsquo;t work well when it is referenced in English-speaking countries. In Farsi, the word for snow is barf. I also have a personal story. I come from a tropical country, and my native language is Spanish. When I first moved to the United States, I realized that I was not genetically engineered to be in the cold, dry weather of a Kansas City winter. My skin was itchy, and I desperately need some good hydrating lotion. My friends at school recommended something called Sarna. I was convinced they were trolling me because in Spanish, Sarna means Scabies. I have endured more than a dozen Kansas City winters, and I still can\u0026rsquo;t make myself use that lotion, because I can\u0026rsquo;t get over the name.   Translation alone is not a solution. The translator must be proficient. In a previous position I was frequently asked how long it takes to learn a language. I was fighting the misconception that anyone who has taken two years of a language could translate. After failing to articulate why that was a misconception, I found a wonderful tool, thanks to the Foreign Language Institute and the Interagency Language Roundtable (ILR), who created a scale that is a set of descriptions of abilities to communicate in a language. One important aspect of this scale is that it is accurate when the evaluations are done by native speakers. It was originally developed by the United States Foreign Service Institute (FSI), and is still widely known as the FSI scale. It consists of descriptions of five levels of language proficiency.\n Level 1: Elementary Level 2: Limited Working Level 3: Professional Working Level 4: Full Professional Level 5: Native or Bilingual  Then the scale estimates how long it takes to achieve that level of proficiency when a student is immersed learning the language at least six hours a day and at least five days per week. It then breaks down the languages by groups based on their difficulty:\n\u0026ldquo;Easy\u0026rdquo; languages are French, German, Indonesian, Italian, Portuguese, Romanian, Spanish, and Swahili:\n 8 weeks (240 hours) to achieve level 1/1+ 16 weeks (480 hours) to achieve level 2 24 weeks (720 hours) to achieve level 2+  \u0026ldquo;Hard\u0026rdquo; languages are divided into the following groups.\n Group 2: Bulgarian, Burmese, Greek, Hindi, Persian, Urdu. Group 3: Amharic, Cambodian, Czech, Finnish, Hebrew, Hungarian, Lao, Polish, Russian, Serbo-Croatian, Thai, Turkish, Vietnamese. Group 4: Arabic, Chinese, Japanese, Korean)  To learn these languages, you must spend the following time:\n 12 weeks (360 hours) to achieve level 1/1+ 24 weeks (720 hours) to achieve level 1+ /2 44+ weeks (1320+ hours) to achieve level 2/2+  Taking a language for two years (usually college or high-school) is simply not enough to achieve the appropriate level of proficiency needed to translate. Even native speakers are not effective at translation. Becoming proficient in a language is very challenging, and cultural aspects are important as well.\nAdditionally, brain activation patterns identified by the University of Turku in Finland show extensive activation of the brain during non-native language use, specifically, the left dorsolateral frontal cortex, which is associated with lexical search, semantic processing and verbal working memory. Brain activation patterns were clearly modulated by direction of translation, with more extensive activation during translation into the non-native language, which is often considered to a be more demanding task. These are not the same patterns used when speaking a language, though, as different brain connections need to exist.\nMachine translation has become more effective in the last few years, but only language experts should leverage it as part of their toolset. Translating something into a language you don\u0026rsquo;t understand is a recipe for disaster. Simply go to an online translation tool, put in a phrase, convert it to another language, then convert it back. Chances are, the original does not match the twice-translated phrase.\nIf you are relying on professional translators, another common misconception is that translation can simply happen overnight. The truth is it CAN happen, just NOT well. A professional translator can usually translate about 2000 words per-day. If they are doing localization testing as well, you have to budget about twice the time for the regression test plans to be completed, as words will iteratively need to be changed as context is understood within the software.\nNow that I have covered the most common misconceptions with internationalization, localization, and translation, the next portion of this blog will focus on practical tips.\nAs a developer, it is imperative that you understand your global strategy and what outcome the business is trying to accomplish. You don\u0026rsquo;t want to invest a lot of development time making sure you can handle Right-To-Left languages or those that are double-byte if your business isn\u0026rsquo;t planning on going to those markets.\nCreating appropriate partnerships with language providers is key to successfully globalizing software, as they can help handle many aspects of the globalization process from translation to localization. Speaking to companies who are already globalizing their software is a good way to find good providers.\nIf you have to use free online tools to translate something internally (where the quality doesn\u0026rsquo;t need to be high), make sure you understand the licensing of the tool. Some free tools will own your content once you run it through their translation engine.\nIncluding internationalization as part of your development process is a key aspect to globalization success. This means it should be included in:\n Design, including user interface design (UX) Estimates Testing Deployment  Localization testing is a new step that should be included in your process, which requires language experts to be involved. From a development perspective, there are a lot of existing artifacts that can be leveraged in this process, such as regression and feature test plans. It is always helpful if those artifacts can be separated into those that have UI changes or not.\nLocalization testing will likely focus on language changes and impacts to the UI. A byproduct of localization testing is that other types of defects might surface. Having a good process so that bugs can be reported and tracked is key.\nIf you are just starting down the road of internationalization, be sure you don\u0026rsquo;t reinvent the wheel on how to do it. Programming languages usually have well-documented libraries that can be used. If you are already internationalizing and working with a localization team, make sure you are commenting your code to provide clarification to the translators. Below is an example of a popular localization tool:\n   Once you have incorporated all the steps for localization into your process, it is important to understand that your text changes will now have financial impact as they will affect translations. Translation memories are commonly used by translators, and any change to the source text will trigger a new translation to be required.\nTranslation companies typically charge per word. The average cost per word in 2012 Common Sense Advisory\u0026rsquo;s 2012 survey of more than 3,700 suppliers in 114 countries, using 220 language pairs, found that the average per-word rate for translation for the 30 most commonly used languages on the web is around 13.4 cents. The more impactful implication to your process is that text changes will trigger your localization testing cycle as it will be important to verify the new translations are correct in context.\nLet\u0026rsquo;s discuss some quick principles that you should be able to apply regardless of what programming language you use.\nDon\u0026rsquo;t concatenate strings. Language order matters with translation. Instead of doing this:\n1 2 3 4 5  \u0026lt;div style=\u0026#34;font-family: monospace\u0026#34;\u0026gt;function getDescription() { var color = getColor(); var element = getElement(); return color + \u0026#34; \u0026#34; + element; }\u0026lt;/div\u0026gt;   Do something like this:\n1 2 3 4 5 6  \u0026lt;div style=\u0026#34;font-family: monospace\u0026#34;\u0026gt;function getDescription() { var color = getColor(); var element = getElement(); return getLocalizedString (\u0026#39;elementDescription\u0026#39;, color, element); } elementDescription = {1} {0}\u0026lt;/div\u0026gt;   Refraining from concatenating strings allows lthe localization professional to ensure that grammar is appropriately represented in the language. If you concatenate strings you will be forcing someone to say something like:\n\u0026ldquo;House Red\u0026rdquo; instead of \u0026ldquo;Red House.\u0026rdquo;\nSpeaking of names, be mindful how you use them in an application. Build appropriate abstraction and flexibility. Names can cause headaches in localization because first names aren\u0026rsquo;t always first, people can have multiple last names, and middle names might not be there at all.\nI have a personal story about this. When I went to file my marriage license, we had a very interesting conversation with the clerk. He asked me what was my middle name. When I said that I don\u0026rsquo;t have a middle name, he said the system had it as a required field. I suggested that he enter a space\u0026hellip;after about 15 tense seconds he confirmed it had worked. This was in early 2000, and I hope things have changed, but since government moves at the speed of snails, I doubt it has.\nMy final tip to better handle localization is to always use the FULL locale, that is, both language and country code to allow for maximum flexibility and to budget more time than what you believe you will need for your project. This last tip sounds very lame, but experience has taught me that chances are you won\u0026rsquo;t do it. Engineers tend to be very optimistic on their estimates, and generally fail to allocate sufficient time.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/shahzad-zafar/",
        "title": "Shahzad Zafar",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/shipit-day-viii-spring-2017/",
        "title": "ShipIt VIII Day: Spring 2017",
        "tags": ["culture", "engineering", "hackathon", "shipit"],
        "description": "",
        "content": "   ShipIt Day VIII was held on April 20 and 21st. Seventeen teams made up of internal associates had just 24 hours to create something innovative, usable, and value-adding. This quarter\u0026rsquo;s ShipIt Day was held at our new Innovations Campus in the Link Conference Center. We were excited to get to utilize the space for this event. Another awesome addition to this quarter\u0026rsquo;s ShipIt Day was our new game rooms. We hosted a Donkey Kong challenge and the winner (@russ_starr) received a five-pound gummy bear!\nThe start of a long day for participants!\n   Donkey Kong.\n  \nParticipants were given loads of snacks, the most important being Monster Energy Drinks!\n     \nAfter a long day of hard work, everyone was ready to eat extra big pizzas from Pizza 51. Eleven big pizza\u0026rsquo;s and one hundred and ninety-six breadsticks were surprisingly enough to feed the hard working brains.\n     \nAfter dinner, it was back to work for the teams. Later on in the evening, we played movies including Rogue One and MST3K. Participants also battled for the top Donkey Kong score.\nWatching movies while working on projects.\n   Friday, April 21st rolled around and by 9:00am the teams were getting antsy to present their projects. Since this was our first ShipIt Day at the new Innovations Campus, we were able to utilize the Assembly space.\nOld school advertising for ShipIt Day Demo\u0026rsquo;s- chalk!\n   Prize table\n  \nShipIt Day VIII brought to life some incredible projects. Team Freudian Ship, made up of Kate Young, Erin Dorpinghaus, Courtney Ground, Chris Farr and Sean Albery, took first place (and the golden keyboard!) with their project.\n The Intellectual Property (IP) Analytics team is tasked with managing a data warehouse. This includes ensuring that data updates take place in \u0026ldquo;jobs.\u0026rdquo; When a job fails it results in stale data and this can impact thousands of associates. This project took advantage of Amazon Web Services by connecting tasks associated with managing the database to a voice command Alexa skill which results in the ability to manage the database from anywhere the user has an internet connection. To put it in perspective, the database admin will now be able to ensure that the database is up to date without even getting out of bed in the morning. A simple voice command to an Amazon Echo Dot (which each of us won as a prize) will result in the restarting of failed jobs and even logging JIRA tickets to begin the troubleshooting process if needed.\n ShipIt Judge Dan Stocksick was impressed with the project, \u0026ldquo;Great use of consumer products into our ecosystem.\u0026rdquo;\nTeam Freudian Ship with the coveted Gold Keyboard!\n   Second place went to team Flowalva (Wuchen Wang and Dhruv Patel).\n Our ShipIt project was a traffic visualization system for web services supporting Millennium. In this project we used Splunk API to collect the data from services and visualize the traffic between nodes in using Netflix\u0026rsquo;s vizceral project.\n Judge Jason Heiting was impressed. \u0026ldquo;Very neat visualizations. Intuitive UI for navigation.\u0026rdquo;\nLast but not least, was third place winners, Flying Mongoose (Cihan Kaynak, Matt Stramel, Mahesh Acharya, Taylor Clay).\n We created a cross platform mobile app which can be used by a pharmacy technician while picking up and scanning medications at the pharmacy to refill Cerner\u0026rsquo;s RxStation cabinets. Prior to this project, an RCP based desktop app was running in a PC mounted on a cart with an attached barcode scanner and label printer. This type of bulky setup was reducing the mobility of the technician.\n The Assembly was crowded with associates watching presentations.\n   Part of the ShipIt Day fun is livestreaming the presentations and allowing our associates from all over to participate by voting for their favorite team name, favorite presentation and favorite project. The winners of the categories are as followed:\n  Favorite Presentation: Sip It - Patrick Gross, Adam Tibbs, Mario Perez, Robert Cornett, Trey Peek\n  Favorite Team Name: CHAPPiE - Karthik Nimmagadda, Manidee Gattamaneni, Venkatesh Sridharan, Prasanth Chakka, Wendell Wolfe\n  Favorite Project: Freudian Ship - Kate Young, Erin Dorpinghaus, Courtney Ground, Chris Farr, Sean Albery\n  People\u0026rsquo;s Choice Winners with the coveted cheeseballs!\n   ShipIt continues to be a great way for associates to work on something they haven\u0026rsquo;t had a chance to work on, build relationships within different organizations and most importantly, enjoy themselves. Ian Kottman, an engineer on the Population Health Genomics team, has participated in multiple ShipIt Days. The reason: \u0026ldquo;I enjoy getting the time to create something that will make someone\u0026rsquo;s life easier.\u0026rdquo; Carlo Filipelli, system engineer, also agrees, adding on, \u0026ldquo;ShipIt is great because it allows associates to work on interesting projects that they might not be able to get to during normal work hours. It fosters innovation and new ideas that would typically get overlooked in our busy day to day jobs. It is also a conducive environment for learning, collaboration, and having some fun!\u0026rdquo;\nThank you to all our participants and a special thank you to judges Jason Heiting, Justin Morrison, Dwight Sloan and Dan Stocksick.\nMark your calendars: ShipIt Day IX will be held in July 2017. If you are interested in learning more about other ShipIt Days, see these earlier posts:\n ShipIt Day VII Winter 2016 ShipIt Day Winter 2016 Highlight Reel ShipIt Day Fall 2016 ShipIt Day Spring 2016  "
    },
    {
        "uri": "https://engineering.cerner.com/tags/awards/",
        "title": "awards",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/influencing-the-young-spanish-seaking-female-generation/",
        "title": "Influencing the Young, Spanish-speaking Female Generation",
        "tags": ["engineering", "awards", "stem"],
        "description": "",
        "content": "Outside of work, married couple Denisse Osorio de Large, a Cerner Director in Population Health Development, and Steven Large, a Cerner Senior Director in IP Development, are passionate about creating a younger generation that is passionate about technology and introducing them to the vast world of opportunities in software development. In addition to volunteering in the STEM community, helping organize Cerner’s development conference (DevCon), and speaking at conferences, Denisse and Steven enjoy working with children in the spanish-speaking community.\nLast year, Denisse (who is Colombian and a spanish-speaking native) and Steven (who is fluent in spanish) hosted two coding events specifically catered to spanish speaking families in partnership with Girl Scouts. The goals of these events, “Pastelitos y Programación” (“Coding and Cupcakes”), were to provide a safe and inclusive environment for girls to program, grow leadership abilities, and love solving problems with code. Girls enjoyed completing coding challenges from code.org and playing Labyrinths for Angry Birds. In the game, the girls had fun figuring out how to guide birds through a labyrinth to the pigs (see more about the event in the video below) using programming concepts.\n  In recognition of their passion for developing future innovators in STEM, Girl Scouts honored Denisse and Steven Large for their work in hosting the Pastelitos y Programación (Coding and Cupcakes) at Cerner. The Girl Scouts Innovator Award recognizes volunteers who saw a problem and developed an innovative solution to further Girl Scouting. These family events help bridge the communication gap between parents and girls when it comes to participating in Girl Scout events. With their passion and dedication for volunteering in the community, Denisse and Steven are encouraging more girls to become involved with STEM activities. Congratulations, Denisse and Steven!\n   "
    },
    {
        "uri": "https://engineering.cerner.com/blog/influyendo-en-una-generacion-de-jovencitas-de-habla-hispana/",
        "title": "Influyendo en una generación de jovencitas de habla hispana",
        "tags": ["engineering", "awards", "stem"],
        "description": "",
        "content": "Cuando no están trabajando, Denisse Osorio de Large, Directora de Cerner en Desarrollo de Salud Poblacional y Steven Large, Director sénior de Desarrollo de IP, quienes están casados, tienen como pasión crear una generación joven motivada por la tecnología y mostrarle a la misma el amplio mundo de oportunidades existentes en el desarrollo de software. Además de dar su tiempo como voluntarios en la comunidad promotora del área de ciencias, tecnología, ingeniería y matemáticas (STEM), contribuyen a organizar la conferencia de desarrollo de Cerner (DevCon) y realizan presentaciones en conferencias, Denisse y Steven disfrutan trabajar con niños de la comunidad de habla hispana.\nEl año pasado, Denisse, originaria de Colombia y cuya lengua materna es el español, y Steven, quién habla español, organizaron dos eventos de codificación ofrecidos específicamente a familias de habla hispana en colaboración con la agrupación Girl Scouts, niñas exploradoras. Los objetivos de estos eventos de \u0026ldquo;Pastelitos y Programación\u0026rdquo;, Coding and Cupcakes, consistieron en ofrecer un entorno seguro e incluyente para que las chicas llevaran a cabo actividades de programación, desarrollaran capacidades de liderazgo y un gusto por la resolución de problemas mediante la codificación. Las jóvenes disfrutaron la satisfacción que brinda resolver los desafíos de codificación de code.org y de juegos como Laberintos de Angry Birds. En el juego, usando conceptos de programación, las chicas se divirtieron al encontrar la forma de guiar a las aves que, atravesando un laberinto, debían llegar a los cerdos (ver video del evento a continuación).\n  Como reconocimiento por su pasión por desarrollar a futuros innovadores en STEM, las Girl Scouts, honraron a Denisse y a Steven Large por conducir el evento Pastelitos y Programación, Coding and Cupcakes que se llevó a cabo en Cerner. El Premio de Innovador de las Girl Scouts, reconoce a los voluntarios que perciben un problema y desarrollan una solución innovadora para fomentar las actividades de escultismo para las chicas, Girl Scouting. Estos eventos familiares contribuyen a favorecer la comunicación entre padres e hijas al participar en eventos de Girl Scouts. Por su pasión y dedicación como voluntarios, Denisse y Steven alientan a cada vez más niñas a participar en las actividades de STEM. ¡Enhorabuena, Denisse y Steven!\n   "
    },
    {
        "uri": "https://engineering.cerner.com/authors/lindsay-ullyot/",
        "title": "Lindsay Ullyot",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/stem/",
        "title": "stem",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/bradley-scott/",
        "title": "Bradley Scott",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/one-cerner-style-icons/",
        "title": "One Cerner Style Icons: The future of icons at Cerner",
        "tags": ["engineering"],
        "description": "",
        "content": "One Cerner Style Icons (OCS Icons) are foundational UI elements required to produce a cohesive and consistent User Experience for Cerner Solutions. OCS Icons consist of a comprehensive icon library of platform agnostic assets which can be easily consumed by development teams.\nWhy? The primary goal of this effort to elevate every aspect of the UI to create the best possible user experience. In order to do that, it is imperative to develop a system composed of strong design patterns and well-founded user research. By creating a single repository of platform agnostic icons, we are able to better manage icons and create a framework that makes consuming icons easier and more efficient. By reducing the number of separate libraries, styles and formats, we ease the burden on implementation for developers. In addition, we dramatically reduce the amount of effort needed to maintain a host of different asset libraries while producing a higher quality product. Going open source also facilitates a more seamless process when icons need updated or when styles change and evolve.\n   What? OCS Icons are the core iconographic communication vehicle between the functional software and the UI. The icons are designed on the foundation of a strict icon language consisting of core design principles and uniform style. Proper implementation requires consumers to be on a code guide and obtain assets by referencing codified Scalable Vector Graphics. To ensure consistent implementation, icon packages are versioned and carefully managed.\n   Who? The individuals responsible for One Cerner Style Icons are: James Bray, Seth Claybrook, Brett Jankord, Will Reynolds and Bradley Scott.\nRepository Link https://github.com/cerner/one-cerner-style-icons\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/seth-claybrook/",
        "title": "Seth Claybrook",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/chef/",
        "title": "chef",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/mike-rzepka/",
        "title": "Mike Rzepka",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/version-databag-a-chef-release-process/",
        "title": "Version Databag - A Chef Release Process",
        "tags": ["chef", "engineering"],
        "description": "",
        "content": "At Cerner, we use Chef. In fact, we\u0026rsquo;re heavily ingrained with Chef in our configuration management practices. We deploy services from Tomcat to Kafka with Chef. Even the first open source project we announced was a tool for Chef!\nWith all of this integration with Chef, we need a simple way to manage all of those versions. This is where the Version Databag cookbook comes in. At a high level, this cookbook allows us to define our versions in a centralized data bag (grouped under conceptual units) and populates the corresponding node attributes required with those versions at runtime. This significantly reduces the effort involved in finding, updating, and maintaining the various versions of things that we have across our Chef configuration.\nBecause we found it so useful, we decided to open source it! We encourage you to try it and manage Chef versions with it for easier configuration management practices. If you feel that it could be further improved, let us know (or better yet, submit the improvement yourself!). We\u0026rsquo;re welcome to take enhancements and feedback to improve our processes.\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/engineers-on-the-road-for-smart-and-fhir/",
        "title": "Engineers on the Road for SMART and FHIR (CHC 2016)",
        "tags": ["SMART", "FHIR", "engineering"],
        "description": "",
        "content": "Over the past few years, Cerner has been developing a standards-based platform on top of our Cerner Millennium® EHR. Rather than roll our own API, we’ve been using the HL7® FHIR® standard. For app integration, we’ve been using the SMART® on FHIR specification. If you’re not immediately familiar with those acronyms, you’re definitely not alone.\nAs we were developing the services, we were also fielding questions from 3rd party developers, answering a lot of questions internally, and trying to keep up with the specifications themselves. We knew that we would need to help provide education around our ecosystem, our implementation of the FHIR and SMART standards to developers since the standards are new and evolving.\nThis need will become much more pressing because Meaningful Use 3 will trigger deployment of our implementations out to most of our Millennium EHRs. Meaningful Use introduces requirements from the Centers for Medicare and Medicaid (CMS) to modernize the US healthcare infrastructure. This is nothing new, but we’ve been hard at work to support stage 3, which includes the requirement for patients to access their data via an API. In order to attest, most of our clients will then require our implementation of the FHIR API.\nThis past November offered up two exciting opportunities to provide hands-on instruction and support, which we’ll cover in separate posts. In this first post, we want to highlight the work we did at our annual Cerner Health Conference. We decided that this year’s event would be a great opportunity to offer a small Code Learning Lab - providing education for some of the APIs that Cerner has available.\nThe Code Learning Lab was run from November 14th - 17th in Kansas City with our annual CHC conference. The goal: give developers a hands-on training session with the different APIs that are part of the Cerner Open Developer Experience (code). One of the tracks covered SMART and FHIR specifically, while another went over some of the APIs available with our HealtheIntent and HealtheLife platforms.\n   We saw participation from many of our clients and partners, from both the US and Canada. Over the four days, the lab was in the format of an overview or \u0026ldquo;lecture\u0026rdquo; followed by a longer hands-on lab to put what participants were learning to use. During the labs, our engineers would walk around to check on how everyone was doing, answer questions, and help troubleshoot.\n   We received a lot of comments and excitement from everyone participating around the fact that our engineering team itself was involved in the learning lab. It was also very exciting for the engineering team to watch everyone try out our newer developer tools, read our documentation, and then put it to practice. Not only were the participants able to actually see the data coming back, but there were a lot of conversations that occurred that will help us improve the class (and our tooling) in the future to make the event better. In the end, it was a great experience for everyone involved!\nIf you’re interested, you can check out some of the presentations and labs that were created for the learning lab here: https://github.com/cerner/ignite-learning-lab and our SMART tutorial: http://engineering.cerner.com/smart-on-fhir-tutorial/#prerequisites\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/jenni-syed/",
        "title": "Jenni Syed",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/smart/",
        "title": "SMART",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/apache/",
        "title": "apache",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/crunch/",
        "title": "crunch",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/micah-whitacre-recognition-blog/",
        "title": "One Cerner Associate&#39;s Contributions in the Tech Industry",
        "tags": ["apache", "crunch", "PMC", "engineering"],
        "description": "",
        "content": "Background: Micah is currently a software architect in Population Health Development in Healthe Intent Development at Cerner. In 2013, Micah Whitacre received committer status on the Apache Crunch project, and is now a Project Management Committee (PMC) member of the project.\nHow has Cerner been been involved in Apache Crunch? In 2012 and 2013, Cerner started using this project within several solutions. We also wanted to give back to the community and decided to invest time in helping answer questions and concerns, and overall project needs. We wanted to get involved in small community like Apache Crunch since it brought great value to our Cerner solutions.\nWhat does it mean to be receive committer status on Open Source? In 2013, I was selected as a committer on Apache Crunch. In reference to the technical aspect, Apache projects have some form of criteria for when to accept a committer to a project. Election to being a committer is the result of consistent and high quality contributions to the project, through documentation, code, or helping to build the overall community.\nBeing selected as a committer was an unexpected honor. It\u0026rsquo;s also an additional responsibility for my involvement with the project. I closely watch for questions and make enhancements and functionality code changes. There are fourteen other committers on the project as well.\nWhat does it mean to be an Apache Crunch PMC? After being a committer for over a year, I was selected as a PMC of Apache Crunch. The goal of a PMC is to set the pace and the direction of the community, as well as facilitate how the community is interacting. PMCs help drive releases and help grow the committee. Some of my other responsibilities also include participating in discussions about voting people into the community, new releases of code, and which functionalities need to get fixed.\nWhat are your favorite parts about being a committer? One of the coolest parts about being a committer is making new connections with larger tech community. It forces you to build new relationships and talk to people you may have not spoken to before. It\u0026rsquo;s cool being able to meet new people who don\u0026rsquo;t work at the company you\u0026rsquo;re working for. I also enjoy being able to look at the project as a whole and figure out the right way to solve something, rather than solving it right now.\nI\u0026rsquo;ve also spoken at ApacheCon twice on Apache Crunch, which have been awesome opportunities.\nHow else are you involved in the tech industry? I also like to be involved in our development culture at Cerner, helping improve our internal community and increasing awareness of Cerner in the industry. I\u0026rsquo;ve been a member of the planning committee and have spoken at our internal conference, DevCon, given internal tech talks at Cerner, and have also spoken at industry conferences like Kafka Summit 2016 and Midwest.io 2014. I\u0026rsquo;ve also written an Engineering Health blog (Scaling People with Apache Crunch). I am an open source reviewer for Cerner projects, helping review and provide feedback on Cerner open source projects prior to them being published.\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/pmc/",
        "title": "PMC",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/automated-deployment-with-apache-kafka/",
        "title": "Automated Deployment with Apache Kafka",
        "tags": ["engineering", "kafka"],
        "description": "",
        "content": "It\u0026rsquo;s likely not a surprise that Cerner would use Apache Kafka as we have used a number of related technologies like Apache Hadoop along with its Map/Reduce, HDFS and even Apache HBase. Our team first started using Apache Kafka in 2014 when Kafka 0.8 first came out. Since then we\u0026rsquo;ve expanded to using Kafka for a number of different use cases (1, 2) and it has become a core piece of Cerner\u0026rsquo;s infrastructure.\nJust like the applications we create, we also needed to automate the deployment of Kafka to handle the ever growing amount of operations work and to ensure a high level of consistency with our environments. Chef was an obvious choice for us since its been our deployment tool of choice for the last few years. We quickly put together a cookbook to help us automate the deployment of the many Kafka clusters here at Cerner. We have continued to support and use this cookbook after open-sourcing it upgrading it to handle newer versions of Kafka (0.9, 0.10), handle Kerberos authentication, and many other improvements.\nIf you use Apache Kafka, feel free to try it out and let us know how it works for you.\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/apis/",
        "title": "apis",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/beadledom/",
        "title": "beadledom",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/beadledom-simple-java-framework-for-building-rest-apis/",
        "title": "Beadledom - Simple Java framework for building REST APIs",
        "tags": ["engineering", "beadledom", "java", "REST", "apis"],
        "description": "",
        "content": "Beadledom is a framework for building Java based web services. It bundles several open sourced components required for building JAX-RS services.\nWhy? HealtheIntent, Cerner’s population health management platform, started 3.5 years ago. We went through the process of investigating different technologies for the platform. We decided on using Java for building services and arrived on a set of libraries that we believed work well together.\nThe long history of Java has led to an ocean of libraries that are available to Java developers. Picking up the right set of tools is very important to get the job done the right way. The trickiest part is to know what “the right set of tools” are. This is only the first of many tough decisions that a developer makes during the lifecycle of a project.\nIf choosing the libraries for a project is a game, then getting them all to play well together is a whole new game. Beadledom not only brings all the awesome java libraries together but also binds them well, making the developer\u0026rsquo;s life easy. It gives a good head start to the developer by letting them start from a strong foundation.\nAt Cerner we understand how important and tedious these decisions are and we always want to contribute back to the community. So we open sourced Beadledom because it helped us develop our platform quickly and consistently. We hope that others in the community can also leverage its power.\nWhat? Beadledom uses several open source projects that are widely used and well maintained. Below is a list of a few of the major components that Beadledom uses.\nGuice for gluing and bootstrapping components; Jackson for JSON serialization/deserialization; Resteasy for JAX-RS implementation; Stagemonitor for Monitoring and metrics; Swagger for API Documentation; Apache Commons-Configuration for handling different types of configurations consistently. Where? You can find the source code on GitHub. Please find our documentation at http://engineering.cerner.com/beadledom/.\nWho? Below are the core developers of Beadledom: John Leacox Sundeep Paruvu Nimesh Subramanian Brian van de Boogaard Supriya Lal Here is the complete list of contributors who made Beadledom awesome.\nCookie Cake Time! Cerner takes pride in our open source contributions. When teams contribute to the community Cerner rewards them with a cake. Here is the cookie cake for Beadledom.\n   "
    },
    {
        "uri": "https://engineering.cerner.com/tags/java/",
        "title": "java",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/rest/",
        "title": "REST",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/sundeep-paruvu/",
        "title": "Sundeep Paruvu",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/shipit-vii-day-winter-2016/",
        "title": "ShipIt VII Day: Winter 2016",
        "tags": ["culture", "engineering", "hackathon", "shipit"],
        "description": "",
        "content": "Cerner’s 7th ShipIt Day took place on December 8th and 9th. ShipIt is a 24-hour hackathon with the goal of building something awesome, usable, and value-adding within a single day. The event was hosted at our DevCenter in the Realization Campus, a large open space that hosts our DevAcademy (to learn more about our DevAcademy, check out this earlier post). We had 18 teams made up of associates from different Cerner organizations. The DevCenter was stocked with all kinds of snacks, which of course included a traditional drink: Monster Energy. Participants worked hard all day long and when dinner came around they were ready to dig in. Good thing we ordered 8 twenty-six inch pizzas and 40 breadsticks from a local Kansas City favorite, Pizza 51. Look how big these pizzas were!\n     \nIt was predicted we hit a record in Kansas City for the most pizza in an elevator at this time, but we were not able to get an official count!\n   When getting pizzas of this size, there was a significant amount to fuel the innovation that was happening. To break it down:\nA = πr2= π x 132 ≈ 530.93 square inches = 3.68 square feet x 8 = 29.44 square feet of pizza. Which is larger than a twin bed size of solid pizza, and slightly smaller than a queen bed. A perfect amount for a ShipIt event! Teams took a break from their work throughout the evening by playing Nintendo and cheering on the Chiefs!\n     \nThere were some impressive projects that came out of ShipIt Day VII. Team GLADoS, made up of Kyle Harper, Sarah Harper, Snehit Gajjar, and Andy Quangvan took first place, and the coveted Golden Keyboard, with their impressive idea.\n This ShipIt, we created a new Alexa skill called Scribe. With our new skill we integrated the Amazon Echo and Echo Dot smart speakers with Cerner\u0026rsquo;s FHIR implementation. This integration allows a patient or care provider to access clinical data by simply speaking to the Echo.\n Judge Sean Griffin was impressed. \u0026ldquo;Really cool, awesome idea. Great innovation! The applicability towards data entry could definitely be useful.\u0026rdquo;\nTeam GLADos with the Golden Keyboard\n   Second place went to Team Trogdor (Kyle Lipke, Derek Dobler, Nikki Justice, Mike Harrison, and Nimesh Subramanain).\n Troubleshooting errors or misconfigurations in Millennium OAuth can be time consuming. Missing information or misconfiguration can lead to hours of checking various sources of information. This web page tool allows us to perform a quick and accurate diagnostic check for what pieces are available and not available. Built with Ruby on Sinatra framework. Made to allow for easy additions to various checks that ETS (Emerging Technology Services group) needs to do.\n Judge Jim Dwyer noted, \u0026ldquo;This is a great tool to help drive down operational costs and reduce TCO.\u0026rdquo;\nTeam Trogdor\n   Third place winners, Team 402 Cheeseballs Required (Andy Nelson, Venkatesh Sridharan, Ian Kottman, Nate Schile, and Anthony Ross) created an application for tracking running mileage for the Healthe Fitness Center.\n Our goal was to make the process of submitting running milestones for the Cerner Running Club easier. The current process involved writing down how far you ran in a binder, and then one of the gym staff checking that binder occasionally to see when you accomplished a milestone so they could send a prize. We decided to make a web application that would integrate with Strava that would simplify tracking how far you\u0026rsquo;ve run. Strava is a platform that aggregates data from multiple fitness apps, such as Fitbit and Garmin. We used the Strava API to pull a person\u0026rsquo;s running data so we could total up their mileage, regardless of what fitness tracker they used. In order to integrate with Strava\u0026rsquo;s public API we had to have an externally facing application. We decided to use an Amazon EC2 instance to do this, since it was easy and cheap. We used Rails to create our web app since that was the web framework we had the most experience in. First, we would ask a person to connect to their Strava account, and then we pulled their running data and totaled up their mileage. If a person had reached a milestone they could press a button to send an email to the gym staff that included their running log, their total mileage, and what milestones they had achieved.\n Team 402 Cheeseballs Required\n   By Friday at 10 a.m. the participants were ready to present (for the most part). Cerner associates from different campuses watched the event via livestream and voted for People’s choice. Those results are as followed\u0026hellip;\nFavorite team name: I Shipped My Pants - Brandon Inman and Steven Goldberg\n Improvements made to the existing Vizceral-based implementation of HealtheIntent Intuition Engineering. Improvements made by the team included features to make it easier to switch between streaming and batch processing, more timely updates of data, and several cosmetic features to improve user experience and differentiate the tool from other Vizceral apps. Voice activation was added as a \u0026ldquo;fun\u0026rdquo; feature.\n Favorite Presentation: Guys on FHIR - Bhaumik Aniruddha, Bhagat Parthiv, Vetri Selvi Vairamutha, Neil Pfeiffer, Sai Praveen Gampa\n Our ShipIt project was a NICU SMART App, intended for the NICU unit. The app will be used to get a live video of the baby inside an incubator alongside the vitals of the baby. This will prevent anyone from disturbing the sleeping pattern of the newborn. The live feedback can also be viewed by the parents which will ease the mental tension. The future scope of this project is to hook up with an app, which can read the facial expression of the baby.\n Best Project: RSA (Readmission Security Admission) - Kristopher Williams, Karthik Nimmagadda, Sai Inampudi\n Our project was to create an intuitive dashboard that lets the user visualize the network traffic for readmission solution\u0026rsquo;s services for a given time period (hour/day/week/month currently). This dashboard will allow the user to easily and quickly identify abnormal behavior (e.g. service goes down, or service experiencing a lot of errors) and is designed in such a way as to return results extremely fast for longer time spans, compared to ad-hoc splunk queries.\n People’s Choice Winners\n   ShipIt Day continues to be a great way for Cerner associates to work on projects they don’t normally have time for, meet people in different organizations and learn something new. Participant Kristopher Williams said:\n My favorite part of ShipIt Day was being able to work on something entirely different, and in my case, with a different group of people. Just a fresh change of pace.\n Thank you to all our participants and a special thanks to our judges Jenni Syed (@JenniSyed), Yegor Hanov, Sean Griffin (@trenchguinea), and Jim Dwyer (@episodicdata).\nCheck out some additional highlights of our event in this video:\n  If you are interested in reading more about previous ShipIt Day events, see these earlier posts:\n Fall 2016 ShipIt Day Spring 2016 ShipIt Day ShipIt - 24-hour hackathon for Millennium+ Platform Dev  "
    },
    {
        "uri": "https://engineering.cerner.com/blog/alan-and-grace-an-origin-story/",
        "title": "Alan and Grace: An Origin Story",
        "tags": ["engineering"],
        "description": "",
        "content": "   Meet Alan and Grace. These lighthearted, 8-bit characters were nominated in early 2014 to become the iconic mascots of Engineering at Cerner. Have you ever wondered who these 80’s video gaming inspired characters on our Engineering t-shirts are? Why were they chosen as mascots? What do they represent? The countless hours spent pondering these questions will be no more. Pull up a chair as we dive into the mystery behind the dynamic duo known as Alan and Grace.\n   Believe it or not, the enjoyable pair highlighted on a myriad of our engineering materials were named after two real life innovators: Alan Turing and Grace Hopper. Alan and Grace were visionaries in their time, championing advances that shaped the computer science innovations of today. These icons were chosen based on their contributions to computer science and the moxie they showed while doing it.\n   Alan, our spiky haired friend who embodies an 80’s meets hipster vibe, is named after the English mathematician Alan Mathison Turing (1912-1954). Turing is known to some as the father of theoretical computer science and artificial intelligence. Alan had an unquenchable mind and was in constant pursuit of knowledge, devoting his life to a vocation in science and mathematics as a computer scientist, mathematician, logician, cryptanalyst, and theoretical biologist. Among his many contributions is the notable Alan Turing Enigma machine. The machine was used to help break the enigma code used in German naval communications during World War II. The concept of the Turing Enigma machine has become the foundation of the modern theory of computation and computability. Alan and his Enigma machine have received some recent fame as featured in the Imitation Game, a 2014 American historical drama-thriller loosely based on the biography Alan Turing: The Enigma. It can be argued that computer science would not be what it is today without the mathematical imagination of Alan Turing. Cue the mic drop now.\n alanturing.net\n    nbcnews.com\n  \n    Cerner Engineering’s jovial female mascot appears as if she’s ready to take on the world. The fearless spirit of the mascot encompases the nature of its muse for whom she was named, Grace Brewster Murray Hopper (1906-1992). Grace Hopper was an American mathematician, computer programmer, military leader, and a woman clearly ahead of her time. Grace received a Ph.D. in mathematics, one of the first women to earn this level of education. Grace’s path to computer science began while serving in the U.S. Naval Reserve as one of the first to program the Harvard Mark I computer. Grace went on to create the first compiler for computer languages leading to COBOL, a widely adapted language that would be used around the world. But Grace wasn’t finished yet. At the age of 60, she was recalled to active duty where she standardized communication between different computer languages. When Grace retired at the age of 79 as a rear admiral, she was the oldest serving officer in the service. In 1991, Grace was the first women awarded the National Medal of Technology. Throughout her life, Grace encouraged young people to learn how to program. Grace’s legacy lives on through the Grace Murray Hopper Award, the Grace Hopper Celebration of Women in Computing Conference, the University of Missouri’s computer museum “Grace’s Place”, and now Cerner’s Engineering mascot. Grace continues to serve as an inspiration to women in tech everywhere. Was there any other choice?\n womenatworkmuseum.org\n    wikipedia.org\n  \nAlan and Grace represent a beacon of inspiration, providing icons that encapsulate and promote a culture of innovation, forward thinking, and just plain fun. The creation of mascots Alan and Grace has been a big win for the passionate Cerner engineers dedicated to fostering Cerner’s Engineering culture.\n   "
    },
    {
        "uri": "https://engineering.cerner.com/authors/lauren-ammons/",
        "title": "Lauren Ammons",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/clara-rules-joins-cerner-open-source/",
        "title": "Clara Rules joins Cerner&#39;s open source",
        "tags": ["engineering"],
        "description": "",
        "content": "Sometimes a small experiment on the side can grow into something valuable. We at Cerner have long used forward-chaining rules engines to express and execute clinical knowledge, but we’ve also had to extend or work around the capabilities of such engines. Engines targeting business users just weren’t expressive enough to model some of our logic. To meet this need we are making Clara Rules an open source project driven by Cerner Engineering.\nClara took an unusual path, starting as a minimal implementation of the Rete algorithm to help my own understanding. After sharing this with others at Cerner, we started to see an opportunity to gain the modularity and reasoning advantages of a rules engine, while preserving the expressiveness and power of a modern programming language. We wanted to combine the best ideas of rules engines with good software engineering practices. You can see the result of this on the Clara Rules home page.\nMike Rodriguez has been essential in turning Clara from an experiment into a valuable system to fill a gap for rules engines which are oriented towards developers. More recently, Will Parker has also made significant contributions to help make Clara fast and reliable. They made it possible for Clara to help analyze millions of medical records every day. Therefore Mike and Will are joining as the initial committers to Cerner Engineering’s Clara project. We will welcome other contributors to make this step in the future.\nYou can find project source code on GitHub, and the documentation on the clara-rules.org site.\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/fall-2016-shipit-day/",
        "title": "Fall 2016 ShipIt Day",
        "tags": ["culture", "engineering", "hackathon", "shipit"],
        "description": "",
        "content": "Another ShipIt Day was held on September 8th-9th at the Realization Campus at Cerner. ShipIt is a 24-hour hackathon with the goal of building something awesome, usable, and value-adding within a single day. This fall marked our sixth ShipIt, which has has grown significantly since our first event.\n   The event kicked off in the morning at the auditorium of our Realization campus. At this point, teams were already formed, instructions were given, and shirts were handed out. This ShipIt day came with cool new t-shirts for all participants featuring the ShipIt squirrel (inspired by GitHub)!\n   Teams broke out to their work spaces and got to work on their projects. One team, LearnStack, decided to focus their project on associate development. The judges greatly appreciated their approach to this very real problem, and decided to award them the third place prize.\nLearnStack - Jake LaCombe, Daniel Stucky, Sujay Sudheendra\n Developers can get to a struggling point when they try to increase their experience level. Sure, getting started with any language can be very easy to do. Rails for example has their impressive tutorial from Michael Hartl for getting started with the framework. However, what happens when you get done with those beginning steps? What does it take to go from a beginner to an intermediate Rails developer, and then again from Intermediate to Advanced?\n  The answer here is LearnStack. LearnStack takes a list of resources that are recommended by other developers, and rated appropriately based on the experience level of the material. Developers can easily share resources related to beginner, intermediate, and advanced knowledge of a particular language, process, framework, or whatever it may be. No need to keep doing endless searches online to figure out how to get to the next level! Let LearnStack point you to the resources you need!”\n Another team decided to tackle the time it takes to provision a virtual machine. This team called themselves, \u0026ldquo;All your base are belong to us,\u0026rdquo; and they were awarded second place.\nAll your base are belong to us - Chris Soukup, Alex Harder, Ben Hemphill\n The current situation for OS image creation is that it is done infrequently, in an ad-hoc manner, with little consistency among the various hypervisors. This means that we take the base image and then use Chef to upgrade the image to the desired state. This adds significant amounts of time to the process of provisioning a virtual machine.\n  Our project was to use Packer to create a generic image with all of the universal settings that we set with our base OS Chef recipe. This image can then be distributed to all the hypervisors and made available to consumers. Since the image would be generated regularly, the provisioning time could be significantly decreased by not having to immediately upgrade the system. Additionally, since the universal settings are already applied, we save considerable amounts of time on the OS preparation as well. We took provisioning time for a RedHat 7.2 OpenStack VM from 12 minutes, 50 seconds to 4 minutes, 1 second, a 320% decrease in time!\n  Future enhancements will allow us to automatically generate these images, distribute them, and make them available to consumers. This will allow a consuming team to naturally stay current on OS patches at provisioning time, decrease the human effort in creating these OS images, and provide a consistent experience for teams across Vagrant, VMWare, OpenStack, EC2, Docker, etc.”\n For the first time we had a unanimous winner for both the judges and the participant winners. The 206 Partial Content team not only won two giant tubs of cheese balls, but also the all important Golden Keyboard and bragging rights until the next ShipIt Day. 206 Partial Content decided to solve a problem to \u0026ldquo;enable solution designers, engineers, and grandparents to easily create, stage, and process data for testing scenarios required for verification without being dependent on upstream processing or rules.\u0026rdquo;\n206 Partial Content - Madhur Sikri, Ian Kottman, Venkatesh Sridharan, Andy Nelson, Brian van de Boogard\n Our project was primarily driven by two rock star engineers on our team, Ian Kottman and Brian van de Boogaard. We coined our project Sinecure, which means a position requiring little or no work but giving the holder status or financial benefit. The goal of the project was to find a solution to creating test data more easily and efficiently for anyone on our team to consume.”\n Thanks to all who participated and congratulations to our winners. Until next time, at ShipIt VII in December!\n   "
    },
    {
        "uri": "https://engineering.cerner.com/authors/melanie-carpenter/",
        "title": "Melanie Carpenter",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/building-a-unified-ui-component-library/",
        "title": "Building a Unified UI Component Library: Lessons Learned",
        "tags": ["UI", "UX", "library", "development", "programming", "JavaScript", "CSS"],
        "description": "",
        "content": "Cerner is building an open source, mobile-first user interface component library for healthcare applications. Pulling years of experience and lessons learned across multiple teams together, we are creating something great. We are very excited to bring this solution to the public and to be contributing back to the community!\nWe didn’t simply decide to create this library, we started with segmented UI libraries focused on different aspects of the company which had smaller isolated needs. Mistakes were made along the way and we have learned much from them. Let’s take a look at where we started and how we got to where we are now.\nHistory In 2013, a group of engineers with strong front-end skills was put together with the task of creating a reusable component library for Cerner’s HealtheIntent platform. Twitter Bootstrap was used initially, but quickly outgrown as the needs and designs of UX expanded. The project this team built is called Blue Steel. It featured everything from simple typography to interactive data visualizations while adhering to UX guidelines and requirements.\nBlue Steel is a Rails gem that provides some basic helpers, CSS, JavaScript, and site templates to simplify layout and correct HTML usage. Blue Steel abstracts complex components to keep things simple and because we didn’t feel it was necessary to abstract native HTML elements.\nIn its infancy, the project was well received; other platforms saw the value and wanted to take advantage of the work being done. To accommodate the additional platforms, Blue Steel was merged with another internal UI library called Style Guide. The best features of each library were pulled together, sometimes one overriding the other, sometimes merging feature sets. The merged project was called Terra.\nProblems The approach of merging existing components saved time up-front, but came with a steep cost. The library was fragmented in the approaches taken during design and development. As such, it became difficult to work on without pre-existing familiarity. It was fairly obvious that we put two frameworks together on a time constraint.\nBlue Steel still exists but is comprised primarily of Terra components. It persists to provide HealtheIntent-specific styles and functionality, while also being a Rails wrapper around Terra. Terra has been historically kept below version 1.0.0 for rapid development, while Blue Steel has been above 1.0.0. This strategy has caused a lot of pain for the consumers of both Terra and Blue Steel.\nKeeping Terra below version 1.0.0 allowed breaking changes to occur without cutting a major release. Although this made development easier in some aspects, it damaged consumer trust. We always attempted to communicate breaking changes, but it didn’t always happen and it wasn’t always clear. Any time a team had to upgrade, they had to be prepared to fix their application which caused them not to trust us.\nThe issue was even worse in Blue Steel since it had to accommodate for Terra’s breaking changes. Blue Steel would consume a version of Terra with breaking changes and would still release as a minor update by providing styles and hooks to work around the breaking changes. Deprecation schedules and documentation were written to help keep Blue Steel backwards compatible. Unfortunately, it often wasn’t; sometimes, breaking changes would only manifest in an application.\nBlue Steel and Terra were both tested separately with their own documentation sites. These tests were extensive and thorough but could not accommodate the complexity of the various applications consuming them. Breaking changes would creep into applications even when we thought everything was backwards compatible in Blue Steel.\nIn Terra, the preference was to style on HTML elements, states, and ARIA-roles whenever possible since they carried far more meaning than CSS classes. Unfortunately, this form of styling is somewhat global in nature. It was easy for style collisions to occur between components within Terra, consumer styles, and 3rd party libraries.\nTerra was developed in such a way that it discouraged consumers from building custom components. The thinking was that applications would write little to no custom CSS or JavaScript. This didn’t scale well as needs evolved and application developers had to move ahead of the UI library. Terra components had a very high CSS specificity and were difficult to override which forced consumers to write even more complex CSS to override it. In turn, many bugs were introduced to consumer applications.\nTerra was also built as a monolith with very little modularity in place. Consumers had the option of taking it all, figuring out how to make a custom build, or not using it. This caused applications which only needed a subset of functionality to become bloated.\nSolutions Today, Terra is in the process of being open sourced. We’ve looked extensively at the issues above and are taking measures to address all of them:\n Each component is in its own repository. The SUIT CSS convention is being used for all Terra components. Base componentry is being developed first. Components are being kept minimal and composable. Each component is being built to be a minimum viable product and released as 1.0.0 to follow SemVer up front. Only as components become complex and specific do we introduce framework opinionation. Base components will be framework agnostic. Everything will have helpers to abstract underlying HTML structures.  SUIT CSS bans the use of styling hooks which are considered global and effectively breaks the cascade in a global sense (you can leverage it within a component). At the root node of each component, only classes can be used. As a result, consumers can rest easy knowing that styles will not leak out of components and they have low specificity which allows them to be overridden as needed.\nBy starting with base components (buttons, images, etc), we effectively create a set of building blocks to build more complex web components. By keeping them small, composable, and framework agnostic, they can be used anywhere with confidence. Following SemVer from the start will boost consumer confidence when using our components.\nIntroducing opinion into more complex components enables us to build better and more maintainable components with our most common use cases in mind. This does not preclude consumers who do not wish to consume the frameworks and libraries we use. By keeping each component in its own repository, it’s possible to create alternative versions of componentry to meet application needs. Additionally, keeping each component in its own repo allows for individual versioning, limiting scope of change, and makes it very easy for consumers to omit what they don’t need.\nFinally, providing helpers for all components will enable easy and consistent development. It will be possible to build complex components solely out of the smaller components leveraging helpers to output the correct HTML. The abstraction also makes it possible to update the implementation in a compatible way while making it so developers don’t have to be aware of complexities like semantic HTML and accessibility. This will enable developers to build solid applications easily and quickly.\nLessons Learned  When creating a reusable library, avoid global state or styles. Create an abstraction or facade to the underlying implementation regardless of simplicity. Keep your library modular. Start with SemVer and stick to it. Start simple then reuse and compose. Keep your library cohesive by developing consistently across components. Don’t build with any particular framework(s) in mind when you can; the web is constantly evolving.  We’ve learned a lot and are excited to open source the results of our work and learning. Keep an eye on github.com/cerner and engineering.cerner.com while we build out this new library. It’s going to be great!\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/css/",
        "title": "CSS",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/development/",
        "title": "development",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/javascript/",
        "title": "JavaScript",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/library/",
        "title": "library",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/programming/",
        "title": "programming",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/rory-hardy/",
        "title": "Rory Hardy",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/ui/",
        "title": "UI",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/cerner-engineering/",
        "title": "Cerner Engineering",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/michelle-brush-receives-the-rising-trendsetter-stemmy-award/",
        "title": "Michelle Brush receives the Rising Trendsetter STEMMY award!",
        "tags": [],
        "description": "",
        "content": "We are excited to congratulate our own Michelle Brush for receiving the Rising Trendsetter STEMMY award. The Rising Trendsetter Award is given to a woman in STEMM areas with less than 20 years of experience and has demonstrated significant achievements early in their career.\nMichelle is a director at Cerner who manages an engineering team in population health as well as a team dedicated to improving Cerner’s development culture. One of Michelle’s most notable career accomplishments was her leadership role in reconstructing Cerner’s software engineer development program in 2013, which she personally oversaw until the end of 2014 (read about the details of this program in her blog). In 2014, she transitioned into an executive role in the Population Health platform, where she fields particularly tricky support issues to help resolve data inconsistencies. Since 2014, Michelle has also managed the Development Culture team at Cerner, a team that focuses on improving associate morale through cultural empowerment, encouraging and providing opportunities for collaboration and innovation, and growing Cerner’s presence in the technical community.\nMichelle actively promotes women entering and growing their STEMM careers through her leadership in the community and at Cerner. She is an integral member of Cerner’s women in tech committee, which has the goal to provide development, networking and leadership opportunities to women in technical roles in a supportive and inclusive environment that promotes authenticity and values differences. Additionally, she is a member of several other leadership councils ranging from helping design the new software engineer campus, improving development culture across the company, and intern planning.\nIn the community, Michelle is also a local chapter leader of Girl Develop It, a nonprofit organization that provides affordable programs for adult women interested in learning web and software development. Michelle is also an organizer of Midwest.io, a two-day industry tech conference in Kansas City that brings together developers across the Midwest for an eclectic collection of talks covering the latest trends, best practices, and research in the field of computing. As a member of the organizing team, she defines the vision for the conference, which includes finding speakers, promoting the conference in the industry, and ensuring diversity is top of mind at the conference.\nMichelle also actively speaks at tech conferences, and has spoken at conferences like OSCON, CityCode, O’Reilly’s Software Architecture Conference, and Strange Loop. Michelle is also a MSU Computer Science Board Member.\nCongratulations, Michelle!\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/devcon-word-cloud/",
        "title": "DevCon 2016 Word Cloud",
        "tags": ["devcon", "tech talks"],
        "description": "",
        "content": "Every year we hold an internal developers conference called DevCon. This year we had 295 submissions for talks, ranging from a deep technical dive into the inner workings of Kafka to a reflection on the power of office pranks.‌ I wondered what, if anything, do these submissions have in common? Are there any common themes/topics being discussed? To find out I decided to visualize the talk submissions in a word cloud to create an easily understandable (and hopefully aesthetically pleasing) view of what topics were most common.\nThe first step was some basic data cleaning so I could get a good set of words to visualize. I used Python to read in the submissions into a single string, lowercase all words, and then remove all common contractions.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # read all submissions into a single string text = open(\u0026#39;./submissions.txt\u0026#39;).read() # lower case text = text.lower() # remove all contractions to prevent situations like \u0026#34;we\u0026#39;ll\u0026#34; becoming \u0026#34;well\u0026#34; once punctuation is removed # normally a regex tokenizer could be used for this but that assumes single quote is only ever used # in contractions with open(\u0026#34;./resources/contractions.txt\u0026#34;, \u0026#34;r\u0026#34;) as file: contractions = file.read().splitlines() for contraction in contractions: text = text.replace(contraction, \u0026#34;\u0026#34;) Next I replaced all punctuation with spaces. I did not just remove the punctuation so special cases such as hyphenated-words and the phrase \u0026#34;and/or\u0026#34; are preserved. # replace all punctuation with spaces. for char in set(string.punctuation): text = text.replace(char, \u0026#34; \u0026#34;)   At this point I had 32,987 total words. A word cloud of this dataset was too noisy, predominated by common words like \u0026ldquo;the\u0026rdquo; and \u0026ldquo;talk\u0026rdquo;. In natural language processing these unwanted common words are called \u0026ldquo;stop words\u0026rdquo;. To remove them I made a list out of the submissions and then only kept words that were not in a list of common English words, supplemented by words specific to this data set, such as \u0026ldquo;Cerner\u0026rdquo; and \u0026ldquo;presentation\u0026rdquo;.\n1 2 3 4 5 6 7  # get list of all words submission_words = re.sub(\u0026#34;[^\\w]\u0026#34;, \u0026#34; \u0026#34;, text).split() # only keep words that are not in the stop words set stopwords = open(\u0026#39;./resources/stopwords.txt\u0026#39;).read() stopwords_set = set(stopwords.split(\u0026#34;\\n\u0026#34;)) filtered_word_list = [w for w in submission_words if w not in stopwords_set]   This left me with a list of 14,291 words and 4,264 distinct words. When visualized I noticed many similar words were taking the highest spots, such as \u0026ldquo;team\u0026rdquo; and \u0026ldquo;teams\u0026rdquo; or \u0026ldquo;technology\u0026rdquo; and \u0026ldquo;technologies\u0026rdquo;. To remove some of this noise I turned to lemmatization. Lemmatization is the process of finding a canonical representation of a word, i.e. its lemma. For example, the lemma for the words \u0026ldquo;runs\u0026rdquo; and \u0026ldquo;running\u0026rdquo; is run. I used the popular Python library Natural Language Toolkit for lemmatization.\n1 2 3 4 5  lemmatizer = WordNetLemmatizer() reduced_list = [] # find the lemma of each word for word in filtered_word_list: reduced_list.append(lemmatizer.lemmatize(word))   This reduced the dataset to 3,834 distinct words. Now that the dataset was cleaned, common words filtered out, and similar words combined it was time to create the word cloud using a project called word_cloud.\n1 2 3 4 5 6 7 8 9 10  cloud = WordCloud(relative_scaling=.5, height=1024, width=760) cloud.generate(submissions) # write to file cloud.to_file(path.join(path.dirname(__file__), \u0026#34;cloud.png\u0026#34;)) # show word cloud using matplotlib plt.imshow(cloud) plt.axis(\u0026#34;off\u0026#34;) plt.show()      Success! It is obvious what some of the most common themes are, such as \u0026ldquo;data\u0026rdquo; and \u0026ldquo;team\u0026rdquo;. Possible future steps for cleaning up the data would be grouping noun phrases, such as \u0026ldquo;software engineer\u0026rdquo; into single words or possibly removing common spelling mistakes using a spell checking library like PyEnchant.\nYou can checkout the full code on Github.\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/tech-talks/",
        "title": "tech talks",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/debugging/",
        "title": "debugging",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/difficult-debugging/",
        "title": "Difficult Debugging: Learning From Utter Nonsense",
        "tags": ["debugging", "development", "programming", "javascript", "css", "IE8"],
        "description": "",
        "content": "Preface As software engineers we invest time into thinking about the problems we are trying to solve every day. Sometimes the problems we face are small or simple. Other times they are large and complex. Either way, we can usually draw from our experience to find a path forward.\nWhile coding, we run into a lot of different bugs. Often, they are simple typos or a misuse of an API. These problems do not bog us down very much, and they are quick and easy to fix. Even complex bugs, while requiring investigation, often follow a familiar pattern(s) that help us identify a path to a solution.\nGiven this experience, it’s easy for us to think that we are good at debugging.\nWe are not. We are good at pattern recognition and recognizing similarities to problems we’ve faced in the past. Because we recognize the pattern, we can quickly debug most of the bugs we come across in our day-to-day work.\nWhich is great. But today, we will be exploring the types of bugs that do not come up in our day-to-day work.\n   How Bugs Can Mislead Investigative Efforts When a bug is brand new to us, we haven’t had the opportunity to derive a pattern from it yet because we have no experience to draw upon to create that pattern(s). Thus, we must investigate using a limited set of tools:\n Error messages Stack traces Guessing and check debugging Colleagues Google  Often enough, that set of tools will guide you to a solution within a reasonable amount of time. Be warned, sometimes the solutions online solve a similar problem, but not your root problem. Testing is always in order with new bugs (or any bug more complex than a typo).\nDebugging gets even trickier and more time consuming when we encounter a bug that resembles a pattern that we’ve seen before. Once, I came across a bug which looked like two other things, but turned out to be a third unexpected thing.\nTreemaps and IE8, and Fonts, Oh My One of our teams created an interactive DOM based treemap which worked in Internet Explorer 8 (IE8) and above. It wasn’t lightning fast in IE8, but it worked sufficiently well with a reasonably complex dataset. That is until the browser started crashing intermittently for some of our users.\n   There were a few things that we (four engineers) noticed right away:\n It only crashed in IE8 The presence of a treemap seemed to be a driving factor Crashing was intermittent  Our initial investigation was good. We isolated the problem browser and determined that there was something wrong with the treemap, which was causing the issues, we were on our way to solving this problem - or so we thought.\nGiven the intermittent nature of the problem, our experiences in the past, and the patterns we had observed, we assumed we had a memory leak, a race condition, or both. In an attempt to rule one of those options out, we tested with small and large datasets for the treemap.\n   The above image serves as proof that modern browsers were unaffected by the bug we were experiencing.\nConvinced that the treemap was the issue and that it had to be a memory leak or a race condition, we created automated tooling using sIEve, virtual machines, and AutoIt scripts. The automation enabled us to test dozens of scenarios hundreds of times easily to gain metrics which might help illuminate where, in the code, we should look next.\nWe chose different sized datasets thinking that smaller datasets would not trigger a memory leak while a large one would. Unfortunately, it crashed either way and we were no closer to figuring out what was wrong.\nAfter several more days of investigation we were just as perplexed as the day we started. We had a plethora of information which told us nothing. At this point our capacity had to be reduced down to just one developer - me. Frustrated with the resultless weeks spent investigating the issue, I decided step back and take a completely different approach.\n   To re-evaluate everything we did, I removed all of the assets from our application to minimize moving parts; regardless of how unlikely a variable was, I wanted to remove it. I ran our test scripts against the application to find that, unsurprisingly, the browser did not crash. I then added JS and CSS assets separately to find that neither alone caused the browser to crash. As a sanity check, I added both back to the application and it would reliably crash. This implied that there was some interplay between CSS and JS which was causing the browser to crash.\nIn our application, it was easier to add CSS file by file than it was for JS so I went that route. Along the way, I noticed that we were sending the wrong X-UA meta tag so I got sidetracked and fixed that to no avail. Eventually, I added back our Font Awesome CSS which caused the browser to crash. Thinking there was no way that a font could be the issue, I added and removed various pieces of our CSS to determine if it was the cause of the problem. I tried changing selectors, changing the load order, and everything else I could think of to no avail. After a while, frustrated, I commented out every line referencing Font Awesome and the browser stopped crashing. At this point, I just started adding code back line by line until the browser started crashing. What I found made no sense:\n\u0026amp;:before { font-family: 'FontAwesome'; } I looked over this for a while and eventually noticed that we were using single quotes when we referenced the font while the font declaration\n@font-face { font-family: \u0026quot;FontAwesome\u0026quot;; src: url(@fontAwesomeEotPath); font-weight: normal; font-style: normal; } was using double quotes.\nHaving exhausted most other options already, I tried making the quotes match to see if the browser would crash and, to my surprise, it wouldn’t. It didn’t matter whether I used double or single quotes as long as they matched. I added back all of the remaining assets and tested this again; sure enough, the app did not crash even after 200 tests.\nYes, it’s true. It wasn’t a race condition or a memory leak within the treemap, or anything else we thought it might be. It was mismatched quotes. I would never have guessed this in a million years.\nI did not stop here! Although I solved the problem, I needed to understand what was going on. I was also very curious which part of the JS was causing issues with Font Awesome. I reintroduced the old CSS and started testing the JavaScript. The application broke as soon as I re-introduced Modernizr. The treemap was, at this point, seemingly faultless as I was able to reproduce the crashes without it.\nI researched mismatched quotes and Modernizr online to try and get a better understanding of what was going on. I found several articles that detailed similar issues, but did not identify the root cause. Eventually I found a post on StackOverflow that enabled me to connect the dots.\nBrowsers do a lot for developers under the hood and IE8 treats different types of quotes differently and kicks off an error handling subroutine to smooth it over. If that error handling subroutine occurs while Modernizr is attempting to shim the browser for HTML5 compatibility, the browser will crash.\nWe weren’t wrong in that we were facing a race condition, but we were very wrong about our presumption about the treemap. The only interplay the treemap had was that it caused Modernizr to take longer to apply the shim thus widening the window of opportunity for the race condition to apply.\nWe worked on this from December 20th to January 9th, a total of twenty days!\n   How To Tackle Perplexing Bugs Oftentimes, our ability to recognize coding patterns enables us to identify and solve bugs in a timely manner. As the example bug shows, however, following them can lead you down a rabbit hole. At face value, twenty days were spent working on that bug, but when you consider that there were up to four developers involved at a time, the lost time actually equates to one to three months. Once you factor in opportunity cost on top of that, you look at two to six months of time lost to debugging this issue.\nThat is a disturbingly significant amount of time to have lost on a bug. It was a difficult bug to solve, but it could have been solved faster had we recognized that our past experience was insufficient to solve this problem. There are some key points to take away from debugging something perplexing like the example bug we looked at:\n Isolate the problem. Find a single way that the bug can be consistently replicated and remove any variance that can trip you up. Automate as much as you can. There are likely to be many tests; let the computer handle that work for you. While working on the example bug, we ran thousands of tests over those twenty days. Remove as many variables as possible and keep a list of what’s remaining. Change one variable at a time. If you change many things simultaneously, you won’t know what yielded which results. Test, document, and repeat. When you change a single variable, record all of your findings. How did the results change? Eventually new patterns will present themselves. Narrow the scope. As your testing reveals patterns, narrow down to a smaller area. In the example bug, we went from “somewhere in our static assets” to “something in our CSS and JS” to “something in this particular file” to “it’s this particular line”. Repeat and keep repeating. Be meticulous in recording your findings. Even if you don’t see a pattern, someone else studying your results might.  When you find that past experience is not yielding results in a timely manner (you’ll have to define what that means for yourself), take a step back and follow the steps above. Doing this allows your past experience to help you when it applies and gets it out of the way when it doesn’t. Don’t see how far the rabbit hole goes when you can scientifically figure it out.\nAdditionally, do not blindly trust answers found online. When working on the example bug, we found several examples of people correctly identifying a similar problems to ours, none of which correctly identified the underlying cause. Treat online resources as what they are - resources. Sometimes they have the answer you seek while other times they are the dots you must connect yourself.\nTools and Techniques Which Will Help With Day-to-Day Developing and Debugging Linters Static analysis, or linting, is a technique to catch common coding mistakes quickly. A linter will evaluate raw source code and give back a report detailing what needs to be fixed. Most languages have some form of linting software available for them. Since this post is front-end oriented, below are a few linters available for JS:\n ESLint JSHint JSCS  All of these tools will notify you when you make typos, aren’t using variables, fail to follow a predefined set of coding styles, and more. They are very fast and are your first line of defense against day-to-day bugs. Linters will catch mistakes much faster than a human.\nExample I have a friend who is a very talented software engineer and mathematician. One day he called me to see if I could help him debug an issue he had been struggling with for about 19 hours. His Angular application kept spin locking (freezing up) and he couldn’t figure out why. We scrolled through the code and luckily for him, a piece of it caught my eye:\nfor (var i = 0; i \u0026lt; 10; i++) { for (var i = 0; i \u0026lt; 5; i++) { // logic  } } He had been working on C++ code prior to this project which features block scoping while JavaScript (ES5) only features function scoping. A linter would have immediately caught this mistake and informed him of it, saving 19 hours of debugging.\n   Automated Testing Frameworks While static analysis is your first line of defense, tests for your code are your second line of defense. Tests should depict the behavior of your codebase and should fail if you deviate from that contract. Just like linters, most languages support automated testing. There are several frameworks that you can leverage for testing JS, a few are:\n Mocha Jasmine QUnit  Defining a set of behaviors for your code helps to protect you from unintentional changes and side effects. Tests will take longer to run than a linter, but are still frequently quicker at catching mistakes than a human. Since automated unit tests are like a contract for behavior, they can also help keep developers unfamiliar with the behavioral contract from making mistakes.\nDebugging Tools When automated approaches are insufficient, it’s time to start the manual debugging process. Every programming language has a debugging toolset that can be used to work through code with bugs. Since this post is front-end oriented, we’ll look at browser tools.\nYou could use JS to alert variables at different times, but this is radically inefficient. Today’s modern browsers have a plethora of tools available to you starting with the debugger keyword for JS code. When the browser’s console is open, the debugger statement acts as a breakpoint; it does nothing otherwise. Although simple, it’s very powerful as it gives easy control of adding/removing breakpoints to step through the code.\nAtop the debugger keyword lay all of the browser tools. These enable you to step through code, inspect code, profile code runtimes, inject assertions, and even monitor the network traffic relating to the current page. Different browsers have different tools; below are some guides for debugging in different browsers:\n Chrome Safari Firefox Edge  Automated Robustness Testing When you are in the process of manually debugging code, it is useful to automate what you can. For example, when testing the IE8 crashes, we used virtual machines and an AutoIt script to recreate and test the scenario as well as to keep logs of what happened for future reference.\nWhenever possible, always automate recreating the scenario(s) for testing purposes as it:\n Speeds up the process Removes human error Ensures consistently gathered metrics  Conclusion Many bugs will follow the pattern(s) of another bug and this can help you debug them quickly. However, this is not always the case; tools and automated tests will help you speed up your debugging time, but cannot keep you from chasing ghosts.\nRemember, the following steps are the key to solving difficult bugs without losing a lot of time and effort:\n Isolate the problem Automate Remove as many variables as possible Change one variable at a time Test, document, and repeat Narrow the scope Repeat  When faced with a challenging bug watch how much time and effort you give to it. When it’s time to step back and re-evaluate your approach, do so and success will follow.\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/ie8/",
        "title": "IE8",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/railsconf-2016-recap/",
        "title": "RailsConf 2016 Recap",
        "tags": ["culture", "engineering"],
        "description": "",
        "content": "At Cerner, we love Ruby and Rails and use it prominently in our cloud based solutions. When we found out RailsConf 2016 was going to be held in the hometown of Cerner\u0026rsquo;s headquarters in Kansas City, Missouri, we were excited for the opportunity to support the conference by attending as well as being a sponsor and hosting one of the official after parties.\n   RailsConf 2016 took place in the heart of downtown Kansas City. From day one, the atmosphere was buzzing. We usually send a few engineers to RailsConf every year, but since it was in Kansas City, we were lucky enough to send 50+ engineers to soak up all of the knowledge and news that was being shared across three session-filled days.\n   We were a silver sponsor this year, which provided us with a booth space where we could share Cerner\u0026rsquo;s story with Rails developers from across the world.\n   In addition to meeting all of the wonderful Rails developers and getting the chance to tell them about Cerner and how we are huge into Rails, we also had some fun at our booth with some multiplayer gaming action\u0026hellip;\n   \u0026hellip;and our daily raffles. We gave away some some awesome BBQ from Joe\u0026rsquo;s KC and a Phantom 3 Professional drone. Congratulations to both winners.\n      And for even more fun, we hosted one of the official after parties after the closing Day 1 keynote at No Other Pub. During the party, RailsConf attendees got a chance to mingle and talk to our developers and architects in a casual setting…and the free food and drinks certainly made for a great time.\n      In addition to all of the fun, one of our distinguished engineers, Nathan Beyer, gave a talk about lessons we\u0026rsquo;ve learned using Rails at Cerner.\n      RailsConf 2016 was a great event, and the fact that it was in our backyard, was icing on the cake. We wish it would be held in Kansas City every year, but we are already looking forward to RailsConf 2017 in sunny Phoenix, AZ.\n   "
    },
    {
        "uri": "https://engineering.cerner.com/authors/sam-bao/",
        "title": "Sam Bao",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/spring-2016-shipit-day/",
        "title": "Spring 2016 ShipIt Day",
        "tags": ["culture", "engineering", "hackathon", "shipit"],
        "description": "",
        "content": "ShipIt Day is an all day, all night hackathon with the goal of building something awesome, usable, and value-adding within the given 24 hours. This Spring marked our fifth ShipIt Day, with participation and reach growing with each event.\nWe kicked off the event at 10 AM on Thursday, when teams broke out and got to work and stopped only for food and caffeine. Many stayed until a few hours after dinner arrived, several stayed all night and took naps in their workspace, and a few worked through the whole night. The event was initially announced early in February, to give teams time to work this into their project plans. The schedule was to start at 10 am on a Thursday and wrap-up at 10 AM on Friday. Teams then presented their awesome projects and then were free to leave for the weekend (and catch-up on some sleep). Each team was free to choose the project they wanted to work on, with the limitation added that they should work on something which can be deployed somewhere in 24 hours (there were bonus points involved for deployed projects). The winning prize not only included bragging rights, but also the Golden Keyboard, which will be a traveling trophy.\n   The event wrapped up on April 1st at Continuous Campus. We had over 50 participants from nine different organizations split into teams. They survived several April Fools pranks, two cases of Monster, and 18 extra large pizzas before presenting their projects to a room full of other participants, judges, and over 50 people watching the live broadcast online.\nBy the time 10 AM on Friday rolled around, the group was looking sleepy but excited for presentations. We moved over to a bigger room where a live streaming service was set up for associates at other campuses to watch presentations. A voting form was sent out to the entire audience, and a scoresheet was given to the judges. All 12 teams got through their presentations despite their lack of sleep, and the judges were thoroughly impressed. One judge, K.K. Kailasam made the remark,\n You guys made it very difficult for us. It was extremely exciting to see the passion and excitement in each of you guys coming up and doing something outside of normal work. I’m totally humbled and very impressed.\n The people’s choice votes were tallied and judges decided on the top three winners.\n   Winning Projects People’s Choice Team Mantri (Dinesh Rauniyar, Raju Karki, Dinesh Bajracharya, Mahesh Acharya, Prannav Shrestha)\nSpoor is a Chrome extension that scans through tools like GitHub, JIRA, Crucible and Jenkins and presents relevant information to us. Just with a single click, you can view all of the notifications from these tools and drill down to the detailed and hyperlinked description of each one of them.\nThird Place Suicide Cupcakes (Travis Collins, Daniel Lloyd, Andy Quangvan, Gared Seats)\n The ETS_SHIV Ship IT project came about due to the frustration of being locked into a chroot jail on an ssh jump box without any ability to lookup the necessary connection info (fully qualified domain name / IP Address) necessary to connect to a node. Shiv is a tool that will query a read-only file system with all the node objects represented as directories and text files. The script writes out the files and directories to a file system that is bind mounted read- only inside chroot. Now users inside the chroot jail can run simple a command like; shiv cloud. This would return the FQDN/IP address of all nodes with a Chef node name containing the the string “cloud”.\n Second Place Really Tardy Measurement mechanicS (Jeff Compton, Himanshi Gulati, Adam Splitter, Timothy Waszac, Sriram Vimaraju)\n The Response Time Measurement System (RTMS) framework, which our team owns, is dependent on many moving parts, such as: Olympus, Hadoop and Jenkins in order to produce a timer. In addition, using Vertica, Tableau, LightsOn Network, and the Olympus portal for reporting timer data. This framework has been consolidated into a single application which can be used for development or troubleshooting of timers. Currently the application supports Splunk (for Millennium+), Millennium application-produced checkpoint CSV files, or produce timers real-time as a Millennium application sends checkpoint data to it. After timers have been produced, the user can inspect the composition of the timer by exposing checkpoints, incomplete timers, and chronological representation of a timer and other timers which may have affected it in a Gantt chart.\n This application will improve the accuracy of timer definitions developed, improve speed of development on engineering, and empower engineers to develop and troubleshoot their timers without depending on the entire RTMS framework.\nFirst Place Wolfe Packe (Bryan Baugher, Micah Whitacre, Christian Duranleau, Michael Barker, Alex Hostetler)\n Kafkaesque is a Web UI to help manage and operate Apache Kafka. It provides insightful statistics about topics, partitions and brokers and is able to balance and re-assign partitions to different brokers.\n    ShipIt Day has become an incredibly valuable experience for participants, giving them a chance to not only focus on projects that they wouldn’t normally have time for, but allowing them to meet people from other teams (in different organizations) and learn from those they normally wouldn’t work with. To quote a participant, Daniel Lloyd:\n ShipIt day was a blast. It was fun, it was serious, it was fun again… We got to make some new friends in IP and participate in an event that spanned both teams and organizations. That in itself is enough to recommend ShipIt day to anyone. It provides so much perspective into how the other teams operate and the cool things other teams and organizations are doing.\n Thanks to all that participated, and we’ll see you again in the summer!\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/deploying-web-services-with-apache-tomcat-and-chef/",
        "title": "Deploying Web Services with Apache Tomcat and Chef",
        "tags": ["engineering"],
        "description": "",
        "content": "Open source is an important part of our engineering culture and we love when we’re able to contribute back to the community. We recently open sourced our Tomcat Chef Cookbook, which we use to automate deploying many of our web services here at Cerner. The cookbook is meant to be simple, yet flexible enough to change most anything about Apache Tomcat to your liking. It supports installing any version of Tomcat, configuring any file within the Tomcat installation and deploying web applications.\nThe cerner_tomcat cookbook is one of our oldest cookbooks which was originally written when we first started using Chef. We created it as the other community Tomcat cookbooks didn’t include all the features we needed. As of today, the Tomcat cookbook provided by Chef seems to be the predominant cookbook in the community; however, it still lacks the functionality and flexibility we need such as deploying the applications themselves.\nOne of the biggest problems when using Chef to configure Tomcat is that Tomcat uses XML for configuration which does not map very well to Chef’s Ruby Hash-based attributes like JSON, YAML or properties files do. Our first implementation was very similar to the Chef community cookbook in which the cookbook provides a configuration template with a static set of properties. We quickly learned that this solution didn’t scale for us given the varying configuration needs of applications and the ever-changing configuration options across Tomcat versions. Our approach now is to allow applications to provide configuration templates themselves, allowing this cookbook to be leveraged by the various teams deploying Tomcat applications.\nOur cerner_tomcat cookbook is one of our most used cookbooks here at Cerner. We would love for you to try out our cookbook and let us know how we can improve it.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/amanda-anderson/",
        "title": "Amanda Anderson",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/girls-in-technology-movement-hour-of-code/",
        "title": "Girls in Technology Movement - Hour of Code",
        "tags": ["engineering"],
        "description": "",
        "content": "As a female Software Engineer, I know the benefits of having other females in the field to fellowship with as well as the benefits of early exposure to coding and the technology field. The Girls in Technology Movement Hour of Code event held December 9th, 2015 at Cerner’s Innovation Campus was a great opportunity for girls from nearby schools to gather and hear a little about the technology industry, the opportunities available for their future, meet some women in the industry, and get a hands-on introduction to coding.\nThe girls worked in pairs to complete exercises using coding constructs. The exercises from code.org followed a Star Wars theme and included things like moving a robot and escaping Stormtroopers while earning points for certain events. The intent of the event was for it to be easy to learn even for those who have no experience with code. Blocks of code had to be put together to accomplish the tasks. They could then run the code to see what happens and how it needed to be adjusted to work next time if the desired outcome wasn’t reached the first run through. A few pairs were enjoying it so much that they finished early and used the time to work on the Minecraft one as well. It was great to see them enjoying it so much that they didn’t want to stop.\n        \nThe girls at this event came for various reasons, from genuine interest in technology and coding to just wanting to get out of school and go on a fun trip. And who can blame them, right? It was a fun event! They learned teamwork through working in pairs, bouncing ideas back and forth to figure out the solution to the problem. Some of them would get excited when they figured out the harder problems. It reminded me of that feeling I get when working on a project and finally figuring out a tough part that had been stumping me for awhile. If you work with code, you know that feeling.\nAfter the exercises, the girls had the opportunity to talk with a female Cerner engineer. This was a chance for them to ask questions and learn more about what it’s like to be a software engineer. I spoke with a senior in high school who asked things about what college I attended, what types of classes I took, how I like my job, and what I do. I was able to share some of the fun events we have at Cerner like DevCon and Programmer’s Day.\nI didn’t have any exposure to coding in school until I got to college, so seeing these middle and high school girls get the opportunity to see what coding and the technology industry is like is awesome. I’m glad to see something like this being put together and to have the chance to be a part of it.\n   "
    },
    {
        "uri": "https://engineering.cerner.com/authors/aaron-blythe/",
        "title": "Aaron Blythe",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/david-crowder/",
        "title": "David Crowder",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/identifying-network-acl-issues-with-chef-locally/",
        "title": "Identifying Network ACL Issues with Chef Locally",
        "tags": ["engineering"],
        "description": "",
        "content": "At Cerner, we use Chef to automate our deployments across a very large distributed system comprised of many thousands of nodes. While we employ ServerSpec and Test Kitchen extensively to test our Chef recipes, this still doesn\u0026rsquo;t identify all possible failure scenarios. In a large distributed system such as ours, we occasionally run into connectivity failures between machines due to network ACL issues. This could be due to a variety of reasons. The simplest being that you didn’t know what all you were connecting to. The second being that you didn’t ask for the ACLs to be opened early enough, or at all. And finally (and most frustrating), you asked and were granted an ACL change but there still exists an issue (like an unfollowed redirect). When we took a look at these issues, we saw an opportunity for improvement in our Chef testing.\nThis is why we created ops_tcpdump_handler, an open source Chef cookbook that is helping us identify network connectivity issues during Chef test runs.\nAs explained in the project README, when you add this cookbook into a Test Kitchen or Vagrant run list, you\u0026rsquo;ll see an output similar to below that shows connection attempts between nodes. This will not only help you understand what nodes you\u0026rsquo;re connecting to in a Chef run, but also diagnose any connectivity issues that arise.\n==\u0026gt; node01: attempted to connect to: proxy06.fedoraproject.org using http ==\u0026gt; node01: attempted to connect to: mirror.sfo12.us.leaseweb.net using http ==\u0026gt; node01: attempted to connect to: li63-48.members.linode.com using https ==\u0026gt; node01: attempted to connect to: 23.235.40.133 using https ==\u0026gt; node01: attempted to connect to: github.com using https ==\u0026gt; node01: attempted to connect to: codeload.github.com using https ==\u0026gt; node01: attempted to connect to: s3-1.amazonaws.com using https We\u0026rsquo;re currently using this cookbook in our Chef test runs to identify and help troubleshoot connectivity issues between nodes. Next, we\u0026rsquo;re going to incorporate this into our continuous delivery pipeline to verify connectivity first before proceeding with the deployment. We decided to open source this work to get feedback from the community and to hear your experience with these types of issues. We\u0026rsquo;d love to hear your experience with incorporating this cookbook into your pipeline or other approaches you\u0026rsquo;ve taken with these types of issues.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/garry-polley/",
        "title": "Garry Polley",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/javascript-logging-we-can-do-better/",
        "title": "JavaScript Logging: We can do better!",
        "tags": ["engineering"],
        "description": "",
        "content": "Currently in the world of JavaScript these options are what we most commonly use to generate logs:\n console.log console.info console.warn console.error  These are actually pretty good in most modern browsers. Even if you go back to Internet Explorer 8 console.log and friends work as long as you have the developer tools open.\nGiven that we have these logging utilities what is the problem with using them? When in local development these are just fine for helping debug and speed up development. They can be used to help you quickly catch errors or see where you\u0026rsquo;re starting to go astray when using a library.\nconsole.log and friends allow you to see what\u0026rsquo;s going on and leave notes for other developers in the future. This is okay for local development. However, what do you do once you move into production? Almost everyone removes console commands before code is served in production.\nWithout console commands in production how do you have the same level of logging you\u0026rsquo;re used to with standard applications? When using a web server you get to see most every error that occurs. Each 500 is logged to an error log file for every error that occurs for every user. This is not something that really exist for JavaScript. By the nature of how web browsers work we do not get any errors that occur for the end user.\nHere are the issues with JavaScript logging today:\n Local development is the only way to see the errors. Logs are distributed across many clients. Errors usually lack local stack context. We do not know when a user sees an error.  Given the problems listed above you may ask: \u0026ldquo;Why should I care?\u0026rdquo; We\u0026rsquo;ve gotten along for years without getting JavaScript errors. Try not to follow this line of flawed reasoning. Despite spending years without tracking analytics about how people use our sites, we now view those tracking analytics as invaluable. Once you start seeing your JavaScript errors at the same rate and volume as your server side errors you will view those logs as invaluable. Most importantly developers will finally be empowered to provide proactive fixes for JavaScript errors the same way we can fix server side errors in a proactive fashion.\nImagine this scenario, you have an advanced search feature in your application. This search feature works two fold: it has an AJAX call to fill out the search results as well as a two-layer UI with a drop down that shows the results. When a result is clicked, it opens a more detailed modal of those results.\nIn most cases this kind of interaction is JavaScript heavy. How do you know when the searches fail due to a scripting error, instead of a network drop on the client? What can you do to be proactive about issues occurring here?\nWe\u0026rsquo;ve released a logging framework, Canadarm, to make identifying and handling these kinds of situations easy. Now each time a script error occurs you\u0026rsquo;ll get to see it. As long as the client can connect to the Internet and execute JavaScript you\u0026rsquo;ll get to see what went wrong. A common issue you may not realize in local testing is a Unicode search error. This logger will tell you what error occurred as well as the language and encoding used to read your page.\nBelow are a some topics that are likely to cross your mind. This post will cover each of them in detail.\n What does Canadarm do? How does Canadarm work? Who has used Canadarm? Has Canadarm helped solve any problems? When can I use it?  This post will cover all of these questions in detail.\nWhat does Canadarm do? Canadarm makes it easy to send logs to a remote server. This alone is nothing novel and isn\u0026rsquo;t all that impressive. It\u0026rsquo;s fairly easy to setup a try/catch around your code and send that error to a server via a GET request. The real advantage to Canadarm comes in what it does to catch the errors.\nCanadarm has three ways to gather errors:\n Automatically catch all window.onerror events (least useful due to lack of context) Automatically catch all errors that occur when events fire (most useful because it \u0026ldquo;just works\u0026rdquo;) Manually watch or attempt individual function calls  These modes allow you to write your code and not have to worry about logging or catching errors yourself. Any global errors will be caught and more specifically all errors bound to events will be caught. The ability to catch errors related to events is the most useful feature of Canadarm.\nMost errors that will occur on your web pages happen when a user performs some sort of action. Canadarm is able to provide you with context specific error messages by automatically hooking into and monitoring functions bound to events.\nWith the Canadarm.watch and Canadarm.attempt functions, you have the power to individually monitor specific functions. Let\u0026rsquo;s say you have a function that gets called without an event being fired. You can call attempt on that function which will immediately invoke the function. If an error occurs, the error will be logged. With watch you can watch a function once and every time it throws an error later during execution the error will be logged.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  function fastMath() { var addedItems = 0, i; for (i = 0; i \u0026lt; arguments.length; i++) { addedItems += argument[i]; // This typo will throw an error when called.  } return addedItems; } // Immediately attempt to execute fast math with the arguments 1,2,3 Canadarm.attempt(fastMath, 1,2,3); // Override fastMath with the watched version. fastMath = Canadarm.watch(fastMath);   If you don\u0026rsquo;t want Canadarm to automatically log global errors and/or event based errors you can opt-out of this feature. With watch and attempt you can write your JavaScript how you want to and not worry about what is going on within Canadarm.\nFinally, you get out of Canadarm what you really wanted from console functions. You can log specific error messages at the point you want to via these logging commands:\n Canadarm.debug(msg, error) Canadarm.info(msg, error) Canadarm.warn(msg, error) Canadarm.error(msg, error) Canadarm.fatal(msg, error)  Optionally, you can provide two more arguments after msg and error. data followed by options. You can see the usage of these arguments over in the Canadarm documentation. Specifically, data is the most useful here. data allows you to pass an extra object that will get its values passed to the appenders. The default appender included in Canadarm will log all these values for you as key-value pairs.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  function addPositives() { var value; try { var addedItems = 0, i; // Add all values together.  for (i = 0; i \u0026lt; arguments.length; i++) { value = arguments[i]; // If the value is negative we throw an error.  if (value \u0026lt; 0) { throw Error(); } addedItems += value; } return addedItems; } catch (e) { // This gives a very specific error, likely relating to business logic of a  // case that should not occur.  Canadarm.error(\u0026#39;A negative value \u0026#39; + value + \u0026#39; was given.\u0026#39;, e) return undefined; } } // In the console (if the console handler is enabled) you will see the error message. addPositives();   To find out more on how to configure and use Canadarm go and check out its documentation. It\u0026rsquo;s pretty easy though. You only need to include the Canadarm code and then configure the logger. As seen on the Canadarm readme, you can do the following to get a working local logger:\n1 2 3 4 5 6 7 8 9 10 11 12  Canadarm.init({ onError: true, wrapEvents: true, logLevel: Canadarm.level.DEBUG, appenders: [ Canadarm.Appender.standardLogAppender ], handlers: [ Canadarm.Handler.consoleLogHandler, Canadarm.Handler.beaconLogHandler(\u0026#39;http://example.com/beacon_url\u0026#39;) ] });   Now you\u0026rsquo;ll see all logged errors in your console with all the information the standardLogAppender provides. Obviously you want more than local logs. Next you\u0026rsquo;ll see how our teams have used this logger.\nHow does this work? Canadarm is fairly simple. The logger catches an error and then sends that error to a central server. Under the covers it uses Appenders and Handlers as the mechanisms to achieve this result.\nAppenders An appender works as a way to process an error or log event that occurs. The appender has this signature: appender(level, exception, message, data).\n level - Level of the log, one of DEBUG, INFO, WARN, ERROR, FATAL exception - An actual JavaScript Error object. message - Text message of the logged error. data - Extra information to provide to the appender, usually this is not used.  An appender must return an object. The object should contain simple data types. They are single key/value pairs, usually strings. The return value of an appender is then passed to a handler.\nHandlers Handlers take action on the objects produces by the appenders. A handler\u0026rsquo;s job is to send the results of the appenders somewhere. By default there are two handlers that come out of the box with Canadarm: a console handler that logs all errors to the console and a beacon handler that sends all errors to a given URL end point.\nAppenders \u0026amp; Handlers Appenders and handlers work together to create your logs. Here\u0026rsquo;s the break down of what happens during an error or logging event:\n Error or log event happens Every appender is iterated over in order (duplicate keys will be replaced with the value of a later appender) A final object is created from the output of all appenders The final object is passed to every handler Each handler usually sends this information somewhere (e.g. console or remote server)  That\u0026rsquo;s it for how the logger works on the client. The real power comes when you combine this log gathering with the BeaconHandler. The logs gathered are then sent to a server. The server receiving these logs should be writing them out to a file that is then read into a logging system. We currently use a simple Apache server and treat its access logs as our JavaScript error logs. We then send the logs to a log aggregation tool, Splunk.\nWho has used it? We have a few applications that have begun using Canadarm.\n HealtheLife - website for patients to manage their health Internal Sites - a few sites we use internally for a few things (e.g. code review, cheat sheets etc.)  HealtheLife HealtheLife was the first client facing application to go into production using Canadarm. They have over 3 million users and at any given moment they usually have at least one thousand concurrent users. These metrics matter for two reasons: first, it shows us that Canadarm works at scale without causing issues to the application, second, we have been able to see trends in JavaScript errors occurring in this application.\nFor those who thought \u0026ldquo;why should I care?\u0026rdquo; when it comes to JavaScript logs this go live was an interesting story. Within the first 20 minutes we noticed errors that occurred on every page load. Specifically this error was a reference to $ (jQuery) before it was defined. Since this was in an analytics tracking snippet, and at the end of a script tag, it did not cause an end user impact, beyond eating processing time to handle an error on every page.\n   However, it did mean that analytics were not getting tracked how the application intended. In fact, without this logger in place the application would have happily continued along with no indication certain actions where never taking place. Since the analytics tool did not report the expected user interactions, it appeared as if features of the application where not getting used, or worse, that the analytics were faulty.\nThe actual messages in the errors for this application are interesting. Since HealtheLife is used in many countries with many different locales, they support various languages. Because of the various supported languages and users being able to use their browsers in any locale they want, we had a few interesting logs messages. Specifically we have had a few logs come across in English, Spanish, German and more. It was kind of eye opening to know that errors are actually translated within a browser.\nInternal Sites Currently a few internal sites are using Canadarm for local development and integration environments. Most interesting so far for has been looking at the logs and seeing who has been copy pasting code around.\nInterestingly enough I found some random logs on our Splunk dashboard in dev.\n   Which lead me to github, specifically a github pages site.\n   Seeing the application and where the logs said the application lived I was able to find the source code. The code then lead me to the owner of the application. At that point I was able to contact the owner and get the issue fixed. Finding another application\u0026rsquo;s errors and letting the owner know about is an interesting experience. The whole interaction was cool because it was not a use case we had considered when building Canadarm.\nHas it helped solve any problems? As mentioned before this has helped to point out two issues: one for HealtheLife and another for an internal application. Pointing out issues is not enough to fix them though. Also, Canadarm does not solve problems on its own. You get the most out of logging when you use a tool to aggregate those logs. We\u0026rsquo;ve been using Splunk to aggregate our logs.\nCombined with the searching and reporting of Splunk we\u0026rsquo;ve been able to leverage the logs generated by Canadarm to see a few common trends in our code. Canadarm has helped us to see a few common problems we have:\n Locally we produce a lot of JavaScript errors We often introduce new errors when we write visualizations Referencing variables before they exist New frameworks are hard to get a handle on  Using Canadarm to generate logs doesn’t solve problems on its own. It\u0026rsquo;s when we combine those logs with the reporting capabilities of Splunk that we can see trends and identify areas we need to improve upon in our development.\n      On our teams it has shown that we need to get better at defining our APIs for data visualizations. I\u0026rsquo;ve been able to see many errors from our developers when they first try to update or modify any of our visualizations. Without this logging in place it would not be so obvious that our current API is not working well for others. This gives us empirical data that developers are having issues using our software. Without this data, we\u0026rsquo;d have to rely on complaints and hope people reported the issues they encountered when using our code.\nAs a more concrete example, recently one of our teams has begun to try and use react. By analyzing the logs it\u0026rsquo;s easy to see that we have had some issues getting a handle on how react works. Over the few weeks of react work we could clearly see a number of errors. This shows we need a lot more training on how to properly use react. Further it shows me that if we plan to adopt react as our frontend framework that we need to put together a \u0026ldquo;gotcha\u0026rdquo; or \u0026ldquo;tips and tricks\u0026rdquo; guide for getting started.\nWhile the logger is not directly \u0026ldquo;solving\u0026rdquo; problems it is helping to illuminate issues we are seeing in local development as well as in production. By shining a light on these issues we are able to move forward and solve these problems ourselves. Sometimes the problems may be solved by additional training or they may be solved by code changes. Most importantly Canadarm is opening our eyes to the kinds of issues we\u0026rsquo;ve had for years with JavaScript. We can no longer ignore these issue because we have solid empirical evidence showing us our problems.\nWhen can I use it? After reading along this far hopefully you\u0026rsquo;re thinking: \u0026ldquo;This all sounds great! When and how can I get started?\u0026rdquo;. It\u0026rsquo;s pretty easy:\n Include Canadarm in your JavaScript (as early as possible) Configure Canadarm Have a server to handle logs Have a reporting tool on top of your logs  Standalone applications If you are a public facing Internet application or a small startup, Canadarm can still be a great investment. Ideally you do not need to worry about steps 3 and 4 above. Simple Log management solutions, such as Loggly should be enough for your needs.\nHere\u0026rsquo;s what I did to get a quick setup working:\n Setup a free account Fill out info in the pop up Go to https://YOUR_SUB_DOMAIN_HERE.loggly.com/sources/setup/https See the \u0026ldquo;step 2\u0026rdquo; section you should have a URL to copy that looks something like:  http://logs-01.loggly.com/inputs/WWWWWWW-55555-5555555-55WW55-WWWWW55555/tag/http/ 5. Configure Canadarm with this end point to see the logs:\nCanadarm.init({ onError: true, wrapEvents: true, appenders: [ Canadarm.Appender.standardLogAppender ], handlers: [ Canadarm.Handler.beaconLogHandler('http://logs-01.loggly.com/inputs/WWWWWWW-55555-5555555-55WW55-WWWWW55555/tag/http/'), Canadarm.Handler.consoleLogHandler ] });  After this setup it was pretty easy to get some graphs going. For example you can easily see what errors occurred by message in this pie chart:\n   Even easier is getting to view the raw output of a given event message:\n   Loggly is a great tool to use for large and small projects. A big bonus for anyone starting to use Loggly for their JavaScript logs it that they can begin to use Loggly for their other logs (if they are not already). While Loggly may not be ideal for every situation when it comes to logging, it is really handy when you do not have the resources, money, or time to setup your own log aggregation tool.\nSummary From desktop, to mobile, to embedded devices, web browsers can be seen everywhere. With the help of Canadarm we can now see what exactly is happening within our applications. An entire world of client side errors and issues can now be properly managed and acted upon. Combine these logs with an aggregation tool such as Splunk or Loggly and you have enabled operational intelligence.\nThe next time a user logs an issue for a JavaScript error you can respond by telling them you\u0026rsquo;ve seen the error and are already working to correct it. Gone are the days of reactive fixes. Now you can worry about proactive solutions.\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/managing-30000-logging-events-per-day-with-splunk/",
        "title": "Managing 30,000 Logging Events per Day with Splunk",
        "tags": ["engineering"],
        "description": "",
        "content": "Our team works on a patient facing web application with a thousand live clients with 2,315,000+ users. On an average, the high traffic results into more than 40,000 visits and 300,000 page views daily generating about 30,000 logging events. A considerable portion of these events are of information and warning level in order to aid proactive monitoring or identify potential issues due to clients’ misconfiguration.\nBefore Splunk To handle this large volume of logging, our team created a rotational support role to manually monitor the logs at regular intervals daily. We built a custom log viewer that would aggregate application logs and the engineer on the support role was expected to watch this tool manually to identify issues. Although we were able to identify problems such as bad client builds or service errors, it was not very efficient nor accurate in quickly determining end user impact stats. Since there was no way to tag previously identified and resolved issues, often times newer support engineers lacked knowledge to react to a problem. This led to unnecessary escalation and engagement of next tier support.\nBelow: Our old log aggregator used to identify the occurrences (2) of a log with the stack trace.\n   Splunk Round 1 Once we migrated to Splunk we were very excited about the capabilities it offered, especially around searching and data visualization. In addition to searching logs more effectively, we were able to extract meaningful information from our logs unlike before. Splunk gave us the ability to identify client and end user impact down to the user id across all events in our logs [see image below]. This helped us gain more insight into our problems and trends in terms of impact to users. For a particular problem, we were able to quickly conclude whether all clients were affected, whether clients affected were over a virtual network only, and if the issue was isolated to a specific user. This information gave us the ability to determine the impact of issues coming into our logs, including the area of the site being impacted and frequency.\nBelow: Once we extracted meaningful fields in our logs, we could identify impact. In this case, an issue is spread across 9 orgs and and 28 users.\n   Although we had crossed some of the hurdles which made log monitoring difficult shortly after moving to Splunk, monitoring logs for issues was still not an easy job. It was possible to overlook issues since there was no effective way of studying trends. Initially, we created dashboards which helped identify organizations having problems. This was slightly useful but failed to depict more important graphical representation of the different types of occurring issues for a particular client or for all clients at a given time.\nBelow: Reports like these weren\u0026rsquo;t very helpful. Clients with more users tend to have more errors, so this trend doesn\u0026rsquo;t necessarily indicate a client is experiencing a downtime.\n   Splunk Round 2 It didn\u0026rsquo;t take us long to realize that we had to get a better handle on our logs to stay on top of increasing traffic caused by a growing user base. Although we were able to identify frequently occurring errors, we still needed a more effective way to identify known issues, service issues, configuration issues and application issues. In order to do that, we needed something more meaningful than a stack trace to track issues. We needed to tag events, and to do that, we turned to the eventtypes feature offered by Splunk.\nEventtypes are applied at search time and allow you to create events to tag search results with. Because they are applied at search time, we were able to add new event types and have them applied historically throughout our logs. This also gave us the ability to tweak our event types to add more known issues as we continued identifying them. Once we successfully gauged a way to take advantage of eventtypes, we came up with a query that created a stacked timechart of eventtypes where eventtypes represented known issues. Once we reached the improved level of production monitoring, the following had to be done:\n Create an eventtype with least priority that catches all problems and label it \u0026ldquo;Unknown Issue.\u0026rdquo; Go through “Unknown Issues\u0026quot; and create prioritized eventtypes that describe the problem in english. Once an issue is logged in our bug tracking system, tag the eventtype with that id for easy tracking. Repeat daily.  Below: Eventtypes give us the ability to see known problems that happen over time. We can even see known problems broken down by client.\n      Once we had our frequently occurring problems categorized, we were able to break it down even further. We could identify problems caused by configuration in our application layer, problems that required escalation or if client side contacts needed to be engaged.\nBelow: We now have the ability to track impact to users from clients not taking a service package [left], or from improper Service Configuration [right].\n      ###Alerting\nWe\u0026rsquo;ve also started taking advantage of Splunk\u0026rsquo;s alerting. With its powerful searching abilities, we have scheduled searches that trigger an alert when a particular condition is met. For example, when a client has misconfigured certain credentials that cause authentication errors all over the site, we can engage support immediately to get it resolved.\nWhat\u0026rsquo;s Next? Although we have a better understanding of our logs now, it can get even better. We plan on continually categorizing our logs so that monitoring our system becomes really simple for everyone. Once all of our known issues are categorized, we wish to have a scheduled search that can identify anomalies in the logs. This would be highly beneficial to find out if a release introduces issues.\nSince our site is dependent on multiple services, most of the service problems are resolved by escalated support. We are currently working on identifying problems with known resolutions along with the people that need to be contacted to perform the resolution steps. Eventually we would like to send alerts/emails from Splunk to Cerner’s general support directly for these issues.\nWe also plan on integrating Jira into splunk with the help of the Splunk Jira app. This will give us the ability to not only track issues in our logs, but also view their current status (investigation, assigned, fixed, resolved). This closes the loop on finding new issues, tracking their impact, and finally their resolution until the end. Splunk has been extremely exciting to work on and has been an invaluable asset to our team. We\u0026rsquo;d love to continue the conversations on how we can improve our usage of Splunk and how others are using it as well.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/mike-hemesath/",
        "title": "Mike Hemesath",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/rima-poddar/",
        "title": "Rima Poddar",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/cerner-and-the-apache-software-foundation/",
        "title": "Cerner and the Apache Software Foundation",
        "tags": ["culture", "engineering"],
        "description": "",
        "content": "At the beginning of this year, [we announced that Cerner became a bronze-level sponsor]({%post_url 2014-01-28-sponsoring-the-apache-software-foundation%}) of the non-profit Apache Software Foundation (ASF). Many of the open source projects we use and contribute to are under the ASF umbrella, so supporting the mission and work of the ASF is important to us.\nWe\u0026rsquo;re happy to announce that Cerner has now increased our sponsorship of the ASF to become a silver-level sponsor. Open source continues to play an integral role in both our architecture and engineering culture. We\u0026rsquo;ve blogged and spoken at conferences about how several ASF projects are core foundational components in our architecture and several of our tech talks have focused on ASF projects.\nFurther increasing our sponsorship of the ASF reaffirms our continued support for an organization that provides homes for numerous open source projects that are important not only to us, but the larger development community.\n ASF Silver Sponsorship\n   "
    },
    {
        "uri": "https://engineering.cerner.com/tags/closures/",
        "title": "closures",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/closures-and-currying-in-javascript/",
        "title": "Closures &amp; Currying in JavaScript",
        "tags": ["javascript", "development", "engineering", "closures", "currying", "functional programming"],
        "description": "",
        "content": "Preface I have been asked many times what closures are and how they work. There are many resources available to learn this concept, but they are not always clear to everyone. This has led me to put together my own approach to exchanging the information.\nI will supply code samples. //\u0026gt; denotes an output or return.\nBefore discussing closures, it is important to review how functions work in JavaScript.\nIntroduction to functions If a function does not have a return statement, it will implicitly return undefined, which brings us to the simplest functions.\nNoop Noop typically stands for no operation; it takes any parameters, does nothing with them, and returns undefined.\nfunction noop() {}; noop(\u0026#34;cat\u0026#34;); //\u0026gt; undefined Identity The identity function takes in a value and returns it.\nfunction identity(value) { return value; } identity(\u0026#34;cat\u0026#34;); //\u0026gt; \u0026#34;cat\u0026#34; identity({a: \u0026#34;dog\u0026#34;}); //\u0026gt; Object {a: \u0026#34;dog\u0026#34;} The important thing to note here is that the variable (value) passed in is bound to that function’s scope. This means that it is available to everything inside the function and is unavailable outside of it. There is an exception to this, being that objects are passed by reference which will prove useful with the use of closures and currying.\nFunctions that evaluate to functions Functions are first class citizens in Javascript, which means that they are objects. Since they are objects, they can take functions as parameters, have methods bound to them, and even return functions.\nfunction foo() { return function () { return true; } } foo()(); //\u0026gt; true This is a function that returns a function which returns true.\n   Functions take arguments and those arguments can be values or reference types, such as functions. If you return a function, it is that function you are returning, not a new one (even though it might have just been made to return).\nClosures Creating a closure is nothing more than accessing a variable outside of a function’s scope (using a variable that is neither bound on invocation or defined in the function body).\nTo elaborate, the parent function’s variables are accessible to the inner function. If the inner function uses its parent’s (or parent’s parent’s and so on) variable(s) then they will persist in memory as long as the accessing functions(s) are still referenceable. In JavaScript, referenceable variables are not garbage collected.\nLet’s review the identity function:\nfunction identity(a) { return a; } The value, a, is bound inside of the function and is unavailable outside of it; there is no closure here. For a closure to be present, there would need to be a function within this function that would access the variable a.\nWhy is this important?\n Closures provide a way to associate data with a method that operates on that data. They enable private variables in a global world. Many patterns, including the fairly popular module pattern, rely on closures to work correctly.  Due to these strengths, and many more, closures are used everywhere. Many popular libraries utilize them internally.\n   Let’s take a look at an example of closure in action:\nfunction foo(x) { function bar(y) { console.log(x + y); } bar(2); } foo(2); // will log 4 to the console The outer function (foo) takes a variable (x), which, which is bound to that function when invoked. When the internal function (bar) is invoked, x (2) and y (2) are added together then logged to the console as 4. Bar is able to access foo\u0026rsquo;s x-variable because bar is created within foo\u0026rsquo;s scope.\nThe takeaway here is that bar can access foo’s variables because it was created within foo’s scope. A function can access variables in its scope and up the chain to the global scope. It cannot access other function’s scopes that are declared within it or parallel to it.\n   No, a function inside of a function doesn\u0026rsquo;t have to reference variables outside of its scope. Recall the example function which returned a function which evaluated to true:\nfunction foo(x) { // does something with x or not  return function () { return true; } } foo(7)(); //\u0026gt; true No matter what is passed to foo, a function that evaluates to true is returned. A closure only exists when a function accesses a variable(s) outside of its immediate scope.\nThis leads into an important implication about closures, they enable you to define a dataset once. We’re talking about private variables here.\nWithout closures, you recreate the data per function call if you want to keep it private.\nfunction foo() { var private = [0, 1, 2]; // Imaginary large data set - instantiated per invocation  console.log(private); } foo(); //\u0026gt; [0, 1, 2] We can do better! With a closure, we can save it to a variable that is private, but only instantiated once.\nvar bar = (function () { var private = [0, 1, 2]; // Same large imaginary data set - only instantiated once  // As long as this function exists, it has a reference to the private variable  return function () { console.log(private); } }()); bar(); //\u0026gt; [0, 1, 2] By utilizing closure here, our big imaginary data set only has to be created once. Given the way garbage collection (automatic memory freeing) works in JavaScript, the existence of the internal function (which is returned and set to the variable bar) keeps the private variable from being freed and thus available for subsequent calls. This is really advantageous when you consider large data sets that may be created via Ajax requests which have to go over the network.\nCurrying    Currying is the process of transforming a function with many arguments into the same function with less arguments.\nThat sounds cool, but why would I care about that?\n Currying can help you make higher order factories. Currying can help you avoid continuously passing the same variables. Currying can memorize various things including state.  Let’s pretend that we have a function (curry) defined and set onto the function prototype which turns a function into a curried version of itself. Please note, that this is not a built in feature of JavaScript.\nfunction msg(msg1, msg2) { return msg1 + \u0026#39; \u0026#39; + msg2 + \u0026#39;.\u0026#39;; } var hello = msg.curry(\u0026#39;Hello,\u0026#39;); console.log(hello(\u0026#39;Sarah Connor\u0026#39;)); // Hello, Sarah Connor. console.log(msg(\u0026#39;Goodbye,\u0026#39;, \u0026#39;Sarah Connor\u0026#39;)); // Goodbye, Sarah Connor. By currying the msg function so the first variable is cached as “Hello,”, we can call a simpler function, hello, that only requires one variable to be passed. Doesn’t this sound similar to what a closure might be used for?\nIn the discussion of functional programming concepts, there is often a sense of resistance.\n   The thing is, you’ve probably already been functionally programming all along. If you use jQuery, you certainly already do.\n   $(\u0026#34;some-selector\u0026#34;).each(function () { $(this).fadeOut(); // other stuff to justify the each });    Another place you may have seen this is utilizing the map function for arrays.\nvar myArray = [0, 1, 2]; console.log(myArray.map(function (val) { return val * 2; })); //\u0026gt; [0, 2, 4] Conclusion We’ve seen some examples of closures and how they can be useful. We’ve seen what currying is and more importantly that you’ve likely already been functionally programming even if you didn’t realize it. There is a lot more to learn with closures and currying as well as functional programming.\nI ask you to:\n Work with closures and get the hang of them. Give currying a shot. Embrace functional programming as an additional tool that you can utilize to enhance your programs and development workflow.  Additional readings and inspirations  JavaScript Allongé MDN Closures Understand JavaScript Closures With Ease Partial Application in JavaScript Curried JavaScript Functions  Bonus Check out how you can utilize closure and currying to manage state throughout a stateful function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  function setFoo(state) { if (state === \u0026#34;a\u0026#34;) { // Specific state  return function () { console.log(\u0026#34;State a for the win!\u0026#34;); }; } else if (state) { // Default state  return function () { console.log(\u0026#34;Default state\u0026#34;); }; } // Empty function since no state is desired. This avoids invocation errors.  return function () {}; } var foo = setFoo(\u0026#34;a\u0026#34;); // Set to the specific state (a) foo(); //\u0026gt; \u0026#34;State a for the win!\u0026#34;;  foo = setFoo(true); // Set foo to its default state foo(); //\u0026gt; \u0026#34;Default state\u0026#34;  foo = setFoo(); // Set foo to not do anything foo(); //\u0026gt; undefined // etc   Bonus 2 Checkout how closures and currying can be used to create higher order functions to create methods on the fly: http://jsfiddle.net/GneatGeek/A9WRb/\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/currying/",
        "title": "currying",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/functional-programming/",
        "title": "functional programming",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/intern-hackfest-2014/",
        "title": "Intern HackFest 2014",
        "tags": ["engineering"],
        "description": "",
        "content": "Ten teams of two to four Cerner interns competed in a week-long HackFest this summer, working to solve any problem they put their minds to. This competition cumulated in a presentation and judging of projects, with prizes of Raspberry Pi Kits for each member of the second place team and Leap Motions for each member of the winning team. From mobile apps, to machine learning algorithms, to drones…this year’s Summer Intern HackFest has been one for the books.\n   We called ourselves Team Rubber Duck Dynasty, and it was made up of Umer Khan (University of Notre Dame), Ryan Boccabella (University of Notre Dame), MaKenzie Kalb (Vanderbilt University), and Jake Gould (University of Kansas).\nWe were excited to get to work the first night when the week-long competition had commenced. Since the beginning of the summer, all of us had been impressed with the caliber of talent Cerner brought into the Software Engineer Internship program. All of the nine teams we were up against were made up of remarkably smart, driven college students from all over the country. One of the most difficult parts of the HackFest was deciding on an interesting and competitive project that could be feasibly completed in only a week (without too many sleepless nights). One of our four team members was a member of the iOS team, and convinced us that an iOS game was the way to go. We wanted to make a game that we would be excited to show our friends as well as the judges.\nWe ended up building an app called Encore. It is a musical turn-based game revolving around the creation and mirroring of three second tunes between users. Tunes are created using four arpeggio based tones from real piano, guitar, or tenor trombone recordings. The initiating iOS device and sends the data to the Parse server using the Parse API for iOS. Parse stores this data on the server and sends a push notification to the receiving iOS device. Each time a new game is created, an activity is logged on the server to keep track of the game data. When the receiving user selects the game, it downloads the game data from the server and starts the game. Once the app downloads the game data, it is programmed to decode an array of dictionaries of instrument key and time and convert the array into an audio playback; this allowed for faster upload and download times, as well as significantly smaller game data files. The receiving user hears and immediately attempts to replay the tune. Scoring is accomplished using a Needleman-Wunsch algorithm for sequence alignment. The receiving user now has their chance to create a tune, and the melodious competition continues.\nOver the week, we began to get to know our teammates even more than we probably wanted. Passion is the main word that comes to mind when we reminisce on this highlighting week of our summer. From the uncertainty when overhearing other groups huddled in a room talking excitedly about cutting-edge technologies, to the shrieks of excitement when a test finally passed that perhaps woke many a consulting intern roommate, this HackFest was filled with memories all around. As we went for a celebratory completion dinner the night before the presentations Monday morning, the satisfaction of completion was sweet in the air. Sitting there, playing our noisy pride and joy on our phones at the table, we agreed that the week was an excellent experience already…and we hadn’t even started the real judging yet.\n   Sound checks were full of nerves and excitement the morning we presented our project. The knowledge that each team had a mere five minutes to “sell” what had been more time consuming than sleep over the past week was a challenge everyone was hoping to ace. Later on that afternoon, when the esteemed judges Chris Finn, Michelle Brush, and Jenni Syed were announced as the event began, the caliber of the resources Cerner provides for their many interns was standing right in front of us. We heard from many enthusiastic, impressive groups that afternoon. The presentations showcased many feats of great teamwork and skill: a recommendation engine, dashboard for developers, chatting website, facial recognition android app, iOS game, machine learning algorithm, twitter-controlled drone, and music website.\nAfter a delicious ice cream break while scores were deliberated and after judges provided valuable feedback for each team, the moment of anticipation was upon us. All teams certainly completed the day with the ultimate reward of new skills learned, friends made, and a fantastic project that some are undoubtedly still building off of. As the first and second place teams were called to the stage, Team Rubber Duck Dynasty was surprised and thrilled to be among them. And as the runner up, Team Marky Mark and the Funky Bunch, received their Raspberry Pi Kits, we were amazed to find out each of us was taking home our very own Leap Motion.\n   We returned to our actual teams late that afternoon, proud of our accomplishments and brand new owners of a cutting-edge technology. We received the congratulations of our superiors and mentors, many of whom were our biggest encouragers to participate and supporters throughout the week. The numerous empowered associates that have guided us through this summer have been an unbelievable community - a community that all of us are incredibly grateful to have been a part of.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/makenzie-kalb/",
        "title": "Makenzie Kalb",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/design/",
        "title": "design",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/elliott-hoffman/",
        "title": "Elliott Hoffman",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/i18n/",
        "title": "i18n",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/text/",
        "title": "text",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/the-plain-text-is-a-lie/",
        "title": "The Plain Text is a Lie",
        "tags": ["design", "engineering", "i18n", "Unicode", "text"],
        "description": "",
        "content": "There is no such thing as plain text \u0026ldquo;But I see .txt files all the time\u0026rdquo; you say. \u0026ldquo;My source code is plain text\u0026rdquo; you claim. \u0026ldquo;What about web pages?!\u0026rdquo; you frantically ask. True, each of those things is comprised of text. The plain part is the problem. Plain denotes default or normal. There is no such thing. Computers store and transmit data in a number of methods; each are anything but plain. If you write software, design websites or test systems where even a single character of text is accepted as input, displayed as output, transmitted to another system or stored for later - please read on to learn why the plain text is a lie!\nThe topic of text handling applies to many disciplines:\n UX/web designers - Your UX is the last mile of displaying text to users. API developers - Your APIs should tell your consumers what languages, encodings and character sets your service supports. DBAs - You should know what kinds of text your database can handle. App developers - You apps should not crash when non-English characters are encountered.  After reading this article you will …\nThis topic has been extensively written about already. I highly recommend reading Joel Spolsky\u0026rsquo;s The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!). You should also read up on how your system handles strings. Then, go read how the APIs you talk to send/receive strings. Pythonistas, check out Ned Batchelder\u0026rsquo;s Pragmatic Unicode presentation.\nOK, let\u0026rsquo;s get started!\nPart I - Gallery of FAIL or \u0026ldquo;When text goes wrong, by audience\u0026rdquo; thumbnail: \u0026ldquo;resume.jpeg\u0026rdquo; Let\u0026rsquo;s start off by demonstrating how text handling can fail, and fail hard. The following screen shots and snippets show some of the ways text handling can fail and who should care about the type of failure.\nUX and web people    The above image shows the English wikipedia article on Résuméswith garbled text. Garbled text can happen if your web pages don\u0026rsquo;t specify an encoding or character set in your markup. Specifying the wrong encoding can also cause garbled text. XML and JavaScript need correct character sets too. It\u0026rsquo;s important to note that no error or exception was raised here. The text looks wrong to the user, but the failure happens silently.\n   This article on Tokyo above is displayed in a language (Aramaic) that my fonts don\u0026rsquo;t support. Instead of a symbol, we see a box with a number identifying the un-showable character. If you think that example is too contrived, here is a more commonly used symbol: a 16th note from sheet music. Many perfectly valid characters are not supported by widely used fonts. Specialized web fonts might not support the characters you need.\n   API developers //Fetch the Universal Declaration of Human Rights in Arabic documentAPIClient.getTitle(docID=123)    The result of this API call (example source) is similar to the last two examples: nonsense text. This can happen if the client and server use different text encodings. By the way, this situation happens so often that there\u0026rsquo;s a term for it: Mojibake.\nHere are some client/server scenarios resulting in Mojibake:\n The server didn\u0026rsquo;t document their encoding and the client guessed the wrong encoding. The server or client inherit the encoding of their execution environment (virtual machine, OS, parent process, etc.), but the execution environment\u0026rsquo;s settings changed from their original values.  DBAs    Database systems can be misconfigured such that characters sent to the database are not stored accurately. In this example, the offending characters are replaced with the imaginatively-named Replacement Character (\u0026ldquo;�\u0026rdquo;). The original characters are forever lost. Worse still, replacement characters will be returned by your queries and ultimately shown to your users. Sometimes, offending characters will be omitted from the stored value or replaced with a nearest match supported character. In both scenarios the database has mangled the original data.\nApp developers    org.scalatest.exceptions.TestFailedException: \u0026#34;d[Ã©]funt\u0026#34; did not equal \u0026#34;d[é]funt\u0026#34; at org.scalatest.MatchersHelper$.newTestFailedException(MatchersHelper.scala:160) ... at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134) The top image shows the 500 page of an app that crashed when improperly encoding. In the Scala error message (bottom), a property file was read in ISO-8859-1 encoding but had UTF-8 encoded bytes in it. This caused the unit test to fail.\nYour source code, web pages, properties files, and any other text artifact you work with has an encoding. Every tool in your development tool chain (local server, terminal, editor, browser, CI system, etc.) is a potential failure point if these encodings are not honoroed.\nPart II - Avoid text handling problems Ghost in the machine You\u0026rsquo;ve seen examples of failure and (hopefully) are wondering how such failures can be avoided. To avoid failure you must ask yourself one question: \u0026ldquo;Can my system store and transmit a ghost?\u0026rdquo;\n   GHOST (code point U+1F47B) is a valid (albeit weird) part of the Unicode standard. Unicode is a system of storing and manipulating text that supports thousands of languages. Using Unicode properly will go a long way to prevent text handling problems. Thus, if your system can store, transmit, read and write GHOST then you\u0026rsquo;re doing it right. But how to handle this GHOST?\nSome Terminology You need to know some terms before the rest of this article will make any sense.\nRemembering the difference between encode and decode can be difficult. One trick to keep them straight is to think of Unicode objects as the ideal state of being (thanks, Joel Spolksy) and byte-strings as strange, cryptic sequences. Encoding turns the ideal into a bunch of cryptic bytes, while decoding un-weirds a bunch of bytes back into the ideal state; something we can reason about. Some systems use different terms but the ideas still apply. For example: Java Strings are Unicode objects and you can encode/decode to/from byte-strings with them.\nNow that you\u0026rsquo;ve got the necessary terminology under your belt, let\u0026rsquo;s prevent text handling problems in our system by making a sandwich; a Unicode sandwich!\nMake a Unicode sandwich Analogy credit: Ned Batchelder coined the Unicode sandwich analogy in his Pragmatic Unicode presentation at PyCon 2012 (video). It\u0026rsquo;s so clever that I can\u0026rsquo;t resist re-using it in this article!\n   Original imageIn this analogy the pieces of bread on the top and bottom are regions of your code where you deal with byte-strings. The meat in the middle is where your system deals in Unicode objects. The top bread is input into your system such as database query results, file reads or HTTP responses. The bottom bread is output from your system such as writing files or sending HTTP responses. The meat is your business logic.\nGood sandwiches are meaty Your goal is to keep the bread thin and the meat thick. You can achieve this by decoding from byte-strings to Unicode objects as early as you can; perhaps immediately after arrival from another system. Similarly, you should do your encoding from Unicode objects into byte-strings at the last possible moment, such as right before transmitting text to another system.\nWorking with Unicode inside your system gives you a common ground of text handling that will largely avoid the errors we\u0026rsquo;ve seen at the top of this article. If you don\u0026rsquo;t deal in Unicode inside your system then you are limiting the languages you support at best and exposing yourself to text handling bugs at worst!\nThe best sandwich bread is UTF-8 Your system ultimately needs to send and receive byte-strings at some point, so you must choose an encoding for your byte-strings. Encodings are not created equal! Some encodings only support one language. Some support only similar languages (for example, German and French but not Arabic). Never assume your system will only encounter languages you speak or write! Ideally you will choose encodings that support a great many languages.\nUTF-8 is the best general purpose encoding for your byte-strings. You\u0026rsquo;ll learn why UTF-8 is an excellent encoding choice later in this article in the Unicode: One standard to rule them all section. For now I recommend you:\n Choose UTF-8 for all byte-strings. Configure your system to use this encoding explicitly. Do not rely on the parent system (OS, VM, etc.) to provide an encoding since system settings might change over time. Document your encoding choice in both public facing and internal documentation.  The UTF-8 encoding supports all the text you\u0026rsquo;d ever want. Yet, in this imperfect world you might be forced to use a more limited encoding such as ISO-8859-1 or Windows-1252 when interfacing with other systems. Working with a limited encoding presents problems when decoding to and encoding from Unicode: not every encoding supports the full Unicode range of characters. You must test how your system converts between your byte-strings and Unicode objects. In other words, test between the meat and the bread.\nTesting between the meat and the bread The critical areas to test are where bytes strings are decoded to Unicode objects and where Unicode objects are encoded into byte-strings. If you\u0026rsquo;ve followed the advice of this article thus far then the rest of your app logic should operate exclusively in Unicode objects. Here is a handy table of how to test regions of your system that encode and decode:\nUnicode sandwich applies to new projects and legacy systems Using UTF-8 for I/O, Unicode inside and testing the in-between points will save you from pain and bugs. If you\u0026rsquo;re building a new system then you have the opportunity to design it with Unicode in mind. If you have an existing system, it is worth your time to audit how your system handles text.\nWith the practical stuff out of the way, let\u0026rsquo;s dive deeper into computers and text!\nPart III - Encodings, Unicode and how computers handle text We\u0026rsquo;ve talked about how you should use Unicode, encodings and byte-strings in your system to handle text. You may be wondering why text handling is so painful at times. Why are there so many encodings and why don\u0026rsquo;t they all work together in harmony? I\u0026rsquo;ll attempt to explain a bit of history behind text handling in computers. Understanding this history should shed some light on why text handling can be so painful.\nTo make things interesting, let\u0026rsquo;s pretend we are inventing how computers will handle text. Also assume we live in the United States and speak only English. That\u0026rsquo;s a pretty ignorant assumption for real world software development, but it simplifies our process.\nASCII: Works great (if you want to ignore most of the world) Our challenge is to invent how computers handle text. Morse code is an encoding that pre-dates digital computers but provides a model for our approach: Each character has a transmission sequence of dots and dashes to represent it. We\u0026rsquo;ll need to make a few changes and additions though\u0026hellip;\n   Image sourceRather than dots and dashes we can use 1\u0026rsquo;s and 0\u0026rsquo;s (binary). Let\u0026rsquo;s also use a consistent number of bits per character so that it\u0026rsquo;s easy to know when one character ends and another begins. To support US English we need to map a binary sequence to each of the following:\n a-z A-Z 0-9 \u0026quot; \u0026ldquo;(space) !\u0026quot;#$%\u0026amp;'()*+,-./:;\u0026lt;=\u0026gt;?@[\\]^_`{|}~ Control characters like \u0026ldquo;ring a bell\u0026rdquo;, \u0026ldquo;make a new line\u0026rdquo;, etc.  That\u0026rsquo;s 96 printable characters and some control characters for a total of 128 characters. 128 is 27, so we can send these characters in seven-bit sequences. Since computers use eight-bit bytes, let\u0026rsquo;s decide to send eight bits per character but ignore the last bit. We have just invented the ASCII encoding!\nASCII forms the root influence of many text encodings still used today. In fact, at one time ASCII was the law: U.S. President Lyndon B. Johnson mandatedthat all computers purchased by the United States federal government support ASCII in 1968.\n   Image sourceInternational and OEM standards: Supporting other languages Starting with similar languages to US English We need more space to pack in more symbols if we want to support other languages and other symbols like currencies. It seems reasonable that people typically deal with a block of languages that are geographically or politically related, and when we\u0026rsquo;re lucky those languages share many of the same symbols. Given that assumption we can create several standards; each one for a block of languages!\nFor each block, we can keep the first 128 characters as-is from ASCII (identical bit sequences) so that the US English characters and Arabic numerals are still supported. We can then use the eighth bit for data instead of ignoring it. That would give us eight bits per character and a total of 256 characters to work with (double ASCII\u0026rsquo;s paltry 128). Now let\u0026rsquo;s apply that eight bit.\nA bunch of countries in Western Europe use the same latin alphabet plus special diacritics (also known as accent marks) like ü or é or ß. In fact, we can pack enough extra characters in those last 128 slots to support 29 other languages like Afrikaans, German, Swahili and Icelandic. Our Western European language block encoding is ready! We call this type of encoding a single-byte encoding because every character is represented by exactly one byte.\n   Image sourceAdditional single byte encodings for other language blocks We can repeat the same process we used to create our Western European language encoding to develop other single-byte encodings for other language blocks; each a 256 character set! To give one more example, let\u0026rsquo;s build a single byte coding for Arabic.\nAgain, we take the first 128 ASCII characters as-is, then fill up the last 128 with the Arabic alphabet We\u0026rsquo;ve got some space left over. Arabic has some diacritics as well, so let\u0026rsquo;s use some of the leftover slots to hold diacritic marks that are only valid when combined with other letters.\nSome languages don\u0026rsquo;t even fit in 256 characters. Chinese, Japanese and Korean for example. That\u0026rsquo;s OK, we\u0026rsquo;ll just use multiple bytes per character to get more room. As you may have guessed, these encodings are called multibyte encodings. Sometimes we choose to use the same number of bytes for every character (fixed widthmultibyte encodings) and sometimes we might choose to use different byte lengths (variable widthmultibyte encodings) to save space.\nRatifying our encodings to standards After we\u0026rsquo;ve built several of these encodings (Russian, Greek, Simplified Chinese, etc.) we can ratify them as international standards such as ISO-8859 for single byte encodings. We previously built ISO-8895-1 (Western European) and ISO-8859-6 (Latin/Arabic). International standards for multibyte encodings exist too. People who use the same standard can communicate without problems.\nThe international standards like ISO-8895 are only part of the story. Companies like Microsoft and IBM created their own standards (so-called OEM standardsor code pages). Some OEM standards map to international standards, some almost-but-not-quite map (see Windows-1252) and some are completely different.\nOur standards have problems Our standards and code pages are better than ASCII but there are a number of problems remaining:\n How do we intermix different languages in the same document? What if our standards run out of room for new symbols? There is no rosetta stone to allow communication between systems that use different encodings.  Enter Unicode.\nUnicode: One standard to rule them all    Image sourceAs mentioned earlier, Unicode is a single standard supporting thousands of languages. Unicode addresses the limitations of byte encodings by operating at a higher level than simple byte representations of characters. The foundation of Unicode is an über list of symbols chosen by a multinational committee.\nUnicode keeps a gigantic numbered list of all the symbols of all the supported languages. The items in this list are called code pointsand are not concerned with bytes, how computers represent them, or what they look like on screen. They\u0026rsquo;re just numbered items, like:\naLATIN SMALL LETTER A - U+0061\n東Pinyin: dōng, Chaizi: shi,ba,ri - U+6771\n☃SNOWMAN - U+2603\nWe have virtually unlimited space to work with. The Unicode standards supports a maximum of 1,114,112 items. That is more than enough to express the world\u0026rsquo;s active written languages, some historical languages and miscellaneous symbols. Some of the slots are even undefined and left to the user to decide what they mean. These spaces have been used for wacky things like Klingon and Elvish.\nFun fact: Apple Inc. usesU+F8FF in the Private Use Area of Unicode for their logo symbol (). If you don\u0026rsquo;t see the Apple logo in parenthesis in the preceding sentence, then your system doesn\u0026rsquo;t agree with Apple\u0026rsquo;s use of U+F8FF.\nOK, we have our gigantic list of code points. All we need to do is devise an encoding scheme to encode unicode objects (which now we know are lists of code points) into byte-strings for transmission over the wire to other systems.\nUTF-8 UTF-8 encodes every Unicode code point in between one and four byte sequences. Here are some cool features of UTF-8:\n Popularity - It\u0026rsquo;s the dominant encoding of the world wide web since 2010. Simplicity - No need to transmit byte order information or worry about endianness in transmissions. Backwards compatibility - The first 128 byte sequences are identical to ASCII.  UCS-2: Old and busted UCS-2 is a fixed width, two-byte encoding. In the mid-nineties, Unicode added code points that cannot be expressed in the two-byte system. Thus, UCS-2 is deprecated in favor of UTF-16.\n UCS-2 was the original Java String class\u0026rsquo;s internal representation C Python 2 and 3 use UCS-2 if compiled with default options Microsoft Windows OS API used UCS-2 prior to Windows 2000  UTF-16: UCS-2++ UTF-16 extends UCS-2 by adding support for the code points that can\u0026rsquo;t be expressed in a two-byte system. You can find UTF-16 in:\n Windows 2000 and later\u0026rsquo;s OS API The Java String class .NET environment OS X and iOS\u0026rsquo;s NSString type  UTF-32: Large bytes, simple representation UTF-32 is a simple 1:1 mapping of code points to four-byte values. C Python uses UTF-32 for internal representation of Unicode if compiled with a certain flag.\nConclusion We\u0026rsquo;ve seen how text handling can go wrong. We\u0026rsquo;ve learned how to design and test our systems with Unicode in mind. Finally, we\u0026rsquo;ve learned a bit of history of text encodings. There is a lot more to the topic of text, but for now I ask you do to the following:\n Examine your system to see if you\u0026rsquo;re using Unicode inside Use UTF-8 when reading and writing data Know that the plain text is a lie!  Thanks for reading!\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/unicode/",
        "title": "Unicode",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/shipit-hackathon-mplus/",
        "title": "ShipIt - 24-hour hackathon for Millennium+ Platform Dev",
        "tags": ["culture", "engineering", "hackathon", "shipit"],
        "description": "",
        "content": "At the end of March, some of our teams held their first 24-hour hackathon, titled ShipIt: Millennium+ Services FedEx Day. We had 41 participants, in 15 teams working on 15 unique projects. The idea was inspired by several teams spending a few hours every so often to work on different projects. After reading about Atlassian’s hack days, we decided to hold one.\nThe event was initially announced early in February, to give teams time to work this into their project plans. The schedule was to start at 10 am on a Thursday and wrap-up at 10 AM on Friday. Teams then presented their awesome projects and then were free to leave for the weekend (and catch-up on some sleep). Each team was free to choose the project they wanted to work on, with the limitation added that they should work on something which can be deployed somewhere in 24 hours (there were bonus points involved for deployed projects). The winning prize not only included bragging rights, but also the ‘Golden Keyboard’, which will be a traveling trophy.\nBehold, the Golden Keyboard:\n   We had reserved a large room off campus to get everyone away from their daily routines. Teams immediately jumped into their projects as soon as the hack day started on March 27th. Plenty of food and snacks were on hand, with lunch and dinner delivered to keep everyone fed. A hackavision dashboard (Sinatra application which subscribed to a list of atom feeds) was created, to track all the github commits by the teams.\n   The projects had amazing breadth. These include Neo4j, Riemann, Capistrano and languages such as Ruby and Clojure. Teams not only learned new languages and projects in the 24 hours, but also had most of them fully functional and deployed at the end of the hackathon.\nThere were other activities as well, such as playing Xbox and watching the NCAA Basketball Tournament, which provided to be great breaks throughout the night. Motivational movies, like Robin Hood: Men in Tights, were also on tap through the night.\n   By 10 am on Friday morning on March 28th, everyone was ready to present their projects. We had four judges representing different areas of expertise. The demos were awesome and judges had a tough time picking the top three projects. Third place went to the SplunkOverflow team, who worked on a Maven plugin that would build site documentation for Thrift RPC services. Second place went to the Short Circuit team, who improved the performance of hash calculations in our Storm topologies. First place (and the Golden Keyboard) went to the Minions team, who created \u0026ldquo;lando\u0026rdquo;, a set of services that supported monitoring and management tasks on JVM-based Thrift RPC services.\nAll in all, it was an exciting 24 hours where teams showed their innovative abilities. All the projects were demo’ed to a larger audience about a week later. Most of the projects started on during ShipIt are being enhanced further, by the teams during their team level hack time or scheduled projects.\n   This was our first hackathon, but it won’t be our last! We will have at least one more later this year and plan on a recurring event, with the Golden Keyboard traveling around with the winning team. It was amazing to see what people can do in a short amount of time and with the flexibility of choosing what you want to work on, the end result will always be something cool. We hope to continue the innovative thinking, not only by team level hack days, but having larger hack days.\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/bigdata/",
        "title": "bigdata",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/micah-whitacre/",
        "title": "Micah Whitacre",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/scaling-people-with-apache-crunch/",
        "title": "Scaling People with Apache Crunch",
        "tags": ["design", "engineering", "bigdata"],
        "description": "",
        "content": "Starting the Big Data Journey When a company first starts to play with Big Data it typically involves a small team of engineers trying to solve a specific problem. The team decides to experiment with scalable technologies either due to outside guidance or research which makes it applicable to their problem. The team begins with the basics of Big Data spending time learning and prototyping. They learn about HDFS, flirt with HBase or other NoSQL, write the required WordCount example, and start to figure out how the technologies can fit their needs. The group\u0026rsquo;s immersion into Big Data deepens as they start to move beyond a prototype into a real product.\n   The company sees the success of using Big Data technologies and the possibilities to solve difficult problems, tackle new endeavors, and open new doors. The company has now shifted its scalability problem out from the technical architecture into a people problem. The small team of experts cannot satisfy the demand and transferring their accumulated knowledge to new teams is a significant investment. Both new and experienced engineers face a steep learning curve to get up to speed. Learning the API is not difficult but challenges typically oocur when applying the WordCount example to complex problems. Making the mental jump from processing homogeneous data which produce a single output to a complex processing pipeline involving heterogeneous inputs, joins, and multiple outputs is difficult for even a skilled engineer.\nCerner has developed a number of Big Data solutions each demonstrating the 3 V\u0026rsquo;s of data (variety, velocity, and volume). The complexity of the problems being solved, evolving functionality, and required integration across teams led Cerner to look beyond simple MapReduce. Cerner began to focus on how to construct a processing infrastructure that naturally aligned with the way the processing is described. Looking through the options available for processing pipelines including Hive, Pig, and Cascading, Cerner finally arrived at using Apache Crunch. Using Crunch\u0026rsquo;s concepts, we found that we were easily able to translate how we described a problem into concepts we can code. Additionally the API was well suited for our complicated data models and building integration contracts between teams.\nWhen we describe a problem we often talk about its flow through the various processing steps. The processing flow is comprised of data from multiple sources, several transformations, various joins, and finally the persistence of the data. Looking at an example problem of transforming raw data into a normalized object for downstream consumers we might encounter a problem similar to the diagram below.\n   If we apply this problem to the raw MapReduce framework we begin to see problems absent in the standard WordCount example. The heterogeneous data and models, the custom join logic, and follow up grouping by key all could result in extra code or difficulty fitting this processing into a single MapReduce job. When our problem expands to multiple MapReduce jobs we now have to write custom driver code or bring in another system like Oozie to chain the workflow together. Additionally while we could fit the steps neatly into 1-2 MapReduce jobs that careful orchestration and arrangement could become imbalanced as we introduce new processing needs into the workflow.\n   This problem is common and fairly basic with respect to some of the processing needs we faced at Cerner. For the experienced MapReduce developers this problem might cause a momentary pause to design it but that is due to your expertise. For those less skilled imagine being able to break this problem down into the processing steps you understand and POJO like models of which you are already familiar. Breaking this problem down using Apache Crunch we can see how we can articulate the problem and still take advantage of the processing efficiency of MapReduce.\nBuilding a Processing Pipeline with Apache Crunch Apache Crunch allows developers to construct complicated processing workflows into pipelines. Pipelines are directed acyclic graphs (DAG) comprised of input data that is then transformed through functions and groupings to produce output data. When a developer is done constructing the pipeline Apache Crunch will calculate the appropriate processing steps and submit the steps to the execution engine. In this example we will talk about using Apache Crunch in the context of MapReduce but it also supports running on Apache Spark. It should be noted that pipelines are lazily executed. This means that no work will be done until the pipeline is executed.\nTo begin processing we need a pipeline instance on which we will construct our DAG. To create a MRPipeline we need the typical Hadoop Configuration instance for the cluster and the driver class for the processing.\nPipeline pipeline = new MRPipeline(Driver.class, conf); With a pipeline instance available the next step is to describe the inputs to the processing using at least one Crunch Source. A pipeline must contain at least one source but could read from multiple. Apache Crunch provides implementations for the typical inputs such as Sequence Files, HFiles, Parquet, Avro, HBase, and Text. As an example if we were to read data out of a text file we might write code like the following:\nPType\u0026lt;String\u0026gt; ptype = Avros.strings(); PCollection\u0026lt;String\u0026gt; refDataStrings = pipeline.read(new TextFileSource(path, ptype)); This code utilizes the TextFileSource to generate a collection of Java Strings from files at a certain path. The code also introduces two additional Apache Crunch concepts of PCollections and PTypes. A PCollection represents potential data elements to process. Since a pipeline is lazily executed it is not a physical representation of all of the elements. A PCollection cannot be created but can be read or transformed. Apache Crunch also has special forms of PCollections, PTable and PGroupedTable, which are useful in performing join operations on the data. A PType is a concept that hides serialization and deserialization from pipeline developers. In this example the developer is using native Java strings instead of dealing with wrapper classes like Writable\u0026rsquo;s Text class.\nProcessing based off of Java Strings is error prone so typically developers would transform the data into a model object that is easier to work with. Transformation of a PCollection is done through a DoFn. A DoFn processes a single element of the PCollection into zero or many alternate forms depending on its logic. The bulk of a processing pipeline\u0026rsquo;s logic resides in implementations of DoFn. Custom implementations of a DoFn requires extending the DoFn class as well as defining the input and output types. This allows Crunch to provide compile time checking as transformations are applied to collections.\nclass ConvertReferenceDataFn extends DoFn\u0026lt;String, RefData\u0026gt;{ public void process (String input, Emitter\u0026lt;RefData\u0026gt; emitter) { RefData data = //processing logic;  emitter.emit(data); } } ... PType\u0026lt;String\u0026gt; ptype = Avros.strings(); PCollection\u0026lt;String\u0026gt; refDataStrings = pipeline.read(new TextFileSource(path, ptype)); PCollection\u0026lt;RefData\u0026gt; refData = refStrings.parallelDo(new ConvertReferenceDataFn(), Avros.records(RefData.class)); Recalling the previous example processing problem we see that we need to perform join and grouping operations based on a key. Instead of converting the strings into a RefData object it would actually be better to convert the string into a key/value pair (e.g. Pair\u0026lt;String, RefData\u0026gt;). Apache Crunch has a PTable\u0026lt;K, V\u0026gt;, which is simply a special form of PCollection\u0026lt;Pair\u0026lt;K, V\u0026raquo;. Adjusting the function we can instead produce the key/value pair.\nclass ConvertReferenceDataFn extends DoFn\u0026lt;String, Pair\u0026lt;String, RefData\u0026gt;\u0026gt;{ public void process (String input, Emitter\u0026lt;Pair\u0026lt;String, RefData\u0026gt;\u0026gt; emitter) { RefData data = //processing logic;  String id = //extract id;  emitter.emit(new Pair(id, data)); } } ... PType\u0026lt;String\u0026gt; ptype = Avros.strings(); PCollection\u0026lt;String\u0026gt; refDataStrings = pipeline.read(new TextFileSource(path, ptype)); PTable\u0026lt;String, RefData\u0026gt; refData = refStrings.parallelDo(new ConvertReferenceDataFn(), Avros.tableOf(Avros.strings(), Avros.records(RefData.class))); Utilizing the PTable\u0026lt;String, RefData\u0026gt; collection we could then join that collection with another similarly keyed PTable using one of the many prebuilt implementations. The built in join functionality helps to avoid developing custom implementations of a common data processing pattern.\nMore functions are applied to the joined data to continue the processing workflow. The processing of the data is distributed in separate tasks across the cluster. In the example problem we need all of the data for a given key to be grouped to a single task for processing.\nPTable\u0026lt;String, Model\u0026gt; data = ...; PGroupedTable\u0026lt;String, Model\u0026gt; groupedModels = data.groupByKey(); A pipeline requires at least one collection of data to be persisted to a target. Crunch provides the standard targets for data but consumer can also easily create new custom inputs.\n//persist Avro models  pipeline.write(models, new AvroFileTarget(path)); When constructing the processing pipline for the example problem we would end up with an executable program that looks like the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69  public void run(){ Pipeline pipeline = new MRPipeline(Driver.class, conf); //Read data from sources  PType\u0026lt;String\u0026gt; ptype = Avros.strings(); PTable\u0026lt;String, RefData\u0026gt; refDataStrings = pipeline.read(new TextFileSource(path1, ptype)); PTable\u0026lt;String, RefModel\u0026gt; refModelStrings = pipeline.read(new TextFileSource(path1, ptype)); //Convert the Strings into models  PTable\u0026lt;String, RefData\u0026gt; refData = refStrings.parallelDo(new ConvertReferenceDataFn(), Avros.tableOf(Avros.strings(), Avros.records(RefData.class))); PTable\u0026lt;String, RefModel\u0026gt; refModel = refStrings.parallelDo(new ConvertReferenceModelFn(), Avros.tableOf(Avros.strings(), Avros.records(RefModel.class))); //Data separate input together.  PTable\u0026lt;String, Pair\u0026lt;RefData, RefModel\u0026gt;\u0026gt; joinedDataModel = refModel.join(refData); //Apply a similar DoFn to convert the Pair\u0026lt;RefData, RefModel\u0026gt; into a single object  PTable\u0026lt;String, Model\u0026gt; models = ...; //Filter out data that is not useful  PTable\u0026lt;String, Model\u0026gt; filteredModels = models.filter(new FilterModelFn()); //Group the data by key to have all model instances with the same key in a single location  PGroupedTable\u0026lt;String, Model\u0026gt; groupedModels = filteredModels.groupByKey(); //Convert the grouped Models into a single model object if they share the same key.  PCollection\u0026lt;PersonModel\u0026gt; personModels = groupedModels.parallelDo(new ConvertPersonModelFn(), Avros.records(PersonModel.class)); //Write out the Person Model objects  pipeline.write(personModels, new AvroFileTarget(path)); //At this point the pipeline has been constructed but nothing executed.  //Therefore tell the pipeline to execute.  PipelineResult result = pipeline.done(); } class ConvertReferenceDataFn extends DoFn\u0026lt;String, Pair\u0026lt;String, RefData\u0026gt;\u0026gt;{ public void process (String input, Emitter\u0026lt;Pair\u0026lt;String, RefData\u0026gt;\u0026gt; emitter) { RefData data = //processing logic;  String id = //extract id;  emitter.emit(new Pair(id, data)); } } class ConvertReferenceModelFn extends DoFn\u0026lt;String, Pair\u0026lt;String, RefModel\u0026gt;\u0026gt;{ public void process (String input, Emitter\u0026lt;Pair\u0026lt;String, RefModel\u0026gt;\u0026gt; emitter) { RefModel model = //processing logic;  String id = //extract id;  emitter.emit(new Pair(id, model)); } } class FilterModelFn extends Filter\u0026lt;Pair\u0026lt;String, Model\u0026gt;\u0026gt;{ public boolean filter (Pair\u0026lt;String, Model\u0026gt; model) { boolean include = //logic to apply to the model  return include; } } class ConvertPersonModelFn extends MapFn\u0026lt;Pair\u0026lt;String, Iterable\u0026lt;Model\u0026gt;\u0026gt;\u0026gt;{ public PersonModel map (Pair\u0026lt;String, Iterable\u0026lt;Model\u0026gt;\u0026gt; models) { PersonModel model = //apply grouping logic to generate model from many items.  return model; } }   It is very easy to see how the processing steps and flow of the original diagram can be mapped to Apache Crunch concepts.\n   Executing the code as written above would cause Apache Crunch to calculate the execution graph to be spread over two MapReduce jobs.\n   In this case having a MapReduce job being reduce only is less than ideal but the team has been able to focus on correctness and functionality first. Focus can now be shifted on performance tuning or adjusting algorithms as appropriate. The solid foundation of functionality and the simplicity of the concepts allows developers to easily understand how the processing pipeline. The ease of understanding helps to allow the team to refactor and iterate with confidence.\nThis blog post is essentially my script for my North America ApacheCon 2014 presentation. Slides are available here.\nLinks  Crunch User Guide  "
    },
    {
        "uri": "https://engineering.cerner.com/authors/andy-gifford/",
        "title": "Andy Gifford",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/migrating-from-eclipse-3.x-to-eclipse-4.x-the-iaware-story/",
        "title": "Migrating from Eclipse 3.X to Eclipse 4.X - The iAware Story",
        "tags": ["engineering"],
        "description": "",
        "content": "This is the blog form of the talk Migrating from Eclipse 3.X to Eclipse 4.X - The iAware Story at EclipseCon 2014.\nThe iAware development team was formed in late 2007 and in a little under six months we developed our first solution, CareAware CriticalCare, a dashboard application written using the Eclipse RCP targeted for use in ICU settings. The goal of this application was to provide clinicians with a complete picture of the patient’s status and to do it in a manner that was contextually relevant; meaning that related information was presented together. Doing so allows them to make rapid and timely clinical decisions.\nIt was from this first solution that we began the process of building the software development platform. We\u0026rsquo;ve created a number of reusable and consumable assets that can simplify and speed up development; chief among them is our Application, Platform and Gadget frameworks. Our application framework manages the startup and initialization of applications built on the platform and allows those building solutions to define the layout (navigation bar and perspectives) of their running application. The platform framework provides management of the active application, contexts and navigation and is the connection point for gadgets to communicate with one another. The gadget framework is a wrapper for Eclipse views and provides a common set of operations and UI elements that provide a consistent look and feel across gadgets. It also handles context changes and user authorization for solutions.\nWe provide two different application types: Dashboard and Personalized. The dashboard application type is intended to be shown on large form factor displays, typically in patient rooms in an always-on operation mode. This type lacks personalization options, such as moving views around or adding or removing them as multiple users will be using the application and a consistent look needs to be maintained. The personalized application type is intended for multiple form factors, but it\u0026rsquo;s primary use case is for laptops and mobile workstations with each user signing into the application with their own credentials. Because of this, we allow users to customize their perspectives by moving, adding or removing views. They can also add and remove perspectives and set preferences such as refresh time.\n      At our core, we develop a reusable application platform built on top of the Eclipse Rich Client Platform to provide the ability to create targeted healthcare workflow applications with the goal of allowing other development teams to focus on solution specific development without worrying about the infrastructure. This means that our team can take on the responsibility and work effort of updating to new Eclipse versions as they become available without having to pass that cost down to those teams that build solutions on our platform.\nFor the first four years or so, we did all of this platform development on top of the 3.x framework. A little over two years ago, we began the uplift process to 4.x, starting first with a feasibility study using 4.1.1 while taking advantage of the compatibility layer. While we wanted to dive in and have a straight e4 application, we had invested time and energy on platform projects based on 3.x that we couldn\u0026rsquo;t afford to scrap, so we took it one step at a time. Additionally, our team and other teams at Cerner had solutions that Clients were using that expect them to continue to function as they had previously regardless of what we did under the hood.\nDuring this feasibility study, we found around 50 items that we needed to address. Some issues were things that we were doing that happened to work in 3.x but no longer did in 4.x. Some were bugs that we either found already logged or we logged to Eclipse. A sampling of things found includes:\n  One of the first things we found was that all views had a view menu whether there were items in it or not. After some checking, we found that there was a bug (https://bugs.eclipse.org/bugs/show_bug.cgi?id=319621) logged for this, and we worked with other contributors to come up with a solution. After it was determined that the correction wouldn\u0026rsquo;t make it to 4.1.1, we modified the renderer (org.eclipse.e4.ui.workbench.renderers.swt) to never show the view menu for our views since we didn\u0026rsquo;t use that functionality anyway.\n  Given the ability to save perspective layouts of our personalized applications, we quickly found that perspective saving was broken and identified a number of Eclipse bugs related to the problem. Again, because the issue wasn\u0026rsquo;t due to be fixed until 4.2, we did our own serialization of the perspective layout and saved that data off into our preference store. After uplifting to 4.2, we removed most of that code and instead use the perspective XML.\n  We also came across another issue that was already logged (https://bugs.eclipse.org/bugs/show_bug.cgi?id=356252) where a perspective will be closed when all of its parts are closed. To resolve this, we added the Cleanup add-on and implemented a patch that was posted to the logged bug.\n  Another issue we ran into was with menu ordering. We had two different plug-ins contributing menu items to the main application menu and both of them declared that they were to be shown after the file menu item; however, we wanted one, Personalization, to show before the other, Help. Once we realized this, it was an easy fix to switch the declaration of the Personalization item to say that it should be before the Help item. While this was an issue with our code, it highlights the passivity problems that we had to be concerned about.\n  Another menu related issue was a dynamic menu that we were building that lists all of the available views for a particular perspective only listed one element. After some investigating, we found that our contribution class was extending ContributionItem instead of CompoundContributionItem as suggested by the Eclipse wiki. We switched the class our contribution was extending and our menu was once again working as expected. Nonetheless we logged a bug (https://bugs.eclipse.org/bugs/show_bug.cgi?id=354190) with respect to the ContributionItem since it was working in 3.7.\n  A handful of issues that we encountered centered on the icons for our various views and how they weren\u0026rsquo;t being found. It was determined that the slash direction in the icon path was incorrect.\n  Another set of issues that we encountered centered on having new functionality that wasn\u0026rsquo;t desired, such as extra preference pages in our preferences dialog and extra menu items in our help menu. An evaluation of the dependencies that were added corrected these issues.\n  For a variety of reasons, detached views is a feature that we needed to remove from applications. In 3.x we used the IWorkbenchPreferenceConstants.ENABLE_DETACHED_VIEWS preference. However, this property isn\u0026rsquo;t supported in 4.x. Our workaround to this is to provide a custom implementation of the DnDAddon which takes away detached views altogether. We logged a bug for this situation: https://bugs.eclipse.org/bugs/show_bug.cgi?id=357289\n  We also found after uplifting that we had a number of jobs that started to fail sporadically due to authentication checks we had in place not having the necessary information. After investigating further, we found that some jobs that previously were executed after our users logged in were occurring before and as such threads in the job pool were being created without the correct subject in place and subsequent jobs would reuse these threads. We employed a two pronged approach to resolve this issue. We updated existing jobs to obtain the current subject on construction (from the access controller) of the job and then use the Subject.doAs call in the jobs run method. At the same time, we created an extension of the Job class that would do this for consumers.\n  After demonstrating that we could move to 4.x, we began the process of making use of the new functionality that was available and to remove as much of our dependency on the compatibility layer as possible. To do that we added and customized the application model, defined a custom renderer to represent our UI and began removing extension points and implementations of 3.x interfaces in favor of dependency injection and behavioral annotations.\nWe utilize the application model both statically and dynamically within our solutions. In the static file we define commands and handlers for Exit and Show View, define the add-ons we\u0026rsquo;re including and specify the top level UI elements and corresponding renderer. The add-ons that we consume include:\n CommandServiceAddon ContextServiceAddon BindingServiceAddon CommandProcessingAddon ContextProcessingAddon BindingProcessingAddon Customized version of CleanupAddon - keep perspectives open when all parts have been closed Customized version of DnDAddon - disables detached views and forces the drop target to be a perspective Customized version of MinMaxAddon - removed the minimize button from views     The remaining UI elements, perspectives and parts, are contributed to the model dynamically through our application and gadget frameworks.\nWithin our application we have a couple of shared UI areas that reside outside of the perspective area and as such we needed to define a custom renderer factory for our container to achieve the same functionality that was found using IWorkbenchWindowConfigurer#createPageComposite(org.eclipse.swt.widgets.Composite) in 3.x.\nOur renderer factory also removes the QuickAccess search field, a piece of functionality we don\u0026rsquo;t want to include in our applications\nThe following diagram represents our UI model:\n   The top pane, generally holds our navigation bar and toolbar contributions including lock and refresh. The bottom pane, generally holds our notification tray and status line UI elements. The iAware teams solutions don\u0026rsquo;t currently make use of the left or right pane in any of our solutions but leave those options open to others building their own solutions.\nThe final piece of the uplift was to go through our various platform projects that were either implementing 3.x interfaces or were relying on singletons or static access to PlatformUI and use injection and behavioral annoations. This included changing our part implementation to no longer be an IViewPart and instead have it use the behavioral annotations @PostConstruct and @Focus.\nWe also wired in a lifecycle hander to make use of @PostContextCreate and @ProcessAdditions across our registries (namely our perspective, gadget) instead of being tied to the calls from the WorkbenchAdvisor and WorkbenchWindowAdvisor.\nWe also began use of the @Execute and @CanExecute annotations with a feature that we added to our gadget framework that allows solutions to contribute toolbar buttons for their gadgets.\nOne annotation that we don\u0026rsquo;t make use of is the @Persist annotation as persistence is a feature that we avoid because we require users to start with a clean state each time they run the application.\nThat brings me to that last topic, where do we go from here. We\u0026rsquo;ve begun the evaluation of 4.3 and have the evaluation of 4.4 on our roadmap when it becomes available.\nSpecifically we\u0026rsquo;re working to bring in 4.3 before June in response to some changes to the rendering of menus that occurred between 4.1 and 4.2. We still define our menu items through extension points in a plugin.xml file and we\u0026rsquo;d created an abstract class that allows other solutions to change the default menu text that we provided (most wanted to change \u0026lsquo;iAware\u0026rsquo; to \u0026lsquo;File\u0026rsquo;). Our application class then used this abstract class and the setText method on the menu item to change the text; however, we found in 4.2 that menu items that come from plugin.xml couldn\u0026rsquo;t be changed in this manner. So, when we make the move to 4.3, we\u0026rsquo;ll also change our menu contributions to come from the application model instead of extension points.\nWe will also re-evaluate workarounds that we\u0026rsquo;ve added for earlier versions that are now fixed in the main line.\nThe final item is something we\u0026rsquo;ve already been working on for a little while know but are really going after hard this year is moving to P2. We released our last version using features and products and we\u0026rsquo;re continuing to play with how we can best leverage them to deliver our solutions to clients.\nThe introduction of Eclipse 4.x represented somewhat of a turning point for our team. While the process of uplifting was challenging at times, it was a great learning experience and it provided us the ability to enhance the functionality of the iAware platform, which was a huge benefit to our teams developing solutions. Integral to our ability to enhance the iAware platform was the fact that with 4.x we’re able to use native API where previously we wouldn’t have been able to accomplish it or it required us to use a workaround, usually entailing use of internal classes. The work also lead to more involvement and participation in the Eclipse community by our team. We were involved in discussions in the forums, logged bugs and provided patches, which is a positive for all involved.\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/sponsoring-the-apache-software-foundation/",
        "title": "Sponsoring the Apache Software Foundation",
        "tags": ["culture", "engineering"],
        "description": "",
        "content": "Open source plays an integral role within engineering at Cerner. In addition to using open source software throughout our architecture, we recently released a few projects back to the community via our Github organization.\nToday, we’re happy to announce that Cerner is now a sponsor of the non-profit Apache Software Foundation (ASF). The ASF is home to several projects that are essential components in many of our systems. We’ve blogged previously about several of these projects: Hadoop, HBase, Crunch, Maven, and Storm. In addition to these projects, there are dozens of other ASF projects that we use and have contributed to via patches and enhancements.\nSponsoring the ASF allows us to show support for their mission and to ensure the ASF can continue to provide infrastructure and resources for the open source projects they host.\n"
    },
    {
        "uri": "https://engineering.cerner.com/tags/big-data/",
        "title": "big data",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/david-edwards/",
        "title": "David Edwards",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/the-raft-protocol-a-better-paxos/",
        "title": "The Raft Protocol: A Better Paxos?",
        "tags": ["big data", "engineering"],
        "description": "",
        "content": "Among the many compelling talks that attendees come to expect every year at the Strange Loop conference was a session given by Ben Johnson that provided an overview of a new distributed consensus protocol originating from research at Stanford University, named Raft.\nWhat is distributed consensus? Distributed consensus can be described as the act of reaching agreement among a collection of machines cooperating to solve a problem. With the rise of open source distributed computing and storage platforms, consensus algorithms have become essential tools for replication, and thus, serve to enhance resiliency by eliminating single points of failure.\nExamples of distributed consensus in action can often be elusive because such protocols are ordinarily buried inside core systems, and consequently, are largely invisible to application developers. For example, a relational database in a clustered configuration would typically employ a consensus algorithm to coordinate commits with other replicas. And similarly, Apache ZooKeeper, a popular distributed synchronization service used in projects such as HBase and Solr, utilizes a consensus protocol to achieve fault-tolerance by replicating its configuration repository across many servers.\nRaft is about understandability and practicality The current Raft paper argues that while Paxos has historically dominated both academic and commercial discourse with respect to distributed consensus, the protocol itself is too complicated to reason about and that a more understandable algorithm was needed, not only for educational purposes, but also to serve as a foundation for building practical systems.\nAn obvious question instinctively arises for the inquisitive reader: what makes Raft better than Paxos? Having personally implemented a replicated log using the Paxos algorithm, there was a natural curiosity in understanding how Raft approached the problem of solving consensus. It is worth noting, however, that comparing Raft and Paxos can be a bit misleading. Even though both address the fundamental problem of reaching consensus among a network of connected machines, Paxos is more academic in nature and primarily concerned with the mechanics of consensus, whereas Raft is oriented around the practical challenges of implementing a replicated log.\nPaxos is about theory The seminal work done by Leslie Lamport in 1989 with the design of the Paxos protocol was an important step forward in establishing a theoretical foundation for achieving consensus in asynchronous distributed systems. His contributions were largely academic and centered around reaching agreement on a single value, thus relying on software engineers to translate these ideas into practical solutions, such as replicated databases, which must decide on many values. However, the actual requirements necessary to build a real system, including areas such as leader election, failure detection, and log management, are not present in the Paxos specification, yet add a degree of complexity that almost always significantly alters the original protocol. This is precisely where the Raft designers correctly argue that the absence of specificity leads to great difficulty in applying Paxos to real world problems. A subsequent paper by Lamport in 2001 does an excellent job of making the original protocol accessible to practitioners, and to some degree, proposes techniques for designing a replicated log, but it stops short of being prescriptive in the way that Raft does.\nYou say tomāto, I say tomäto Raft is unique in many ways compared to typical Paxos implementations, but despite those differences, both are grounded in a similar set of core principles. For example, Raft requires leader election to occur strictly before any new values can be appended to the log, whereas a Paxos implementation would make the election process an implicit outcome of reaching agreement. The Raft designers claim that doing so has the consequence of simplifying log management, particularly with respect to edge cases in which a succession of leadership changes can result in log discrepancies, but the tradeoff is that leader election in Raft is more complicated than its counterpart in Paxos.\nWhat both protocols acknowledge, though, is that leader election is imperative if systems want to ensure progress. The notion of progress simply means that a system eventually does something useful. An important discovery in 1985, called the FLP Impossibility Result, proved that consensus was impossible in asynchronous distributed systems with the presence of only one faulty process. The practical implication follows: a system that cannot reach consensus is a system that cannot make progress. To be clear, the finding did not state that consensus was unreachable, just that some executions cannot reach consensus in bounded time. As a consequence, leader election, combined with timeouts, is often used as a technique for eliminating a class of conditions under which reaching agreement could take an arbitrarily long period of time. Interestingly, the Paxos algorithm as originally described by Lamport, makes no guarantees about progress, so implementations are compelled to incorporate timeouts as a compensatory measure. Raft, on the other hand, is prescriptive about the use of timeouts.\nRaft wins on accessibility One especially interesting component of the Raft specification is the mechanism for coordinating changes to cluster membership. The protocol employs a novel approach in which joint consensus is reached using two overlapping majorities, i.e. quorums defined by both the old and new cluster configuration, thereby supporting dynamic elasticity without disruption to operations.\nThe emergence of Raft has clearly seen a positive embrace by the software development community as evidenced by nearly 40 open source implementations in a variety of different languages. Even though Paxos is beautifully elegant in describing the essence of distributed consensus, the absence of a comprehensive and prescriptive specification has rendered it inaccessible and notoriously difficult to implement in practical systems.\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/cerner-and-open-source/",
        "title": "Cerner and Open Source",
        "tags": ["culture", "engineering", "operations"],
        "description": "",
        "content": "(This post was written by Nathan Beyer, Bryan Baugher and Jacob Williams.)\nThe use of open source software has become nearly ubiquitous in contemporary software development and it is no different for us, here at Cerner. We have been using open source software, directly and indirectly, for decades. Over the past decade, we’ve grown in maturity both in our use of open source software as well as our participation in open source communities. Our associates have long been contributors to open source communities, including helping users, logging bugs and enhancements, and submitting patches. Cerner associates also spearheaded the development of the Java Reference Implementation of the Direct Project.\nRecently, we’ve decided to take another step in the open source journey by releasing complete projects on our Github organization. Although these projects seem small, they are a big step for us and just the beginning of what we hope to open up and share in the future. We hope you’ll check out these projects and participate in their development.\nProject: knife-tar Source: https://github.com/cerner/knife-tar\nKnife-tar is a Chef tool for uploading and downloading Chef components from a tar file. It can be used for creating backups of your chef-server or for uploading released Chef artifacts from a repository.\nProject: scrimp Source: https://github.com/cerner/scrimp\nScrimp is a tool for interactively testing Thrift services in a web browser. It’s meant to fill the same role that browser-based REST clients fill for web services. Given the IDL files for the services, it provides a UI to help construct requests, invoke services, and display formatted responses.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/nathan-beyer/",
        "title": "Nathan Beyer",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/tags/operations/",
        "title": "operations",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/cerner-tech-talks/",
        "title": "Cerner Tech Talks",
        "tags": ["culture", "engineering", "tech talks"],
        "description": "",
        "content": "We are always looking for ways to share knowledge and learn new things within engineering at Cerner. Whether that be through meetups, lunch \u0026amp; learns, conferences, or DevCon, we have a variety of outlets available to us.\nToday, we’re announcing a new program we recently launched: Cerner Tech Talks.\nCerner Tech Talks brings in great speakers for talks that would be of interest to engineers at Cerner. These talks will be held periodically and will vary widely in their content. Each talk will be recorded and available for viewing via our Cerner Engineering YouTube channel.\nLast week, we held our first tech talk, which you can view below. Robert Binder, an accomplished software testing expert, presented on model-driven development.\n  "
    },
    {
        "uri": "https://engineering.cerner.com/blog/software-intern-hackfest/",
        "title": "2013 Software Intern Hackfest",
        "tags": ["culture", "engineering"],
        "description": "",
        "content": "Providing opportunities for students to gain experience in software development and grow the skills necessary to excel in their careers after graduation is a top priority for Cerner Engineering. Our annual Software Intern Program gives students insight into the design, implementation, testing, deployment, and maintenance of large-scale software projects, which are often beyond the scope of the typical academic experience.\nIn 2013, we had a total of 224 interns across all business segments with 107 placed into the Software Intern Program to grow their experience as Software Engineers. Each intern was placed on an engineering team, paired with a dedicated mentor, and immediately began learning to use industry-standard tools and processes. They worked on real projects that will ultimately end up in the hands of our clients and play a part in transforming healthcare through the use of technology.\nThis summer, interns worked on a wide variety of projects spanning from Eclipse RCP applications, Web services, iPad applications, or working with Big Data in the cloud. They used Java, iOS, Ruby, .NET, and many other technologies to solve problems and create new solutions to ensure each intern learned something new while challenging them to take away a new set of skills at the end of their summer experience.\nBut it’s not all work - the summer schedule was full of opportunities to learn about Kansas City, Healthcare, network with other interns in the program, and have as much fun as possible. This year, we kicked off the internship program at a Sporting KC soccer game, including a private tailgating event. Interns also got a chance to experience the Tucker Leadership Lab, attend lunchtime tech-talks, participate in a CodeRetreat, attend our annual Developer’s Conference (DevCon), go to Worlds of Fun, and go on tours around Kansas City to learn more about the different parts of town and what makes this a great place to live and work.\nOne of the most fun events this past summer was the Software Intern Hackfest. An optional event, Interns formed teams of 2-4 and were given a week with the goal of:\n Dream it. Build it. Present it. Profit!!!  The teams were given a week to build anything they wanted and presented the end result to a panel of Cerner software developers for prizes and glory. We took some time to sit down with two of the winning teams to get their thoughts on the Hackfest and their overall experience with the internship program.\nGrand Prize winners: Team Golf Addison Shaw, Richard Habeeb\nCerner: Describe your project and how you created it.\nA\u0026amp;R: When we came up with the project, we thought, in one week, how far could we push ourselves? Let\u0026rsquo;s do a game. We wanted a challenge.\nWe chose JavaScript because neither of us had much experience with it previously. JavaScript is really cool because it has a lot of game frameworks already. We found KineticJS - it mainly handled the html 5 canvas. It lets you do simple animations, rotations, and scaling.\nWhen we were coming up with the design of the game, it was more of a \u0026ldquo;create as you go\u0026rdquo; project. We didn\u0026rsquo;t just sit down and immediately come up with a game of aliens, cows and robots. We started out with a framework.\nWe wanted the game to be dynamically generated, and we wanted it to be a traditional survival game.\nWe knew it had to be 2-d. We built the framework with a grid structure. We made all the graphics ourselves with a pixelater.\nCerner: What were your biggest takeaways?\nAddison: I learned a lot of JavaScript and CSS Less along the way. I\u0026rsquo;m a lot more interested in web development now. I also learned that it\u0026rsquo;s okay to go well into the night to finish a project, and it relates to the real-life work environment. Knowing that we were competing against other teams and having the hard stop was motivation. We knew it didn\u0026rsquo;t have to be perfect; we just had to finish.\nRichard: I called it free motivation. It\u0026rsquo;s a competition, and there\u0026rsquo;s that side of me that wants to do really well. There\u0026rsquo;s also a deadline. It\u0026rsquo;s a chance to show off your work to others. The project was very self-directed. We came up with the tools and decided it ourselves. The creativity part was awesome.\nCerner: What was the most fun?\nAddison: I had Richard over to my house one day. We bought a bunch of snack food and red bull and worked on the game. My mom got us a pizza and it was really fun! We also did some trash talking with other interns. That was fun.\nCerner: What will you do with your prize? (Raspberry Pi)\nAddison: It was an awesome prize! I want to do something with home automation. We are living in a house [at college] this year. Maybe I’ll create something like clap on lights.\nRichard: I do robotics a lot during the school year and I was thinking of working with it at our club.\nCrowd Favorite: Mad Scientists Alex Valentine, George Li, Jack Miles\nCerner: Describe your project and how you created it.\nAlex: It was my idea to make a game. For the longest time, we didn’t know what we wanted to do. Originally we were going to make a 2-D game. We thought it would be easy not making that 3rd dimension. There is a game that puts 2D and 3D together. It looked really simple and easy. So we went with that.\nJack: A few of us weren’t familiar with C#.\nGeorge: We all liked to play video games! We used Unity to create it. Someone told me if you want to learn Git you should forget everything about SVN. That didn’t necessarily happen for me.\nCerner: How did you communicate the tasks that needed to be done?\nAlex: We assigned people different aspects of the game. I took characters.\nGeorge: I took responsibilities for weapons.\nJack: I took charge of enemies.\nCerner: How did you deal with integration?\nJack: For the longest time Git was giving me problems. I was on a Mac and they were on PCs.\nAlex: We learned that we should never commit directly to the master branch. Making mistakes and then figuring out how to fix them really helped me learn Git.\nGeorge: I learned more about Git than creating the game and ended up giving a lightning talk on Git. The Unity engine isn’t setup well for source control. We would have to diff in different stuff.\nCerner: You learned some new technologies – what else did you learn?\nAlex: Start early. Be efficient.\nJack: If you’re working on new technologies, learn each technology before going too deep. That way when we’re trying to do real work, you aren’t worrying about how to use it.\nGeorge: It was about Wednesday when we still didn’t have anything working. When he told me, \u0026ldquo;I’ll be happy if it shoots,\u0026rdquo; I knew we were in trouble. [Editor’s Note: At the competition, the main character was able to shoot, but only backwards, which was a huge crowd pleaser]\nAlex: When you realize that what you were going to do and your deadline are not going to jive, what do you do? We decided we wanted one of everything instead of having multiples of everything. We needed to scale our project down a bit.\nCerner: What was the most fun?\nAlex: The lunch meetings were pretty entertaining!\nJack: The \u0026ldquo;fun\u0026rdquo; of frustration. The [other teams'] presentations were fun for me because, although we set the goal and didn’t quite make it, you could tell there were a lot of people in the same situation. Everyone who has worked on software has been in our position.\nLearn More For more information about the Cerner Summer Intern Program, visit Cerner Careers Opportunities for College Students.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/frank-rydzewski/",
        "title": "Frank Rydzewski",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/authors/chris-finn/",
        "title": "Chris Finn",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/project-lead-the-way/",
        "title": "Project Lead the Way",
        "tags": ["culture", "engineering"],
        "description": "",
        "content": "Improving the state of healthcare through innovation requires investing in others to join you on the journey; not just for today, but for the decades to come.\n   Project Lead the Way has established the Computer Science and Software Enginering course that teaches computational thinking to high school students, and it will pilot in 60 schools across the country this fall. Providing exposure to a wide variety of computational and computer science concepts, students can program a story or game in Scratch, write a mobile application for Android, and learn about knowledge discovery and data mining, computer simulation, cybersecurity, GUI programming, web development, version control, and agile software development.\nRecently, I had the privilege of working with experts across the country to define the curriculum for Project Lead the Way as well as the opportunity to sit in on the training sessions given to the pilot teachers, hosted at Cerner\u0026rsquo;s Innovation Campus. Bennett Brown, Director of Curriculum and Instruction, led the class into the deep end to solve the hard problems their students would face throughout the course. Experts augmented the training sessions from organizations such as Cerner, Lawrence Livermore National Laboratory, Purdue University, Carnegie Mellon University, University of Virginia, and more.\nWhile the Computer Science and Software Engineering course (CSE) isn\u0026rsquo;t focused specifically on programming, students will pick up introductory skills in a number of languages and environments including Python, MIT App Inventor, JavaScript, Logo, and Scratch among others. As software developers, we all have our origin stories with what sparked our interest in programming; there was a particular moment for each of us where we solved a problem and saw firsthand the power of what software can create. The CSE course seeks to generate those sparks through activities, projects, and problem-based learning.\nThis course follows in the footsteps of other Project Lead the Way courses in engineering and biomedical science currently operating in over 4,700 schools with more than 10,000 trained teachers.\nWhat I Learned My role in the class was to teach one simple section and provide some practical answers from the industry trenches. I\u0026rsquo;ll let you in on a secret: I\u0026rsquo;m pretty sure I learned more about teaching and how to teach novices than I taught on any particular topic.\nProblems and projects in PLTW are designed to be approachable by all students; there is no ceiling for the advanced students who want to go beyond what is specifically covered. The teachers receiving training were given the same end-of-unit problems that their students will face during the year and had to work out solutions in a pair programming environment, often working in their dorms or in the Cerner training rooms until 10pm. The experience level of teachers ranged from those having engineering degrees and multiple years of industry experience before going into teaching to some who had never programmed in their lives before this class.\nWhat I learned was that even in this environment of mixed skill levels, the activity/project/problem-based approach surprisingly requires just a minimal introduction to a new concept. After simple activities which build quick wins, natural curiosity swells up within each of the students; skill and understanding start to form, not just learning a particular technology but how to problem solve using software.\nFor example, during the App Inventor sections, teachers received step-by-step simple examples of putting together programming blocks to create a simple Android application before working on homework to through one Agile sprint with their partner that evening to come up with their own application from scratch. The next morning, pairs went through their demos in a rotating show-and-tell fashion, showing a variety of apps including a flash card game, as well as a \u0026ldquo;one of these things is not like the other\u0026rdquo; game using the accelerometer for randomization and featuring the appropriate Sesame Street music.\nOne night during the course I pulled down the Codea app for my iPad and showed my own kids the relationship between a few behaviors in a physics app and the corresponding code in Lua. Three hours later, all of the iPads in the house were engaged in hacking.\nIn addition to some great instruction over the course from Bennett Brown on a variety of problems in genetics, chaos theory, Tkinter and GUI programming, other highlights included:\nVic Castillo, Group Leader in the Quantitative Risk Analysis at Lawrence Livermore National Laboratories with a PhD in computational physics gave a great three hour survey presentation on what he sees as the \u0026ldquo;big three\u0026rdquo; topics in the future of computing: computer simulation and modeling (a subject near and dear to my own origin stories in computing), 3-D printing and mobile robotics featuring almost a dozen demos of cases where modeling was used to learn something new about a real-world system or design. Using just the NetLogo multi-agent programmable modeling environment developed at Northwestern University, Vic showed a variety of models that could be built with just a basic knowledge of the Logo programming language: for instance, modeling the heat absorption of a home design and how different designs and window configurations affect the internal temperature of the living space), \u0026ldquo;solving\u0026rdquo; the game Lunar Lander, or how a First Robotics team modeled the software algorithms incorporated into their TurtleBot (think Roomba + netbook + Kinect sensor).\n   Joanne Cohoon talked about achieving diversity in STEM classrooms and the state of the job market\u0026ndash;how 50% of the jobs available involve some need for computational thinking and the challenges and how teachers can create classroom environments that attract students who might otherwise rule themselves out from the start based on the stereotypes and environments commonly surrounding STEM in schools. With half the workforce needing some kind of computational skill, we can\u0026rsquo;t drive anyone away.\nPeter Chapman, a PhD student at Carnegie Mellon University and Technical Leader of the picoCTF project joined us via Skype to describe the program. Set up along the storyline of an interactive adventure called Toaster Wars, picoCTF is a competition among high school students to solve as many of 57 challenges as they can using whatever means necessary\u0026ndash;hacking, decrypting, reverse engineering, breaking or whatever. The challenges force students to get their hands dirty, learning on their own via documentation and their browser the role of cookies in web security, how to use grep and tar to find a secret key in a file, and other examples designed to give students the experience of tackling unknown problems using the resources available to them.\n   Cerner interns and former Lee\u0026rsquo;s Summit High School Team Driven First Robotics team members Dakota Ewigman and Victoria Utter taught a condensed version of the KC Power Source Android App Camp using MIT App Inventor which set the stage for more advanced App Inventor instruction provided by Dave O\u0026rsquo;Larte around calling and integrating web services and open APIs. We had developed a simple ballot and question and answer database-backed web service that let students and teachers hack around independently. In the category of unintended learning, and no doubt inspired by their picoCTF hacking, some of the teachers quickly found out how to troll other schools' services by injecting funny questions and answers about the course material.\nWhy It\u0026rsquo;s Cool At one point during the week, Bennett Brown shared with me the growth potential for PLTW\u0026rsquo;s CSE program. In the engineering and biomedical engineering pathways, the year-over-year growth has been tremendous which is paving the way for an even faster uptake of the computer science offering. Such an important and widespread channel offers a great opportunity to influence the future of our entire industry and communities. With around 20% of the total pilot schools in this curriculum, Kansas City is getting a head start on building this pool of talent. We can use little advantages such as these to increase the value of KC as a hub for computing and entrepreneurial activity.\nAn instructor asked me at one point, \u0026ldquo;What is Cerner looking for in high school students who take this course?\u0026rdquo; I had the sense that the question sought an answer more about a specific technology or language. I don\u0026rsquo;t really see that as the focus. Instead, I think what we want is defined more by sparking that interest in solving hard problems and coming away with the understanding that these problems exist in their community and that there are people in the community at companies like Cerner solving those problems every day.\nWhy It Matters We have a lot to accomplish in \u0026ldquo;engineering health,\u0026rdquo; and K-12 outreach programs can feel like a long game that often take a back seat to immediate concerns around designing and shipping solutions. But seeing how even the less computing-savvy teachers started to get hooked on programming their Android devices or trying to get to the next level in picoCTF while beating their heads against man pages and HTTP dumps, it became clear to me that by getting involved in these programs we can accelerate not only the numbers of students comfortable with computational thinking, but also give them a network of relationships and experiences in their own communities to return to as their career and learning progresses. The long game may not be as long as we think.\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/devcon/",
        "title": "DevCon",
        "tags": ["culture", "engineering"],
        "description": "",
        "content": "This past June, 2,500 associates from across Cerner came together for DevCon, our internal developers conference. Now in its 3rd year, DevCon is a two-day, engineering-led conference that was created to bring together Cerner associates involved in all aspects of development and technology. DevCon is organized and run like many other developer conferences, complete with a call for papers and a talk selection committee. This year, we had 80 talks covering a wide array of topics such as big data, user experience and design, DevOps, and mobile development.\nA keynote presentation kicks off each day followed by talks across multiple tracks. Chris Brown, CTO of Opscode, the creators of the open-source Chef Platform, kicked off DevCon this year with a talk on the emergence and importance of DevOps. Within engineering at Cerner, embracing DevOps has had a big cultural impact on how we approach the software and systems we write and operate.\n  David Hogue, a leading Interaction Designer and instructor at San Francisco State University, kicked off the second day talking about the importance of interaction design in business and software. Hogue\u0026rsquo;s keynote was especially relevant to us as interaction designers play a prominent role in the creation and maintenance of our software.\n  As you can see from the two keynote videos, the visual theme for DevCon this year was 8-bit gaming. Two 1980’s living room setups, packed with vintage NES consoles, CRT televisions, and piles of classic games, provided entertainment and nostalgia during the breaks in the conference schedule. At night, 20 teams competed for bragging rights and awesome prizes at Geek Trivia Night, answering questions ranging from name the sci-fi spaceships to identify the programming language from a snippet of code.\nAs in previous years, DevCon ends with lightning talks in which anyone can present on a topic of their choosing provided it lasts for no more than 5 minutes. Lightning talks at DevCon can cover the gamut. Past lightning talks have covered Monads in Scala, a visual tour of Comic-Con, and world record strategies in Donkey Kong.\nWe\u0026rsquo;ve also made available the DevCon 2012 keynote presentations, both from Dr. Jeff Norris, a scientist at NASA JPL responsible for the robotic spacecraft in the solar system. Norris talked on remaining agile while working on mission critical systems as well as the importance of specialization in the advancement of fields \u0026ndash; both of these talks are well worth watching.\nDevCon allows engineers across organizations to come together to learn and present on a variety of topics, fostering a culture of innovation. It is no wonder then that many engineers at Cerner cite DevCon as their favorite and most anticipated event within Cerner.\n    "
    },
    {
        "uri": "https://engineering.cerner.com/blog/devacademy/",
        "title": "DevAcademy",
        "tags": ["culture", "engineering"],
        "description": "",
        "content": "When I graduated from college, I thought I understood what it meant to develop software in the real world. It required process. It required troubleshooting. It required quality. However, to me, process meant waterfall. Troubleshooting meant trying a few things and then asking for help. Quality meant manual testing.\nAgile methods were not unheard of when I graduated in 2001. My professors noted that iterative development was better than waterfall; they just only taught waterfall. Debuggers had been around since the 50’s, but my classmates and I still debugged with what I call \u0026ldquo;Hi Mom!\u0026rdquo; techniques. (We peppered our code with print statements.) Kent Beck had written the JUnit framework 4 years before, but it wasn’t entrenched in the Java culture yet. So it’s not surprising that my education didn’t cover these topics.\nIt took a few painful experiences in the real world to make me realize the way I programmed in college wasn’t the best way to engineer software. I needed to adopt some new practices.\nNot much has changed in terms of software education. Being a part of Cerner’s software engineer training program, I am able to ask every group of new engineers three questions:\n\u0026ldquo;Do you use an agile process?\u0026rdquo;\n\u0026ldquo;Do you use a debugger when troubleshooting your code?\u0026rdquo;\n\u0026ldquo;Do you write automated unit tests?\u0026rdquo;\nCerner has had explosive growth in engineering, so I’ve asked those questions of hundreds of recent graduates. Almost no one says yes. This told me that while colleges are doing a great job of teaching computer science, many schools are not teaching best practices in software development.\nUntil recently, Cerner wasn’t doing that great of a job of teaching them either. Our training program covered them, but we still saw the new engineers struggle to understand agile development, debug their code, and write their first unit test. One of Cerner’s core values is if you recognize something is broken, you are empowered to fix it. I knew our training program wasn’t working. It became my job to fix it.\nBefore tackling the whole program, I tried a little experiment. I wanted to see what it would take to get engineers in training to write just one unit test. At the time, training included a class on JUnit. In spite of the class, only 5% of the engineers were writing unit tests for training assignments.\nTo correct this, I started telling the engineers that I would take points off assignments that didn’t have a unit test. The idea was to create structural motivation. We immediately saw 40% of the engineers writing unit tests. A step forward, but it wasn’t enough.\nThe biggest obstacle to broader use of unit tests in training was that they didn’t know how to include the testing framework in their Java projects. That, more than the effort of writing the test, was keeping them from doing something we expected of them.\nSomething was wrong. We were teaching Maven in our training program. If you are not familiar with it, Maven helps you manage your project builds, and as a result, it helps you manage your dependencies. The engineers were already attending a class that taught them how to add dependencies to Java projects. They just weren’t able to associate what they had learned with the goal of bringing JUnit into their projects. They weren’t making the connection.\nThis connection was missed because engineers were learning about Maven in the absence of a problem. They were being told it’s an important tool, but we hadn’t given them a reason to use it. Later, when they did encounter the problem - \u0026ldquo;How do I add the JUnit jar to my project?\u0026rdquo; – it was too late. They had forgotten about Maven.\nThe key was to move the Maven training closer to when they needed the information. This is called \u0026ldquo;Just in Time Teaching.\u0026rdquo; It became the first requirement of the new program.\nAnother interesting aspect of my experiment is that 40% would write the tests even given the delay between training and practice. It should be obvious to anyone that’s ever taken a college programming class that some programmers can get it from lectures alone. Others have to practice. Any one-size-fits-all approach to training is flawed. The second requirement for the new program was that it must flex to meet the skills and learning styles of the engineers.\nWith these goals in mind, I started the redesign of Cerner’s training program. My first step was to interview a large sample of our software leaders. I asked them what they wanted engineers to learn. Time and time again, the top answers would be agile development, debugging, and automated unit tests. Surprisingly, it was not a list of technologies like iOS, Hadoop, JSON, or ReST.\nThe resumes of our newly hired engineers are full of languages and technologies. However, when asked what they wanted of new engineers, our lead architects described practices. If Cerner could get engineers to improve in practices, we could take the great engineers we were hiring and immediately make them more productive.\nThe scary thing is that sharing knowledge is easy, but changing people’s behavior is hard. Once I realized our problems were about software development behavior and not knowledge, I realized we would need to completely rebuild the way Cerner trains its new engineers. The result is DevAcademy.\n   Imagine you are a new engineer starting at Cerner. In your first week at Cerner, you report to the DevAcademy. The first two weeks focus on in-class instruction and assessment. The goal here is to introduce core software development behaviors and then assess your skills.\nAfter completing the first two weeks of instruction, you join what we call the DevCenter and are assigned a real project. However, that project isn’t assigned by your team. You get to pick. The projects come from all over Cerner including web applications and services, tools to make engineers more productive, and even contributing to open source projects used by Cerner. In picking a project, you are telling Cerner the types of work you find interesting. This helps us determine the best place for you across our diverse range of solutions and technologies.\n   While working on that first project, you have a dedicated mentor. You are expected to make progress on the project while receiving feedback. You also get just-in-time training on user stories, source code management, unit testing, and scrum. You get to use Cerner’s software development ecosystem in an isolated, safe environment.\n   Once you show readiness to join a team, you are allowed to demo your work to the teams that have open positions. Those teams can then pick the engineer that best fits their team. In this way, Cerner makes sure you are assigned to the right team for both Cerner and you.\nDevAcademy recognizes that you should never stop learning, so the program continues well into your first few months on your team. You are offered classes on different technologies and more advanced topics as part of an elective-based training plan. You work with your manager to decide which classes to take. It’s Cerner’s way of making sure all of our engineers continue to grow.\nWe’ve had 150 engineers join DevAcademy since it was launched. I’ve had the privilege of seeing the new engineers struggle and then succeed on their projects. I’ve seen the light come on when they realize the usefulness good development practices and apply them effectively. I’ve seen them get excited about git and other powerful tools that they didn’t have the opportunity to learn during their formal educations. The best part of my job is that I’ve seen many very good engineers start down the career-long path towards becoming really great ones.\n   "
    },
    {
        "uri": "https://engineering.cerner.com/blog/the-30-days-of-code-experiment/",
        "title": "The 30 Days of Code Experiment",
        "tags": ["culture", "engineering"],
        "description": "",
        "content": "In software development, we solve problems. As we solve these problems, we build connections in our minds of how to look at a problem, relate it to previous problems and solutions, and re-apply past approaches and techniques.\nThese behavior habits build dogmatic ways of thinking and limit design choices to selective technologies we’ve used in the past. As we all know, you have to continually learn new technologies and different ways of thinking to stay current in the ever-changing landscape of software development. Unfortunately, keeping up-to-date on technologies and approaches isn’t an easy behavior to maintain when you have many other priorities.\nThis spring, a few engineers wanted to bring focus to this important behavior trait by giving a tech talk on \u0026ldquo;Honing your Craft,\u0026rdquo; which discussed how to continuously strengthen and refine skills. Not only did we want engineers that we work with on a daily basis to be part of the tech talk; we wanted engineers from other teams at Cerner to be involved too. Also, we didn’t just want to talk about it; we wanted to pose a challenge to put people into action and re-enforce these behaviors.\nAt the end of the talk, we announced the challenge: 30 days of code. It was a challenge like many other \u0026ldquo;30 day\u0026rdquo; programs but was centered on learning new aspects of software development, languages, or just hacking up a tool that you find useful for your day-to-day development. The goal being that after 30 days, new habits and behaviors would be established to help promote continuous learning.\nTo make this a social learning experience, we built a Ruby web application called \u0026ldquo;mural\u0026rdquo; using async_sinatra and eventmachine to consume our GitHub Enterprise instance and show gists, which contained a code comment of \u0026ldquo;30_days_of_code\u0026rdquo;. The result was really interesting. Similar to Twitter, where you follow a specific hashtag, we were following code snippets of fellow developers. Since most of them were gists, they were small enough so you could easily see what they were doing (without having to go through a mountain of code). With this dashboard, a score was calculated based on count of your posts and was displayed with your avatar. Having scores displayed in descending order added a little peer pressure to keep people active in challenge.\n   By the time I got back to my desk, I received questions of where the URL was to see the app, so they could verify their posts were showing. Soon, people wanted GitHub repos to show up with gists, so a pull request was requested for that. We then wanted anonymous gists to also get pulled in, which required developing our own crawler since these were exposed on the gist search. It was apparent that people wanted to share what they were doing, and people wanted to see it.\nAt the end of the challenge, we returned to the auditorium to highlight some of the posts that came in over the past month. During the 30 days, we had 124 gists posted and 17 different contributions in repos. People were showing their skills over a wide range of technologies. Examples were:\n  Building a plugin to invoke Jenkins commands over a phone (using Siri)\n  Ruby scripts that interact with our Github Enterprise instance API that will send out emails when pull requests or branches are getting old (executed periodically through Jenkins)\n  Clojure application that looks for pull requests based on Github organizations, which have two \u0026ldquo;+1\u0026rdquo; comments (alerting which pull request may be candidates to close out)\n  Illustrating Crucible code interactions by extracting data with python and visualizing with D3 Using Node.js to flash lights on a Raspberry Pi when a health check from web service is failing\n  Presenting statistics from a storm cluster with Rickshaw\n  Eleven people presented what they worked on and learned. It was amazing to see all of the different ideas people came up with in this time frame.\nEven more interesting was how quickly people learned from the ideas of others. Not only were developers sharing their code snippets, but they were also sharing the problem they were attempting to solve or the idea of what they wanted invent. For example, the Ruby script which alerted the last committer of a dead branch through email, spawned into other implementations that would send alerts based on different pieces of Github data (ex. old pull requests). Sharing these ideas in their early stages (through code snippets) really accelerates the rate that an idea can be seeded in other minds and helps inspire even more innovation and learning.\nThis wasn\u0026rsquo;t only a challenge of what we would build, but it was also an experiment of what can happen by taking a small portion of your day and doing something different. By structuring this exercise around a formal challenge that included a competitive aspect, there was additional motivation to get involved and stay involved to the end.\nIn summary, find a way to take a little time out of your day to try something different; you will be amazed with the different perspectives that you gain.\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/thinking-in-mapreduce/",
        "title": "Thinking in MapReduce",
        "tags": ["big data", "design", "engineering"],
        "description": "",
        "content": "This is the blog form of the Thinking in MapReduce talk at StampedeCon 2013. I’ve linked to existing resources for some items discussed in the talk, but the structure and major points are here.\nWe programmers have had it pretty good over the years. In almost all cases, hardware scaled up faster than data size and complexity. Unfortunately, this is changing for many of us. Moore\u0026rsquo;s Law has taken on a new direction; we gain power with parallel processing rather than faster clock cycles. More importantly, the volume of data we need to work with has grown exponentially.\nTackling this problem is tremendously important in healthcare. At the most basic level, healthcare data is too often fragmented and incomplete: an individual’s medical data is spread across multiple systems for different venues of care. Such fragmentation means no one has the complete picture of a person’s health and that means decisions are made with incomplete information. One thing we’re doing at Cerner is securely bringing together this information to enable a number of improvements, ranging from better-informed decisions to understanding and improving the health of entire populations of people. This is only possible with data at huge scale.\nThis is also opening new opportunities; Peter Norvig shows in the Unreasonable Effectiveness of Data how simple models over many data points can perform better than complex models with fewer points. Our challenge is to apply this to some of the most complicated and most important data sets that exist.\nNew problems and new solutions Our first thought may be to tackle such problems using the proven, successful strategy of relational databases. This has lots of advantages, especially the ACID semantics that are easy to reason about and make strong guarantees about correctness. The downside is such guarantees require strong coordination between machines involved and in many cases the cost of that coordination grows as the square of data size. Such models should be used whenever they can, but to reason about huge data sets holistically means we have to consider different tradeoffs.\nSo we need new approaches for these problems. Some are clear upfront: as data becomes too large to scale up on single machines, we must scale out across many. Going further, we reach a point where we have too much data to move across a network \u0026ndash; so rather than moving data to our computation, we must move computation to data.\nIn fact, these simple assertions form the foundation of MapReduce: we move computation to data by running map functions across individual records without moving them over the network and merge and combine, or reduce, the output of those functions into a meaningful result. Word count is the prototypical example of this pattern in action. MapReduce implementations as offered by Hadoop actually offer a bit more than this, with the following phases:\n  Map \u0026ndash; transform or filter individual input records\n  Combine \u0026ndash; optional partial merge of map outputs in the mapping process, usually for efficiency\n  Shuffle and Sort \u0026ndash; Sort the output of map operations by an arbitrary key and partition map output across reducers\n  Reduce \u0026ndash; Process the shuffled map output in the sorted order, emitting our final result.\n     We have our building blocks: we can split data across many machines and apply simple functions against them. Hadoop and MapReduce support this pattern well. Now we need to answer two questions: How do we use these building blocks effectively and how do we create higher-level value on top of them?\nThe first step is to maximize parallelism. The most efficient MapReduce jobs shift as much work into the map phase as possible, even to the point where there is little or no data that needs to be sent across the network to the reducer. We can gauge the gains made by scaling out by applying Amdahl’s Law where the parallelism is the amount of work we can do in map tasks versus more serial reduce-side operations.\nThe second step is to compose our map, combine, shuffle, sort, and reduce primitives into higher-level operations. For example:\n  Join \u0026ndash; Send distinct inputs to map tasks, and combine them with a common key in the reducers.\n  Map-Side Join \u0026ndash; When one data set is much smaller than another, it may be more efficient to simply load it in each map task, eliminating the reduce phase overhead outright.\n  Aggregation \u0026ndash; Summarizes big data to be easily computed.\n  Loading into external systems \u0026ndash; The output of the above operations can be exported to dedicated tools like R to do further analysis.\n  Beyond that, the above operations can be composed into sophisticated process flows to take data from several complex sources, join it together, and distill it down into useful knowledge. The book MapReduce Design Patterns discusses all of these patterns and more.\nHigher-Level APIs Understanding the above patterns is important but much like how higher-level languages have grown dominant, higher-level libraries have replaced direct MapReduce jobs. At Cerner, we make extensive use of Apache Crunch for our processing infrastructure and of Apache Hive for querying data sitting in Hadoop.\nReasoning About the System Most of development history has focused on variations on Place-Oriented Programming, where we have data in objects or database rows and we apply change by updating our data in place. Yet such a model doesn’t align with MapReduce; when dealing with mass processing of very large data sets, the complexity and inefficiency involved in individual updates becomes overwhelming. The system would become too complicated to perform or reason about. The result is a simple axiom for processing pipelines: start with the questions you want to ask and then transform the data to answer them. Re-processing huge data sets at any time is what Hadoop does best and we can leverage that to view the world as pure functions of our data, rather than trying to juggle in-place updates.\nIn short, the MapReduce view of the world is a holistic function of your raw data. There are techniques for processing incremental change and persisting processing steps for efficiency but these are optimizations. Start by processing all data holistically and adjust from there.\nBeyond MapReduce The paper From Databases to Dataspaces discusses a new view of integrating and leveraging data. A similar idea has entered the lexicon under the label \u0026ldquo;Data Lake\u0026rdquo; but the principles align: securely bring structured and unstructured data together and apply massive computation to it at any time for any new need. Existing systems are good at efficiently executing known query paths but require a lot of up-front work, either by creating new data models or building out infrastructure for the immediate need. Conversely, Hadoop and MapReduce allow us to ask questions about our data in parallel at massive scale without prior build.\nThis becomes more powerful as Hadoop becomes a more general fabric for computation. Projects like Spark can be layered on top of Hadoop to significantly improve processing time for many jobs. SQL- and search-based systems allow faster interrogation of data directly in Hadoop to a wider set of users and domain-specific data models can be quickly computed for new needs.\nUltimately, the gap between the discovery of a novel question and our ability to answer it is shrinking dramatically. The rate of innovation is increasing.\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/first-robotics-championship-competition-in-st-louis/",
        "title": "FIRST Robotics Championship Competition in St. Louis",
        "tags": ["culture", "engineering"],
        "description": "",
        "content": "   Cerner places a high value on talent development programs offering students the experience to build practical and tangible skills for the modern work environment. As part of this focus, Cerner supports FIRST Robotics, a competition providing experience in software engineering, where students learn to deal with complexity, time constraints, quality, and technical communications. Sound familiar? I wish they had this program when I was a kid!\nHigh school students from Kansas City will be testing their minds, willpower, and teamwork in this global robotics championship competition April 24-27 in St Louis, Missouri. The secret game design was revealed in January, and with just six weeks to build, over 2,000 teams created completely unique robots. Design, engineering, metal fabrication, project management, marketing, and fundraising are all activities that students gain exposure to in this real-world project. Most importantly, they practice creative problem solving in complex team dynamics.\nThe championship teams you may have seen in the Kansas City Regional are:\n Team 1710 - The Ravonics Revolution Team 1730 - Team Driven Team 1806 - S.W.A.T. Team 1939 - The Kuh-nig-its Team 1986 - Team Titanium Team 1987 - Broncobots Team 3528 - Up Next!  Students are challenged with two software programming components: digital media marketing and robot controls. Students will use a wide range of web, mobile, and media development technologies to create their team\u0026rsquo;s marketing strategy.\nRobot controls is broken down into two types: autonomous and teleoperated programs. In autonomous mode the robot responds exclusively to preprogrammed commands based on sensor feedback from a camera, accelerometer, gyro, encoders, and more. In teleoperated mode, the robot continues to use sensors but now can receive input from the drive team using game joysticks.\nStudents can use three different programming languages for the robot control system:\n LabVIEW from National Instruments C++ with Wind River Workbench Java with Netbeans  The most important tool for control system programmers is a white board. Students diagram and visualize all the inputs and outputs of each system: motors, actuators, sensors, and driver station. Mapping inputs to outputs is important not just during the construction of the program, but it also helps to train the drive team. Students use online resources, collaborate with other teams, and receive guidance from their technical mentors. Practicing resourcefulness prepares them for the complex professional engineering environment they will soon become a member of.\nCerner is engaged in the Kansas City community at many levels. We are a sustaining partner in the KC STEM Alliance. Many Cerner associates mentor local teams, volunteer at local events, and are involved parents. These experiences exercise student minds in very real and practical ways. They are more prepared for technical and non-technical Cerner careers. Their passion and commitment is the fuel for delivering our future innovation.\nHere is this year’s gameplay video:\n  For more information, check out the links below:\n http://www.usfirst.org/roboticsprograms/first-championship http://www.kcfirst.org/ http://www.kcstem.org/ http://www.youtube.com/user/FRCTeamsGlobal http://wpilib.screenstepslive.com/s/3120  "
    },
    {
        "uri": "https://engineering.cerner.com/blog/learn-what-the-rules-dont-cover/",
        "title": "Learn what the rules don&#39;t cover",
        "tags": ["engineering", "operations"],
        "description": "",
        "content": "Most technical problems are like games. All of them have a way to win and all of them have rules; the easiest way to ensure you always win is to learn the rules inside and out, and more importantly what the rules don’t cover! Paying attention to what the rules don’t cover is what leads to out of the box thinking. What sets the great players apart from the rest is learning what the rules don’t cover which allows for creativity and, sometimes, shortcuts.\nRecently, I played a game similar to musical chairs. It was a game of diminishing resources (a management exercise) with about 12 of us and six large sheets of paper. The only rules were: 1) Walk around while the music is playing and 2) Put your foot on a rectangle when the music stopped (he instructor pointed at one of the sheets as a visual queue). When we first started, there was ample space for each of us to have a foot somewhere on one of the 2’ x 4’ paper.\nAs the game continued, the instructor started cutting the sheets into smaller and smaller pieces until we got down to 3\u0026quot; x 5\u0026quot;. Most of us were precariously trying to balance and keep a portion of a shoe touching the paper, a few just gave up, and two paid special attention to the rules and found an out. One gentleman took at a business card and promptly stood on it. Another just lifted his leg and put it up on the door. We all were following the same rules but two found what the rules didn’t cover and won.\nMost people think that Printing is easy. You hit a button or Ctrl + P and the paper comes out, right? This seamless process appears so to users because of the hard work that goes in behind the scenes. So when I started down this path, I knew a strong foundation would be important. I settled on CUPS as the basis for my brave new world of printing. It is open source and has a pretty wide following. It’s the basis for Apple printing, fairly hardened, and is the backbone of several educational systems printing making it enterprise ready.\nThe more I worked with it, the more I saw its full potential. CUPS had turned out to be the most useful \u0026ldquo;Swiss Army Knife\u0026rdquo; in my toolbox. It is extremely robust and easy to integrate / stack with other solutions. One great example of this is my \u0026ldquo;Coffee CUPS\u0026rdquo; demo.\n  I picked CUPS as the underlying backbone for my new architecture for the same reason. It had the maximum amount of possibilities with the least amount of rules. With a minimum amount of rules, it allows for the maximum amount of creativity. Sure, I could develop my own solution from the ground up where I get to make the rules and have unlimited creativity, and I have had to do that in the past. In general, if you write your own software, you have the least amount of rules (constraints of the compiler). OpenSource Software is a close second with few limitations. Closed source 3rd party software usually has the most rules (ever read a EULA in your life?). When selecting a solution to a problem, it’s also important to do a cost benefit analysis. Is it really worth reinventing the wheel? I’m often reminded of the below picture. I’ve seen several variations of it over the years (Credit to the picture unknown, but it wasn’t me).\nCUPS handles a lot of the general architecture that doesn’t need to be re-invented on either end of the spectrum (managing print queues, accepting jobs, sending jobs to printers, etc) but allows for a lot of creativity in the middle (the middle being what is done to the print job in between getting it from the user and sending it to the printer). A good example of this (filters) can be found here. CUPS gives you an overall framework of how it will call a filter, and what it expects as a return, but beyond that, it’s up to the programmer. You can write in pretty much any language you want and alter the print job as much as you want. I leveraged this to maximize the amount of devices I could talk to and input file types I could accept, while also being able to make business decisions based on the content of the print job.\n   Source: www.projectcartoon.com\nI think that picture aptly describes the disconnect in all of the processes that exist in problem solving. So in addition to learning what the rules don’t cover, we need to be keenly aware of what you are trying to accomplish: what is the problem and the success criteria? How do we win the game?\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/paul-conklin/",
        "title": "Paul Conklin",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/ruby-at-cerner/",
        "title": "Ruby at Cerner",
        "tags": ["engineering"],
        "description": "",
        "content": "Cerner\u0026rsquo;s journey with Ruby started in the summer of 2009. We selected Ruby on Rails for rapid development of the Cerner Store to prepare for release at the Cerner Health Conference that year. In three months, with three engineers and a designer, we wrote and released the first version of an e-commerce web application. Two of those engineers, including me, had never worked with Ruby before but quickly realized the power and expressiveness of the language due to resources like Why\u0026rsquo;s (Poignant) Guide to Ruby.\n   Our experience with the Cerner Store taught us that Rails led to high productivity. Ruby is a very natural language to write code in and principles like convention over configuration enabled us to solve our problems instead of spending time wrangling the framework. In addition, we valued the good practices of the Ruby community like easy-to-understand code and thorough unit testing with tools that aren\u0026rsquo;t painful.\nIn the summer of 2010, we attended the first Ruby Midwest as a team. We learned about developments in Ruby like JRuby and Chef as well as some of the great gems under development. The Cerner Store continued to grow and we learned about maintaining a Rails web app over time.\nIn early 2012, we were planning a massive undertaking to create a new platform for our Clients' healthcare data. It was to be called Millennium+ and it needed an architecture that could scale well to petabytes of data and dozens of engineers across many teams. We planned a service-oriented architecture and chose Rails to serve as the server side of the application. Our Rails services call JVM services that retrieve data from HBase and serves the resulting data as JSON to the client-side applications, including our iOS app, PowerChart Touch Ambulatory. The high productivity we enjoyed on a small team scaled well to a large team of people who had never written Ruby before.\nThis was the start of Cerner\u0026rsquo;s Ruby community. We developed reusable libraries and development processes that we continue to use today. The complexities of our architecture also led to the adoption of Chef to automate operations and build a devops mindset. Chef is another integral use of Ruby that penetrates teams that did not use Ruby at all.\nWhen planning our newest platform, Population Health, we needed to determine which platform made the most sense for a large-scale development of web applications on a tight timeline. We decided on Rails due in large part to its prioritization of convention over configuration, as well as the existing Ruby community at Cerner. These attributes would enable us to develop applications at the planned pace and scale. This decision is now disseminated to dozens of engineers working on brand new web applications and REST services powering them.\nWe\u0026rsquo;ve bolstered our internal Ruby community with lots of documentation and guidelines. We make use of the Ruby on Rails Tutorial book and Why\u0026rsquo;s (Poignant) Guide to train engineers in Ruby. Bozhidar Batsov\u0026rsquo;s Ruby style guide aligns closely with how we write our code, so we use rubocop to keep ourselves in line. Our development devices run on RVM and our servers run on Phusion Passenger.\nRuby has quickly become a very important part of Cerner\u0026rsquo;s engineering culture, primarily as the language behind Rails and Chef. An internal Ruby meetup has begun and there have been presentations involving Ruby at our annual Developer Conference.\n   Additionally, we are sponsoring Ruby Midwest because we want developers to know that Ruby is highly valued at Cerner. We\u0026rsquo;re also sending a large number of our own associates there to learn.\nWe look forward to bringing on more engineers to use Ruby and other technologies to engineer the future of healthcare.\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/modularity-in-medical-imaging/",
        "title": "Modularity in Medical Imaging",
        "tags": ["operations"],
        "description": "",
        "content": "Developers often take for granted the level of flexibility and customization that is available within the software they use every day. Consumers of imaging software have traditionally been completely confined to interpret exams a specific way, and frequently in ways that are unintuitive. Every physician, specialist, technologist, med student, and others across the continuum of care has a preference as to not only how exams are laid out, but what information is displayed with the images and what transformations would be applied or processed automatically. As it turns out, the only workflow they have in common is that they all view exams differently and they all want the experience to be clear and understandable.\nWe decided to take this to heart in the development of our viewing solution by allowing the application react to the user, opposed to the user reacting to the application and constraining their workflow. Working in the medical community can already be a high stress environment; introducing a piece of software intended to augment patient care that often times makes delivering care more difficult is not a viable solution. Finding the best way to address these issues starts with knowing the solution’s target audience. Before developers can help streamline a workflow, they first have to understand it. If you understand the user’s workflow and how they expect the solution to function, you can design for usability and they will want to use it, as opposed to feeling forced to use it. A key aspect of this is the reliability and simplicity of the application. If the user feels like the application is unstable or overly complex, they will immediately lose interest and view the whole experience as a chore, as opposed to something that can provide value.\n   The interface is the first application element the user encounters. If it feels intimidating and complex, before they even perform any functions, you have set a negative tone for your application and adoption will be low. In the case of imaging, Radiologists often spend more than 60 hours a week reading over 16,000 procedures per year, all in front of the same application. Imagine working with an application that causes frustration throughout your day. The first impression can propel an application in the marketplace or stifle it.\nWith the advent of open source software coming to the forefront, there are a variety of options for developing rich applications backed by a large community of contributors. One of those happens to be the Eclipse Rich Client Platform (RCP) and the corresponding Standard Widget Toolkit (SWT). RCP and SWT both provide a way for developers to rapidly develop a professional looking and stable application but are somewhat limiting when viewed from an imaging standpoint. Radiologists typically read on workstations that have three or more monitors and in environments that are dimly lit. Using an application that is primarily grey and white can not only cause eye strain for the user but it can be distracting from what the user is trying to accomplish.\n   Our team addressed this by starting out with the native components and then closely working with radiologists and visual designers to plan a user interface that was not only visually pleasing but also functional. The end result was a set of skinnable SWT widgets that are reusable within any SWT based application and can be specifically themed to match a desired color scheme for an environment.\n  )\nAnother aspect of imaging that we needed to plan for was the wide array of uses (diagnostic versus distribution). In a diagnostic scenario, radiologists will use the solution to perform diagnosis and will use the application within a multi-monitor high-resolution environment. For purposes of distribution imaging, clinicians will view images for reference and often times will view these on a much lower resolution single screen device (such as a laptop). While these two scenarios have vastly different hardware environments, users want the same experience across both and do not want to have to learn a new application depending on the workflow they are targeting at the time. As developers, we can address this by using service APIs instead of hard implementations and then create different assemblies that are easily accessible based on what the user is trying to accomplish. This not only helps address production concerns but allows developers to code against mock data sources and stores using the same API that they would within a fully scaled production environment.\nOnce we had the basics covered, we rethought how we approached the workflow, and sought to intentionally design for usability and responsiveness within the application. Users previously were constrained to grids and a limited amount of information while reading. With modular components designed to perform specific tasks and provide specific information, the user can now decide how they want to consume and view exams.\n         So now we have a solid application for use within diagnostic scenarios, but how do we give access to the enterprise without adding overhead? Following the path of reuse, we decided that we would use the main application that we already had. By starting with these components instead of rebuilding it for the browser or an entirely new framework, we could focus on how we could change the delivery of that application.\nWe needed a way to deliver the same high power application without requiring the high power hardware. Traditional IT solutions would use Citrix, VMWare, or some other type of commercially available virtualization solution. While these are all great solutions for certain scenarios, the requirement of plugin installation on the client device accessing the application the heavy graphics processing within an imaging solution does not play nicely in these environments. We can build on what these other solutions use (MS Remote Desktop Services) but choose to marshal the end result in an entirely different way.\nIf we want to deliver an application via a web browser, we have a limited selection of tools that we can depend on being available. IE has its own quirks and implementations, Chrome has some fancy extras on top of the WebKit rendering engine, and while Safari/Firefox use the same engine they still behave differently. Developing and deploying an application that must be focused on performance for the end user and still usable within a variety of environments has to take all of these constraints and challenges into consideration.\nFirst, we needed a way to communicate between the client and server. The HTML5 WebSocket API is great for this, but only has minimal support and user adoption. Socket.IO has a great framework that allows for WebSocket emulation (or their native use if available) that provides great performance. We decided to use this for our real-time web app and it has proven to be invaluable throughout our development process.\n   Now that we can communicate to the server what the user wants to do, we need a way to provide a representation of the images and UI that corresponds to the user input. The HTML5 Canvas API would seem like a good choice but continually redrawing canvas areas is process intensive and as this API isn’t supported within IE (the vast majority of our user base), it simply wasn’t an option. Instead, we decided to use a streaming mechanism, but couldn’t use the HTTP Streaming spec as this inherently introduces latency, which wouldn’t provide an acceptable user experience. All browsers support JPEG and PNG decompression and we can easily compress representations of the screen and shuttle these to the client. Even better, we can do this with a minimal amount of latency (we eliminate the need for key frames within an MPEG stream); we can choose which frames to drop and we can adjust the quality of each frame on the fly. This allows the client to adapt to the current network constraints and machine capabilities by tracking the effective FPS and adjusting the parameters on a sliding scale accordingly.\n   With all of these tiers in place, we now have an application in the browser that looks identical to the desktop deployment with plugins or installation required.\n   Beyond that, we have an SDK where third parties can build on top of our current deployment and provide even more specialized functionality within the imaging space, while still allowing users to customize their layout and how the data is delivered. We have now managed to deliver the same application to any device with a browser and our developers can develop from a single code line. This pleases our users and makes our developers’ lives easier. A true testament to how ease of development and a simple, yet powerful, user experience can exist in the same application.\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/cerner-and-tycho/",
        "title": "Cerner and Tycho",
        "tags": ["engineering", "operations"],
        "description": "",
        "content": "Introduction Tycho is a build tool for integration between PDE and Maven. Cerner has a long history of working with Eclipse RCP but P2 integration is something of a more recent phenomenon. This post is going to talk about how Cerner is using Tycho, what prompted our transition to Tycho and how we accomplished moving our build to using Tycho.\nWhy we use Tycho The first question when evaluating any tool is asking yourself: \u0026ldquo;why would I use that\u0026rdquo;? In our case, Cerner already has an extensive Maven ecosystem. For years it has been common practice, when developing Java, to use Maven as the go to build, site, and deployment tool. What we needed was a set of extensions that allowed us to use all the facets of Maven while creating PDE artifacts. Traditionally, the set of PDE artifacts that we needed to create and manage was limited to products, targets, and bundles. Using the Maven conventions, we could build out all of these PDE artifacts while allowing the Maven POM to be the single source of truth for most build projects. This worked well enough until we needed to rework our assemblies to be comprised of features, inserting another PDE artifact into the build lifecycle.\nWhen we first began RCP development, no tooling existed to help us with creating PDE artifacts through Maven, so we created our own. When the time came to start integrating additional PDE concepts, so that we could begin the path of integrating with P2 director, there was Tycho.\nHow we use Tycho We use tycho generally to build the following set of PDE artifacts:\n Products Features Mirrors + Targets Mirrors  While the mirrors are not technically Eclipse artifacts, we use Tycho to mirror the Eclipse (for now Juno) repositories so that our builds are not hammering the Eclipse update site. When you have 10\u0026rsquo;s to 100\u0026rsquo;s of users updating targets and running builds, it is just common courtesy to have your own mirror.\nFeatures We began our journey with Tycho by using it to generate P2 repositories from feature definitions. The bulk of the heavy lifting is accomplished through the use of the tycho-packaging-plugin (as well as the base plugin, tycho-maven-plugin). Since we deploy all of our plugins through maven, it is ideal to have Maven (Tycho in this case) do the dependency resolution for us and fill out the contents of our feature. It also does all the work of turning that feature into a p2 repository.\nThis was actually a nontrivial exercise for us. While we could have used a single feature definition, we tightly controll the dependencies that are pulled into our assembly. Therefore, as we looked to breakdown our feature definition it was more complicated than simply pulling in the Juno feature and heaping everything into a feature on top of that. Since we only use a subset of the Eclipse platform, we only want to pull forward the pieces we use explicitly (we also ran into some rather strange behavior with the Eclipse Jetty features). We wanted to define the features in such a way that consumers of our platform could select a single feature that represented our core platform + Eclipse (as well as layer in additional functionality related to building their application). In the end, we defined a set of about 20 features that rolled into a single working repository. This way, consumers only needed to know the location of a single repository and then could use the metadata in the features to determine which features they needed.\nProducts Starting at features made sense because there was no existing infrastructure for us to overhaul to build out the features. The products were a different story. With the products, we had an entire set of plugins that managed which version of Eclipse was pulled in for the product, how the .product was generated, and how we rolled that into an .exe.\nThe extra time we took in order to define the features thoroughly actually made this process significantly easier. Today, we build a set of 8 applications:\n CareAware Criticalcare Dashboard CareAware Criticalcare Personalized CareAware Infusion Management Dashboard CareAware Infusion Management Personalized  Each of these products also has a corresponding mock build (where we substitute in mock resources to populate data for our views). The definitions of these products was actually very simple and each ended up being around 8-10 features to create the product. With the aggregated repository, we had a single repository in each POM where the versions for the features were determined and filled out. Using the tycho-packaging-plugin and the tycho-p2-director plugin, we were able to automate the build of the corresponding p2 repositories which resulted from the Tycho builds.\nMirrors + Targets Being the kind citizens that we are, we didn\u0026rsquo;t want to abuse the Eclipse p2 release repository (also we didn\u0026rsquo;t want them to cut us off like Maven Central). To be good citizens in this build world, we decided we needed to mirror the Eclipse repositories ourselves. Luckily for us, Tycho developers had already anticipated that need! By using the tycho-p2-extras-plugin, we were able to easily automate the mirroring of the required features from the Eclipse release repository. Score (another) 1 for Tycho.\nAs part of the process for deploying these mirrors, we actually used the build-helper-plugin to deploy .target files to point to the mirrors into our maven repository.\nWhy not bundles? The all important question: Why didn\u0026rsquo;t we do bundles? The two biggest things stopping us from uplifting our bundles was the lack of an easy way to incorporate non bundle sources into builds and the difficulty in generating sites with code coverage for projects that use both Java and Groovy (a sad state of affairs I don\u0026rsquo;t recommend). In the end, these hurdles plus the amount of time it would take to uplift our own projects to match the expected build structure was a significant overhaul. We decided that, for now, the maven-bnd-plugin and m2eclipse can support our needs well enough for bundle development.\nFrom Custom Maven To Tycho So I\u0026rsquo;ve mentioned throughout this blog post about how we moved from custom maven to tycho; so what does that mean exactly? Back at the dawn of time (as I reckon it, others may know it as 2007), Cerner set down the path of doing Eclipse RCP development. At the time, there weren\u0026rsquo;t many good tools to automate your PDE builds. Until very recently, most of the Eclipse projects have used Ant as their build system. For our ecosystem and needs, it was Maven or bust. So we set about creating plugins that would help us at all levels of the build. Maven plugins were created for all of the following actions:\n Creating prodcuts Creating Manifests for bundles Running OSGi tests Generating OSGi diff reports (on the manifests) Accessing mirrored Eclipse instances Generating target platforms based on POM configuration  This turned out to be a rather extensive amount of work. The model worked relatively successfully until a consumer came along with a set of requirements that read an awful lot like: \u0026ldquo;we\u0026rsquo;d like P2 integration included if we move to the iAware platform\u0026rdquo;. This threw a wrench in our system, because while P2 can work (and does) work by simply using .product files to define content sets, we had no tooling to generate P2 repositories. We also had no mechanisms that would allow us to update only parts of their application (since this was 5 development teams working in tandem to produce a single assembly). When all of this converged, we realized our build system had become insufficient and it was time to explore alternatives. As you can see from the path we took, we started with the least intrusive changes and scaled up from there. As of this blog, iAware is preparing for the first release using features and products.\nHow do we deploy? Get ready for one ugly POM configuration:\n   If that isn\u0026rsquo;t enough to scare you, I don\u0026rsquo;t know what is. I wrote it and it terrifies me. What it really means though is that I haven\u0026rsquo;t found a great way for deploying p2 repositories as part of the maven build process. Options exist, such as deploying the zip file of the repository into the maven repo or writing shell scripts to automate the deployment, but neither of these sat well with me. So I went for the least distasteful choice and wrote some basic ant scripts :).\nAll in all, Tycho has become a large part of our development ecosystem. This blog only really touches the surface, but Tycho has become integral at all stages (besides bundle dev) to managing our assemblies. So, thank you Tycho contributors!\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/jonny-wright/",
        "title": "Jonny Wright",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/near-real-time-processing-over-hadoop-and-hbase/",
        "title": "Near Real-time Processing Over Hadoop and HBase",
        "tags": ["design", "engineering"],
        "description": "",
        "content": "From MapReduce to realtime This post covers much of the Near-Realtime Processing Over HBase talk I’m giving at ApacheCon NA 2013 in blog form. It also draws from the Hadoop, HBase, and Healthcare talk from StrataConf/Hadoop World 2012.\nThe first significant use of Hadoop at Cerner came in building search indexes for patient charts. While creation of simple search indexes is almost commoditized, we wanted a better experience based on clinical semantics. For instance, if a user searches for \u0026ldquo;heart disease\u0026rdquo; and a patient has \u0026ldquo;myocardial infarction\u0026rdquo; documented, that document should be highly ranked in the results.\nAnalyzing and semantically annotating can be computationally expensive, especially when building indexes that could grow into the billions. Algorithms in this space may be discussed in a future blog post, but for now we focus on creation of an infrastructure up to the computational demands. For this, Hadoop is a great fit. A search index is logically a function of a set of input data, and MapReduce allows us to apply such functions in parallel across an arbitrarily large data set.\n   A trend towards competing needs The above pattern is powerful but creates a nice problem to have: people want the output of the processing \u0026ndash; in this case, updates to search indexes \u0026ndash; faster. Since we cannot run a MapReduce job over our entire data set every millisecond, we encounter competing needs; the need to process all data holistically conflicts with the need to quickly apply incremental updates to that processing.\nThis difference may seem simple, but has deep implications. For instance:\n  With MapReduce we can move our computation to the data, but fast updates require moving data to computation.\n  MapReduce jobs produce output as a pure function of the input; realtime processing needs to handle outdated state. For instance, we build a phone book and a name changes from Smith to Jones, realtime processing must remove the outdated entry, whereas MapReduce simply rebuilds the whole phone book.\n  MapReduce jobs often assume a static set of complete data, whereas realtime processing may see partial data or new data introduce in an unexpected order.\n  And despite these differences, our processing output must be the identical; we need to apply the same logic across very different processing models.\nRealtime and batch layers These significant differences mean different processing infrastructures. Nathan Marz described this well in his How to Beat the CAP Theorem post. The result is a system that uses complementary technologies: stream-based processing with Storm and batch processing with Hadoop.\nInterestingly, HBase sits at a juncture between realtime and batch processing models. It offers aspects of batch processing; computation can be moved to the data via direct MapReduce support. It also supports realtime patterns with random access and fast\n   reads and writes. So our realtime and batch layers can be viewed like this:\n Data entering the system is persisted in HBase. MapReduce jobs are used to create artifacts useful to consumers at scale. Incremental updates are handled in realtime by processing updates to HBase in a Storm cluster, and are applied to the artifacts produced by MapReduce jobs.  Processing HBase updates in realtime So new data lands in HBase but how does Storm know to process it? There is precedent here. Google’s Percolator paper describes a technique for doing so over BigTable: it writes a notification entry to a column family whenever a row changes. Processing components scan for notifications and process them as they enter the system.\nThis is the general approach we have taken to initiate processing in Storm. Google’s Percolator strategy does not translate directly to HBase. Differences in the way regions are managed versus BigTable tables made using a different column family impractical. So we use a separate \u0026ldquo;notification\u0026rdquo; table to track changes to the original. Updates to HBase go through an API that writes notification entries as well as the data itself. We then wrote a specialized Storm spout that scans the notification table to initiate processing of updates.\nThe result is processing infrastructure like this, with Storm Spouts and bolts complementing conventional MapReduce processing:\n   The processed data model may be another set of HBase tables, a relational database, or some other data store. Its design should be centered on the needs of the applications and services, letting the processing infrastructure build data for those needs. It is important to note that MapReduce output should be done with a bulk load operation in order to avoid saturating the processed data store with individual updates.\nThis basic model turns out to be robust. Volume spikes from source systems can be spread throughout the HBase cluster. There are a couple key steps for success here:\n  Regular major compactions on the notification HBase tables are essential. Without major compactions, completed notifications will pile up and performance of the system will gradually degrade.\n  The notification tables themselves may be small in size, but should be aggressively split across the cluster. This spreads load to handle volume spikes and improve concurrency.\n  Also note that MapReduce is still an important part of the system. It’s simply a better tool for batch operations like bringing a new data set online or re-processing an existing data set with new logic.\nMeasure Everything There are a number of moving parts in this system, and good measurements are the best way to ensure it’s working well. For example, in development we found our HBase Region Servers would encounter frequent but short-lived process queues during heavy load. This didn’t look like an issue in HBase, but when we measured the performance of the calling process there was a noticeable degradation. The point is, instrumentation built into Hadoop and HBase are great but not sufficient. Measuring the observed performance at all layers is important to create an optimal system.\nThere are many good technologies for doing so. We generally use the Metrics API by Coda Hale. Here is an example of HBase client throughput using an instrumented implementation of HTableInterface. The data is collected by the Metrics API and displayed with Graphite:\n   Different models, same logic The same logic needs to be applied to both batch and stream processing despite the necessary differences in infrastructure. This is a challenge since the models speak very different languages: InputFormats describe an immutable and complete set of data, whereas event streams expose incremental changes without context.\nIt turns out the function is the only real commonality between them; simply taking a subset of input and returning useful output. So, our strategy is this:\nBuild all logic as a set of simple functions, then compose and coordinate those functions with higher-level processing libraries.\nWe use Storm to compose our realtime processing and Apache Crunch to compose our MapReduce jobs. Here are some lessons we have learned to apply this strategy effectively:\nMinimize intermediate state Persisting intermediate state can be expensive and creates complex relationships between moving parts. This is particularly true if a MapReduce job creates intermediate state used by realtime processing or vice versa. Instead, keep processing pipelines independent whenever possible and combine the results at the end.\nIsolate processing models Our MapReduce jobs are typically run on separate infrastructure than realtime processing to ensure expensive jobs do not saturate time-critical processing.\nBe aware of the semantic differences in the processing models A \u0026ldquo;join\u0026rdquo; in a MapReduce job sees all data, whereas a \u0026ldquo;join\u0026rdquo; in stream processing gets incremental subsets. If a function needs the full context to execute, that context must be externally loaded in the realtime processing system. In our case, external state is loaded from HBase and cached, but projects like Trident are now providing some aggregation facilities over storm as well.\nThe path forward The patterns here have been successful but require significant scaffolding and infrastructure to bring together. Near-realtime processing demands over big data are bound to increase, which means there is an opportunity here; higher level abstractions should emerge. Similar to how tools like Crunch and Hive offer abstractions over MapReduce, it’s likely that similar primitives can express the patterns described here.\nHow these higher abstractions emerge remains to be seen, but there is one thing I’m sure of: it’s going to be fun.\nAcknowledgments I’d like to acknowledge key contributors to building this and related systems: Jason Bray, Ben Brown, Robert Farr, Preston Koprivica, Swarnim Kulkarni, Kyle McGovern, Andrew Olson, Mike Richards, Micah Whitacre, Greg Whitsitt, and others.\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/evangelizing-user-experience/",
        "title": "Evangelizing User Experience",
        "tags": ["design"],
        "description": "",
        "content": "In the dark ages of development, great software meant packing in the functionality. People began doing more and more with their software. Updates meant newer and more exciting functionality. Sounds great, right? Of course it does, but something went horribly wrong. Slowly we became inundated with cluttered screens as software developers struggled to find a place to put their latest innovative functionality. Buttons began adding up and before we knew it, we were inventing user interface controls like ribbons to hold all the buttons.\nOn the bright side, things can only get better from here.__\n   Somewhere along the straight and narrow path to UI nirvana, we strayed. Distracted by the allure of \u0026ldquo;doing more,\u0026rdquo; we forgot to question why more needed to be done in the first place. We overlooked the importance of designing how something should be done because we were busy discovering new things to do. And, most importantly, we failed to include the user in the development process. Instead, we used inaccurate perceptions and engineering constraints to dictate how users should interact with our solutions.\nFortunately, software developers everywhere are beginning to see the light.\nWe are entering a new golden age of software solutions—one where the greatness of software is not measured by the number of functions, but by its ease of use. Usability is in the spotlight more now than ever before. Perhaps you\u0026rsquo;ve caught wind of some of these buzz words lately: Usability, User Experience, Interaction Design, Emotional Designer, and User-Centered Design. Generally speaking, they all point to the same thing: making software easier, more elegant, more intuitive, and (dare I say) more enjoyable to use.\nInteraction designers may be the instigators of software development’s Great Awakening, but user experience experts cannot do the job alone. It takes a great team to bring everything together and produce an exceptional product. Without user-focused engineering, great ideas and concepts would never come to life, regardless of their theoretical merit. If leadership is not committed to doing whatever it takes to ensure the users have an intuitive, enjoyable experience with our solutions, entire projects and initiatives would never see the light of day. On the other hand, having a team of leaders, designers, and engineers working to produce software that users will love, can produce incredible results.\nThe creators of Paper by FiftyThree, Apple’s iOS App of the Year, understand the importance of working together to produce an awesome experience. The results of their work speak for themselves. Watch this short video to catch a glimpse of their development culture and ideals:\n  We are about to make a big splash of our own in the world of iOS with the eminent release of a new iPad app for ambulatory physicians. Designing fast, smart, and easy workflows, and creating an elegantly robust and beautifully simple interface to go with them has helped foster the rapidly growing culture of flawless execution at Cerner. From the outset of the iOS initiative, user-centered design has been the main focus. We created simple and intuitive interactions for complex user processes. Then, when we thought we had it right, our team of user researchers ran our designs through extensive usability testing, which validated our concepts or helped us discover where we could improve. Based on their feedback, we tweaked the designs, and re-tested. We rinsed and repeated. We are confident that our clients will be pleased. Why? Because we’ve been talking to them throughout the entire process.\n   For Paper by FiftyThree, success means getting their users in touch with their creative side. At Cerner, producing amazing software solutions ultimately means improving the workflows of clinicians across the world, which impacts the health and wellbeing of countless individuals. We have a mission, and that mission is to make our solutions intuitive enough so that they simply fade into the background, allowing clinicians to focus on what is truly important: their patients.\n"
    },
    {
        "uri": "https://engineering.cerner.com/authors/charlie-huggard/",
        "title": "Charlie Huggard",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/blog/why-engineering-health/",
        "title": "Why Engineering Health?",
        "tags": ["culture"],
        "description": "",
        "content": "Hello and welcome to a public face for Cerner Engineering, by Cerner Engineering associates, to talk about engineering, technology, and all of the other awesome things we do.\nCerner has been recognized as a visionary company, transforming the delivery of healthcare around the world. Improving the health of individuals and the delivery of care is an extremely large, complex, ever-changing problem. Along with the efforts of our strategists and consultant organizations, solving this problem takes a ton of smart folks in our Engineering and CernerWorks Hosting organizations, who are free to play with, adopt, and embrace new technologies and ways of working.\nSpeaking of working differently, I’m currently at a coffee shop, writing this introduction post on my Cerner-issued Mac. I’ve completed a few code reviews for my team online using Crucible, and have been pushing minor tweaks to the code behind this website to our GitHub Enterprise instance. We collaborate on changes internally as well as directly with our clients using our Jive-powered uCern Connect site.\nBut it’s more than just how cool is my day\u0026hellip; Cerner understands the value in participating in a meaningful dialog about technology, sharing our discoveries, and learning from our peers in the industry. That’s why this summer our Engineering organization will be holding it’s 3rd annual DevCon, where all of our Engineering associates take two days away from their normal work to present and share their ideas and discoveries with each other. Cerner holds regular Hack Nights, where pizza and drinks are provided to get people together to try out new ideas and new technologies and solve things that may or may not apply to our usual work. Cerner also sends associates to attend and present not only at conferences around technologies we already use, such as ApacheCon, EclipseCon, PyCon, RailsConf and WWDC, but also at conferences around new technologies and ways of thinking like Compute Midwest, Strange Loop and SXSW.\nIn a similar vein, by launching this Engineering Health site, we hope to provide transparency to our internal organizations, as well create another way to connect to the broader community involved in solving big problems with software and technology. You will see that in addition to being a leader in the healthcare industry, Cerner is also at the forefront of software technologies. With our inaugural blog post from Ryan Brush, you will get a taste for some cool ways we’re putting big data processing methods to work for some of our new initiatives. Future blog posts will get into other technologies used, lessons learned, and just fun things direct and unfiltered from our Engineering organization.\nI hope you enjoy this site, and feel free to connect with us on Twitter @CernerEng.\n"
    },
    {
        "uri": "https://engineering.cerner.com/blog/composable-mapreduce-with-hadoop-and-crunch/",
        "title": "Composable MapReduce with Hadoop and Crunch",
        "tags": ["design", "engineering"],
        "description": "",
        "content": "Most developers know this pattern well: we design a set of schemas to represent our data, and then work with that data via a query language. This works great in most cases, but becomes a challenge as data sets grow to an arbitrary size and complexity. Data sets can become too large to query and update with conventional means.\nThese challenges often arise with Hadoop, simply because Hadoop is a popular tool to tackle such data sets. It\u0026rsquo;s tempting to apply our familiar patterns: design a data model in Hadoop and query it. Unfortunately, this breaks down for a couple of reasons:\n For a large and complex data set, no single data model can efficiently handle all queries against it. Even if such a model could be built, it would likely get bogged down in its own complexity and competing needs of different queries.  So how do we approach this? Let\u0026rsquo;s look to an aphorism familiar to long-term users of Hadoop:\n Start with the questions to be answered, then model the data to answer them.\n Related sets of applications and services tend to ask related questions. Applications doing interactive search queries against a medical record can use one data model, but detecting candidates for health management programs may need another. Both cases must have completely faithful representations of the original data.\nAnother challenge is leveraging common processing logic between these representations: there may be initial steps of data normalization and cleaning that are common to all needs, and other steps that are useful for some cases. One strategy is for each shared piece of logic to write output to its own data store, which can then be picked up by another job. Oversimplifying, it may look like this:\n   Such a model can be coordinated with Hadoop-based tools like Oozie. But this model of persisting every processing stage and using them downstream has some drawbacks:\n The structure and location of each intermediate state must be externally defined, creating a barrier to easily leverage the output of one operation in another. Data is duplicated unnecessarily. If one of our processing steps requires intermediate data, we must persist it, even if it is used only by other processing steps. Ideally, we could just connect the processing steps without unnecessary persistence. (In some cases, Hadoop itself persists intermediate state for MapReduce jobs, but there are many useful jobs that don\u0026rsquo;t need to do so.) Each step must be fully processed before the next step can run. Therefore, each processing step is only as fast as its slowest component. Each persistent state must use a data model that can be processed efficiently in bulk. This limits our options for the data store. We must choose something MapReduce friendly, rather than optimizing for the needs of application queries.  So how do we solve this? Rather than making intermediate data stores as the point of reuse, let\u0026rsquo;s reason about the system at a higher level: make abstract, distributed data collections our point of reuse for processing. A data collection is a set of data that can be persisted to an arbitrary store when it makes sense, or streamed between processing steps when no persistence is needed. One data collection can be converted to another by applying functions to it. So the above diagram may now look like this, where arrows are functions used to transform data collections:\n   This has several advantages:\n Processing logic becomes reusable. By writing functions that consume and produce collections of data, we can efficiently share processing code as needed. Data no longer needs to be stored for processing purposes. We can choose to store it when there is some other benefit. The data models need not account for downstream processing; they can align to the needs of apps and services. Processing is significantly more efficient as it can stream from one collection to another.  This model supports storing and launching processing from intermediate states, but it doesn\u0026rsquo;t require it. Processing downstream items from a raw data set will probably be a regular occurrence, but that need not be the case for other collections.\nPerhaps the biggest advantage of this approach is that it makes MapReduce pipelines composable. Logic expressed as functions can be reused and chained together as necessary to solve a problem at hand. Any intermediate state can optionally be persisted, either as a cache of items expensive to process or an artifact useful to applications.\nImplementation Strategy So, how does this work? Here are the key pieces:\n All processing input comes from a source. For Cerner, the source is typically data stored in Hadoop or HBase, but other implementations are possible. Each source is converted into a data collection, which is described above. One or more functions can be run against each data collection, converting it from one form to another. We\u0026rsquo;ll discuss below how this be very efficient. Collections can be persisted at any point in the processing pipeline. The persisted collections could be used for external tooling, or simply as a means to cache the results of an expensive computation.  Fortunately, a newer MapReduce framework supports this pattern well: Apache Crunch, based on Google\u0026rsquo;s FlumeJava paper, represents each data set as a sort of distributed collection, and allows them to be composed together with strongly-typed functions. The output of a Crunch pipeline may be a data model easily loaded into a RDBMs system, inverted index or queried via tools like Apache Hive.\nCrunch will also fuse steps in the processing pipeline whenever possible. This means we can chain our functions together, and they\u0026rsquo;ll automatically be run in the same process. This optimization is significant for many classes of jobs. And although persisting intermediate state must be done by hand today, it will likely be coming in a future version of Crunch itself.\nAn end to direct MapReduce jobs We may have reached the end of direct implementation of MapReduce jobs. Tools like Cascading and Apache Crunch offer excellent higher-level libraries, and Domain-Specific Languages like Hive and Pig allow the simple creation of queries or processing logic. Here at Cerner, we tend to use Crunch for pipeline processing and Hive for ad hoc queries of data in Hadoop.\nMapReduce is a powerful tool, but it may be best viewed as a building block. Composing functions across distributed collections that use MapReduce as its basis lets us reason and leverage our processing logic more effectively.\n"
    },
    {
        "uri": "https://engineering.cerner.com/culture/",
        "title": "",
        "tags": ["culture"],
        "description": "",
        "content": " Transforming Healthcare      We love solving the hard problems that no one else wants to tackle. We embrace polyglot programming, distributed systems, and open source. We also know that it's important to allow engineers to take a step back and try new things. That's why we host periodic hackathons, meetups, tech talks, and an annual internal two-day developers conference in addition to attending, sponsoring, and speaking at industry conferences.      At Cerner, we use the latest technologies to transform healthcare. We are the development and operations teams solving complex problems in the health care field to make you, your family, and friends healthy and happy.    Learn \u0026amp; Share   We attend a lot of technology conferences, and we also sponsor conferences to show Cerner's support for the industry. Cerner engineers are getting their talks accepted at conferences like Strange Loop, OSCON, ApacheCon, Midwest.io, and more. We feel so strongly about Cerner engineers speaking at conferences that if your talk is accepted at a conference, we'll pay your way to go!  Connect with Peers    DevCon DevCon is an internal, engineering-led conference that was created to bring together Cerner associates involved in all aspects of development and technology. DevCon is an opportunity for associates to learn, interact with distinguished leaders in their profession, and discuss significant topics to their specific role. The conference is intended to acknowledge the work associates do and see it as a craft, exciting other developers and engineers alike to connect and share ideas. Check out some of our session recordings on our Tech Talks page.      ShipIt ShipIt is our 24 hour hackathon we routinely host throughout the year in Kansas City, Malvern, Bangalore, and Brasov. We believe this encourages ingenuity and collaboration by promoting cross-pollination of ideas and teams as they solve problems together. Teams in KC compete for the coveted Golden Keyboard and Golden Mouse trophies that travel the campus, each winning team adding a trinket to the trophy representing their unique winning project. Learn more about our ShipIt events in our past blog posts.     Share the Love   Today, the core technology behind many of our architectures and projects is open source. Participation in open source - both as consumers and as contributors - is a key component of our Engineering Culture strategy. The software industry as a whole has proven time and again that some of the best ideas, most robust tools, and broadly used tools come from the bazaar. We find it important to not only consume and contribute to these projects, but to recognize the contributions of our associates and support organizations like the Apache Software Foundation that we benefit from.     Stretch yourself as a developer, learn new ways of approaching a problem, and get exposed to other languages and techniques.    Get Social      Cerner hosts a plethora of internal and external events for engineers to network and learn from their peers including coderetreats, hack nights, and meetups. We value the local tech community by sponsoring meetups, conferences, and hack events as well as encourage and recognize our associates that participate and/or lead those activities.    Connect with us! Tweets by CernerEng     "
    },
    {
        "uri": "https://engineering.cerner.com/images/",
        "title": "",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/open-source/",
        "title": "",
        "tags": [],
        "description": "",
        "content": " Cerner Open Source  At Cerner, we ❤️ open source. Open source software is an integral part of our software and is crucial to running and operating our applications and services globally. Our engineers contribute to open source projects and Cerner is proud to provide financial support to organizations like the Apache Software Foundation. Here are some of our own projects that we’ve released as open source. Terra   Terra offers a set of configurable React components designed to help build scalable and modular application UIs. The library easily integrates with webpack-based workflows and was created to solve real-world issues in projects we work on day to day. Learn More...  Bunsen   Bunsen integrates Apache Spark and FHIR, combining Spark's scalable processing advantages with FHIR's well-defined and curated data. This enables a number of deep analytic and machine learning workloads that can be ported across systems. Learn More...  Clara Rules   Clara is a forward-chaining rules engine written in Clojure(Script) with Java interoperability. It aims to simplify code with a developer-centric approach to expert systems. Learn More...   CDS Services Tutorial   Clinical Decision Support (CDS) Hooks is a specification that enables integration of remote decision support within a clinician's EHR workflow through a \"hook\"-based pattern. This tutorial will help walk you through setting up real CDS Services and acting as the 3rd party providing decision support to an EHR system. Learn More...  Beadledom   Beadledom is a bundle of common components required for building JAX-RS web services. It is the starting point for new services. The core of Beadledom provides health checks, monitoring via Stagemonitor, API docs via Swagger, JSON serialization via Jackson, and integration of these components with Guice. Learn More...  SMART® on FHIR® Tutorial   The SMART tutorial is intended to help developers quickly create an example smart app, register it with Cerner's code console, and test it against Cerner's sandbox. It is a Slate site hosted through GitHub Pages and includes a small example app in the source folder. Learn More...   Splunk Pickaxe   A tool for serializing and syncing your repo of Splunk objects across Splunk instances. This provides a development workflow for Splunk components (e.g., dashboards, alerts, reports, etc) and an easy way to apply them consistently. Learn More...  Canadarm   Canadarm makes JavaScript logging easy and seamless, helping you figure out what went wrong with your scripts. Named for the Canadarm we hope this too helps you piece together log information to figure out what went wrong with your scripts. Learn More...  GC Stats   gc_stats is a Ruby gem for Rails applications that collects and logs garbage collection statistics during each request. GC statistics are only logged if a GC event is detected. This allows you to track and analyze the GC characteristics of your Rails application. Learn More...   Carbon Graphs   Carbon is a graphing library built using D3 visualization library. It provides an API for generating native graphs such as Line and Paired Result graphs based on design standards. Learn More...  Jwala   Jwala is a web application that provides management for a group of Tomcat servers. Jwala is capable of creating and persisting definitions of Group instances, and exposes a RESTful interface to do so. The definition of a Group includes Web Apps, JVMs, Web Servers, and Resources. Learn More...    "
    },
    {
        "uri": "https://engineering.cerner.com/tech-talks/",
        "title": "",
        "tags": [],
        "description": "",
        "content": "Tech Talks   Cerner Tech Talks brings in great speakers for talks that would be of interest to engineers at Cerner. These talks are held periodically and vary widely in their content. Check out this playlist for all of our current and past talks.\n   DevCon Keynotes     DevCon 2020     DevCon 2019      DevCon 2018     DevCon 2017     DevCon 2016      DevCon 2015     DevCon 2014     DevCon 2013      "
    },
    {
        "uri": "https://engineering.cerner.com/about/",
        "title": "About",
        "tags": [],
        "description": "Life as an Engineer",
        "content": "At Cerner, we use the latest technologies to transform healthcare. We are the engineers and designers that focus on making you, your family, and friends healthy and happy.\nWe love solving the hard problems that no one else wants to tackle. We embrace polyglot programming, distributed systems, and open source.\nWe also know that it’s important to allow engineers to take a step back to learn and try new things. That’s why we host periodic hackathons, meetups, tech talks, and an annual internal two-day developers conference. We also love to attend, sponsor, and speak at conferences.\nIf this sounds interesting to you, we’d love to work together. Get it touch with us here.\n"
    },
    {
        "uri": "https://engineering.cerner.com/categories/",
        "title": "Categories",
        "tags": [],
        "description": "",
        "content": ""
    },
    {
        "uri": "https://engineering.cerner.com/series/",
        "title": "Series",
        "tags": [],
        "description": "",
        "content": ""
    }]
