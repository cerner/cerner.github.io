<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<title>
  
     Near Real-time Processing Over Hadoop and HBase | 
    Engineering Health
  
</title><meta name="description" content="a blog by engineers, for engineers"><meta name="author" content="Ryan Brush">

<link rel="icon" href="/favicon.png">


    
        
            <link rel="stylesheet" href="/dist/main.2b1315bd4c5d4f5eb804.min.css">
        
    




<link rel="canonical" href="https://engineering.cerner.com/blog/near-real-time-processing-over-hadoop-and-hbase/"><script src="/js/jquery-3.5.1.min.js"></script><meta property="og:title" content="Near Real-time Processing Over Hadoop and HBase" />
<meta property="og:description" content="From MapReduce to realtime This post covers much of the Near-Realtime Processing Over HBase talk I’m giving at ApacheCon NA 2013 in blog form. It also draws from the Hadoop, HBase, and Healthcare talk from StrataConf/Hadoop World 2012.
The first significant use of Hadoop at Cerner came in building search indexes for patient charts. While creation of simple search indexes is almost commoditized, we wanted a better experience based on clinical semantics." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://engineering.cerner.com/blog/near-real-time-processing-over-hadoop-and-hbase/" />
<meta property="og:image" content="https://engineering.cerner.com/images/alan-grace.png"/>
<meta property="article:published_time" content="2013-02-27T00:00:00+00:00" />
<meta property="article:modified_time" content="2013-02-27T00:00:00+00:00" /><meta property="og:site_name" content="Cerner Engineering" />
<meta itemprop="name" content="Near Real-time Processing Over Hadoop and HBase">
<meta itemprop="description" content="From MapReduce to realtime This post covers much of the Near-Realtime Processing Over HBase talk I’m giving at ApacheCon NA 2013 in blog form. It also draws from the Hadoop, HBase, and Healthcare talk from StrataConf/Hadoop World 2012.
The first significant use of Hadoop at Cerner came in building search indexes for patient charts. While creation of simple search indexes is almost commoditized, we wanted a better experience based on clinical semantics.">
<meta itemprop="datePublished" content="2013-02-27T00:00:00+00:00" />
<meta itemprop="dateModified" content="2013-02-27T00:00:00+00:00" />
<meta itemprop="wordCount" content="1392">
<meta itemprop="image" content="https://engineering.cerner.com/images/alan-grace.png"/>



<meta itemprop="keywords" content="design,engineering," />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://engineering.cerner.com/images/alan-grace.png"/>

<meta name="twitter:title" content="Near Real-time Processing Over Hadoop and HBase"/>
<meta name="twitter:description" content="From MapReduce to realtime This post covers much of the Near-Realtime Processing Over HBase talk I’m giving at ApacheCon NA 2013 in blog form. It also draws from the Hadoop, HBase, and Healthcare talk from StrataConf/Hadoop World 2012.
The first significant use of Hadoop at Cerner came in building search indexes for patient charts. While creation of simple search indexes is almost commoditized, we wanted a better experience based on clinical semantics."/>

</head>
<body>
    
<nav class="navbar navbar-expand-md navbar-light bg-light fixed-top shadow-sm" id="navbar-main-menu">
    <div class="container">
        <a class="navbar-brand" href="/"><img src="/logo.png" style='width: 100px;' /></a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#main-menu" aria-controls="main-menu" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div id="main-menu" class="collapse navbar-collapse" >
            <ul class="nav navbar-nav ml-auto">
                
                    <li class="nav-item"><a class="nav-link" href="/">Home</a></li>
                
                    <li class="nav-item"><a class="nav-link" href="/culture/">Culture</a></li>
                
                    <li class="nav-item"><a class="nav-link" href="/open-source/">Open Source</a></li>
                
                    <li class="nav-item"><a class="nav-link" href="/tech-talks/">Tech Talks</a></li>
                
            
            </ul>
            
            <div class="form-inline my-2 my-lg-0 searchbox">
    <input id="search-by" type="search" class="form-control mr-sm-2" placeholder="Search" aria-label="Search">
</div>

<script type="text/javascript" src="/js/lunr.min.js"></script>
<script type="text/javascript" src="/js/auto-complete.min.js"></script>
<script type="text/javascript">
    var baseurl =  "https:\/\/engineering.cerner.com";
</script>
<script type="text/javascript" src="/js/search.js"></script>
        </div>
    </div>
</nav>


    
<main class="content-page container pt-7 pb-5">
    
    <div class="row">
        <div class="col">
            <article class="article-content">
                <div class="row justify-content-center">
                    <div class="col-lg-8">
                        <div class="meta text-muted mb-3">
                            <p class="created text-muted text-uppercase font-weight-bold mb-1">February 27, 2013</p>
                            <span class="mr-2"><i class="fas fa-book-open mr-2"></i>1392 words</span>
                            <span><i class="fas fa-clock mr-2"></i>7 mins read</span>
                        </div>

                        <h1>Near Real-time Processing Over Hadoop and HBase</h1>

                        <ul class="authors list-inline">
        By: &nbsp;<li class="list-inline-item mr-3">
                    <div class="media author">
                            👤&nbsp;<div class="media-body">
                            <h5 class="name my-0 "><a href="/authors/ryan-brush/" class="small">Ryan Brush</a>
                            </h5>
                        </div>
                    </div>
                </li></ul>
                    </div>
                </div>

                <div class="row justify-content-center">
                    <div class="col-lg-8">
                        <div class="content">
                            <h2 id="from-mapreduce-to-realtime">From MapReduce to realtime</h2>
<p>This post covers much of the <a href="http://na.apachecon.com/schedule/presentation/161/">Near-Realtime Processing Over HBase</a> talk I’m giving at <a href="http://na.apachecon.com/">ApacheCon NA 2013</a> in blog form. It also draws from the <a href="http://strataconf.com/stratany2012/public/schedule/detail/25387">Hadoop, HBase, and Healthcare</a> talk from StrataConf/Hadoop World 2012.</p>
<p>The first significant use of Hadoop at Cerner came in building search indexes for patient charts. While creation of simple search indexes is almost commoditized, we wanted a better experience based on clinical semantics. For instance, if a user searches for &ldquo;heart disease&rdquo; and a patient has &ldquo;myocardial infarction&rdquo; documented, that document should be highly ranked in the results.</p>
<p>Analyzing and semantically annotating can be computationally expensive, especially when building indexes that could grow into the billions. Algorithms in this space may be discussed in a future blog post, but for now we focus on creation of an infrastructure up to the computational demands. For this, Hadoop is a great fit. A search index is logically a function of a set of input data, and MapReduce allows us to apply such functions in parallel across an arbitrarily large data set.</p>
<div align="center">
    <figure class="figure">
        <a href="chart-search-screen.png" class="d-block" data-toggle="lightbox" data-gallery="post-gallery">
            <img src="chart-search-screen.png"
                alt="Chart Search"class="figure-img img-fluid"
            /> 
        </a>
    </figure>
</div>
<h4 id="a-trend-towards-competing-needs">A trend towards competing needs</h4>
<p>The above pattern is powerful but creates a nice problem to have: people want the output of the processing &ndash; in this case, updates to search indexes &ndash; faster. Since we cannot run a MapReduce job over our entire data set every millisecond, we encounter competing needs; the need to <em>process all data holistically</em> conflicts with the need to <em>quickly apply incremental updates</em> to that processing.</p>
<p>This difference may seem simple, but has deep implications.  For instance:</p>
<ul>
<li>
<p>With MapReduce we can move our computation to the data, but fast updates require moving data to computation.</p>
</li>
<li>
<p>MapReduce jobs produce output as a pure function of the input; realtime processing needs to handle outdated state. For instance, we build a phone book and a name changes from Smith to Jones, realtime processing must remove the outdated entry, whereas MapReduce simply rebuilds the whole phone book.</p>
</li>
<li>
<p>MapReduce jobs often assume a static set of complete data, whereas realtime processing may see partial data or new data introduce in an unexpected order.</p>
</li>
</ul>
<p>And despite these differences, our processing output must be the identical; we need to apply the same logic across very different processing models.</p>
<h2 id="realtime-and-batch-layers">Realtime and batch layers</h2>
<p>These significant differences mean different processing infrastructures. Nathan Marz described this well in his <a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html">How to Beat the CAP Theorem</a> post. The result is a system that uses complementary technologies: stream-based processing with <a href="http://storm-project.net/%22">Storm</a> and batch processing with Hadoop.</p>
<p>Interestingly, <a href="http://hbase.apache.org/%22">HBase</a> sits at a juncture between realtime and batch processing models. It offers aspects of batch processing; computation can be moved to the data via direct MapReduce support. It also supports realtime patterns with random access and fast</p>
<div align="center">
    <figure class="figure">
        <a href="realtime-layer-batch-layer.png" class="d-block" data-toggle="lightbox" data-gallery="post-gallery">
            <img src="realtime-layer-batch-layer.png"
                alt="Realtime and Batch Layers"class="figure-img img-fluid"
            /> 
        </a>
    </figure>
</div>
<p>reads and writes. So our realtime and batch layers can be viewed like this:</p>
<ol>
<li>Data entering the system is persisted in HBase.</li>
<li>MapReduce jobs are used to create artifacts useful to consumers at scale.</li>
<li>Incremental updates are handled in realtime by processing updates to HBase in a Storm cluster, and are applied to the artifacts produced by MapReduce jobs.</li>
</ol>
<h2 id="processing-hbase-updates-in-realtime">Processing HBase updates in realtime</h2>
<p>So new data lands in HBase but how does Storm know to process it? There is precedent here. Google’s <a href="http://research.google.com/pubs/pub36726.html">Percolator paper</a> describes a technique for doing so over BigTable: it writes a notification entry to a column family whenever a row changes. Processing components scan for notifications and process them as they enter the system.</p>
<p>This is the general approach we have taken to initiate processing in Storm. Google’s Percolator strategy does not translate directly to HBase. Differences in the way regions are managed versus BigTable tables made using a different column family impractical. So we use a separate &ldquo;notification&rdquo; table to track changes to the original.  Updates to HBase go through an API that writes notification entries as well as the data itself. We then wrote a specialized Storm spout that scans the notification table to initiate processing of updates.</p>
<p>The result is processing infrastructure like this, with Storm Spouts and bolts complementing conventional MapReduce processing:</p>
<div align="center">
    <figure class="figure">
        <a href="processing-diagram.png" class="d-block" data-toggle="lightbox" data-gallery="post-gallery">
            <img src="processing-diagram.png"
                alt="Processing Diagram"class="figure-img img-fluid"
            /> 
        </a>
    </figure>
</div>
<p>The processed data model may be another set of HBase tables, a relational database, or some other data store. Its design should be centered on the needs of the applications and services, letting the processing infrastructure build data for those needs. It is important to note that MapReduce output should be done with a bulk load operation in order to avoid saturating the processed data store with individual updates.</p>
<p>This basic model turns out to be robust. Volume spikes from source systems can be spread throughout the HBase cluster. There are a couple key steps for success here:</p>
<ul>
<li>
<p>Regular major compactions on the notification HBase tables are essential. Without major compactions, completed notifications will pile up and performance of the system will gradually degrade.</p>
</li>
<li>
<p>The notification tables themselves may be small in size, but should be aggressively split across the cluster. This spreads load to handle volume spikes and improve concurrency.</p>
</li>
</ul>
<p>Also note that MapReduce is still an important part of the system. It’s simply a better tool for batch operations like bringing a new data set online or re-processing an existing data set with new logic.</p>
<h2 id="measure-everything">Measure Everything</h2>
<p>There are a number of moving parts in this system, and good measurements are the best way to ensure it’s working well. For example, in development we found our HBase Region Servers would encounter frequent but short-lived process queues during heavy load. This didn’t look like an issue in HBase, but when we measured the performance of the calling process there was a noticeable degradation. The point is, instrumentation built into Hadoop and HBase are great but not sufficient. Measuring the observed performance at all layers is important to create an optimal system.</p>
<p>There are many good technologies for doing so. We generally use the <a href="https://github.com/codahale/metrics">Metrics API</a> by Coda Hale. Here is an example of HBase client throughput using an instrumented implementation of HTableInterface. The data is collected by the Metrics API and displayed with <a href="http://graphite.wikidot.com/">Graphite</a>:</p>
<div align="center">
    <figure class="figure">
        <a href="measure-everything.png" class="d-block" data-toggle="lightbox" data-gallery="post-gallery">
            <img src="measure-everything.png"
                alt="Measure Everything"class="figure-img img-fluid"
            /> 
        </a>
    </figure>
</div>
<h2 id="different-models-same-logic">Different models, same logic</h2>
<p>The same logic needs to be applied to both batch and stream processing despite the necessary differences in infrastructure. This is a challenge since the models speak very different languages: InputFormats describe an immutable and complete set of data, whereas event streams expose incremental changes without context.</p>
<p>It turns out the function is the only real commonality between them; simply taking a subset of input and returning useful output. So, our strategy is this:</p>
<p><em>Build all logic as a set of simple functions, then compose and coordinate those functions with higher-level processing libraries.</em></p>
<p>We use Storm to compose our realtime processing and Apache Crunch to compose our MapReduce jobs. Here are some lessons we have learned to apply this strategy effectively:</p>
<h4 id="minimize-intermediate-state">Minimize intermediate state</h4>
<p>Persisting intermediate state can be expensive and creates complex relationships between moving parts. This is particularly true if a MapReduce job creates intermediate state used by realtime processing or vice versa. Instead, keep processing pipelines independent whenever possible and combine the results at the end.</p>
<h4 id="isolate-processing-models">Isolate processing models</h4>
<p>Our MapReduce jobs are typically run on separate infrastructure than realtime processing to ensure expensive jobs do not saturate time-critical processing.</p>
<h4 id="be-aware-of-the-semantic-differences-in-the-processing-models">Be aware of the semantic differences in the processing models</h4>
<p>A &ldquo;join&rdquo; in a MapReduce job sees all data, whereas a &ldquo;join&rdquo; in stream processing gets incremental subsets. If a function needs the full context to execute, that context must be externally loaded in the realtime processing system. In our case, external state is loaded from HBase and cached, but projects like <a href="http://engineering.twitter.com/2012/08/trident-high-level-abstraction-for.html">Trident</a> are now providing some aggregation facilities over storm as well.</p>
<h2 id="the-path-forward">The path forward</h2>
<p>The patterns here have been successful but require significant scaffolding and infrastructure to bring together. Near-realtime processing demands over big data are bound to increase, which means there is an opportunity here; higher level abstractions should emerge. Similar to how tools like Crunch and Hive offer abstractions over MapReduce, it’s likely that similar primitives can express the patterns described here.</p>
<p>How these higher abstractions emerge remains to be seen, but there is one thing I’m sure of: it’s going to be fun.</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>I’d like to acknowledge key contributors to building this and related systems: Jason Bray, Ben Brown, Robert Farr, Preston Koprivica, Swarnim Kulkarni, Kyle McGovern, Andrew Olson, Mike Richards, Micah Whitacre, Greg Whitsitt, and others.</p>

                        </div><div class="tags my-3"><a class="badge badge-pill badge-light border mr-2" href="/tags/design">
                                    <i class="fas fa-tag mr-2"></i>design
                                </a><a class="badge badge-pill badge-light border mr-2" href="/tags/engineering">
                                    <i class="fas fa-tag mr-2"></i>engineering
                                </a></div><ul class="share nav my-3 justify-content-end">
        <li class="nav-item">
            <a class="nav-link" target="_blank" href="https://twitter.com/intent/tweet?url=https%3a%2f%2fengineering.cerner.com%2fblog%2fnear-real-time-processing-over-hadoop-and-hbase%2f&text=Near%20Real-time%20Processing%20Over%20Hadoop%20and%20HBase">
              <i class="fa-fw fab fa-twitter"></i>
          </a>
        </li>
        <li class="nav-item">
            <a class="nav-link" target="_blank" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fengineering.cerner.com%2fblog%2fnear-real-time-processing-over-hadoop-and-hbase%2f&title=Near%20Real-time%20Processing%20Over%20Hadoop%20and%20HBase">
                <i class="fa-fw fab fa-linkedin-in"></i>
            </a>
        </li>
        <li class="nav-item">
            <a class="nav-link" target="_blank" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fengineering.cerner.com%2fblog%2fnear-real-time-processing-over-hadoop-and-hbase%2f&t=Near%20Real-time%20Processing%20Over%20Hadoop%20and%20HBase">
                <i class="fa-fw fab fa-facebook-f"></i>
            </a>
        </li>
        <li class="nav-item">
            <a class="nav-link" target="_blank" href="https://reddit.com/submit?url=https%3a%2f%2fengineering.cerner.com%2fblog%2fnear-real-time-processing-over-hadoop-and-hbase%2f&title=Near%20Real-time%20Processing%20Over%20Hadoop%20and%20HBase">
                <i class="fa-fw fab fa-reddit-alien"></i>
            </a>
        </li>
    </nav>
                    </div>
                </div>

                <div class="row justify-content-center">
                    <div class="col-lg-8">
                        
                    </div>
                </div></article>
        </div>
    </div>

    <div class="related-content row mt-5 row-cols-1 row-cols-lg-3"><div class="col mb-3">
                <div class="card h-100">
    
    <a href="/blog/composable-mapreduce-with-hadoop-and-crunch/" class="d-block"><img data-src="/blog/composable-mapreduce-with-hadoop-and-crunch/diagram1_hu7094931f5df834b55429a2ec082916c6_53870_700x350_fill_box_smart1_2.png" class="card-img-top mx-auto d-block" alt="Composable MapReduce with Hadoop and Crunch"><div class="card-body">
            <h4 class="card-title">Composable MapReduce with Hadoop and Crunch</h4>
            <p class="card-text text-muted text-uppercase">February 3, 2013</p>
            <div class="card-text">
                Most developers know this pattern well: we design a set of schemas to represent our data, and then work with that data via a query language. This works great in most cases, but becomes a challenge as data sets grow to an arbitrary size and complexity. Data sets can become too large to query and update with conventional means.
These challenges often arise with Hadoop, simply because Hadoop is a popular tool to tackle such data sets.
            </div>
        </div>
    </a>
  </div>
            </div><div class="col mb-3">
                <div class="card h-100">
    
    <a href="/blog/evangelizing-user-experience/" class="d-block"><img data-src="/blog/evangelizing-user-experience/roadNotes_before_huf7c3e169f390c06ddf0f18116a36b0c9_244540_700x350_fill_box_smart1_2.png" class="card-img-top mx-auto d-block" alt="Evangelizing User Experience"><div class="card-body">
            <h4 class="card-title">Evangelizing User Experience</h4>
            <p class="card-text text-muted text-uppercase">February 12, 2013</p>
            <div class="card-text">
                In the dark ages of development, great software meant packing in the functionality. People began doing more and more with their software. Updates meant newer and more exciting functionality. Sounds great, right? Of course it does, but something went horribly wrong. Slowly we became inundated with cluttered screens as software developers struggled to find a place to put their latest innovative functionality. Buttons began adding up and before we knew it, we were inventing user interface controls like ribbons to hold all the buttons.
            </div>
        </div>
    </a>
  </div>
            </div></div>
</main>


    <footer class="footer text-center bg-dark py-6">
    <div class="container">
        <div class="row">
            <div class="col">
                <ul class="list-inline">
                    <li class="list-inline-item"><a href="https://engineering.cerner.com/index.xml" rel="alternate" type="application/rss+xml" class="icons d-block">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a></li><li class="list-inline-item">
                            <a href="https://github.com/cerner" class="icons d-block">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li><li class="list-inline-item">
                            <a href="https://www.instagram.com/cernercorporation/" class="icons d-block">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fab fa-instagram fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li><li class="list-inline-item">
                            <a href="https://www.linkedin.com/company/cerner-corporation" class="icons d-block">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li><li class="list-inline-item">
                            <a href="https://twitter.com/intent/follow?original_referer=https%3A%2F%2Fengineering.cerner.com%2F&amp;screen_name=CernerEng" class="icons d-block">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li><li class="list-inline-item">
                            <a href="https://www.youtube.com/user/cernereng?sub_confirmation=1" class="icons d-block">
                                <span class="fa-stack fa-lg">
                                    <i class="fa fa-circle fa-stack-2x"></i>
                                    <i class="fab fa-youtube fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                </ul>

                <p class="text-light">
                    
                        Copyright &copy; 2022
                    
                </p>

                <p class="text-light">
                Made with ❤️ by Cerner engineers.
                </p>
            </div>
        </div>
    </div>
</footer>

    
    
        
            <script src="/dist/main.0f92af9103b4f0550a4b.min.js"></script>
        
    






    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-37701128-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

</body>
</html>
